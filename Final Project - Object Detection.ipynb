{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fDltP3fqgj"
      },
      "source": [
        "# Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1A8Wnm7fqgm",
        "outputId": "eb5a759f-8420-47db-bffc-cb8b01e15c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 41.4/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBbiI7mKfqgo",
        "outputId": "3776156d-bfdc-49f9-9040-0dabfb620d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.95  Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "Setup complete  (4 CPUs, 8.0 GB RAM, 112.4/116.4 GB disk)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Anaconda\\envs\\CPE313_Cuadra\\Lib\\site-packages\\torch\\cuda\\__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        }
      ],
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGziPrIafqgq"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58w8JLpMnjH",
        "outputId": "12539689-3b78-4b12-ad61-87a4bdb729c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.95 ðŸš€ Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "                   all          4         17       0.57       0.85      0.847      0.632\n",
            "                person          3         10      0.557        0.6      0.585      0.272\n",
            "                   dog          1          1      0.548          1      0.995      0.697\n",
            "                 horse          1          2      0.531          1      0.995      0.674\n",
            "              elephant          1          2      0.371        0.5      0.516      0.256\n",
            "              umbrella          1          1      0.569          1      0.995      0.995\n",
            "          potted plant          1          1      0.847          1      0.995      0.895\n",
            "Speed: 3.9ms preprocess, 123.5ms inference, 0.0ms loss, 10.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E:\\Anaconda\\envs\\CPE313_Cuadra\\Lib\\site-packages\\torch\\cuda\\__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\datasets\\coco8\\labels\\val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\datasets\\coco8\\labels\\val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.37it/s]\n"
          ]
        }
      ],
      "source": [
        "!yolo val model=yolo11n.pt data=coco8.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktegpM42AooT",
        "outputId": "bed26378-051b-4dc5-c90f-cf14da3da18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m comet_ml.init() is deprecated and will be removed soon. Please use comet_ml.login()\n"
          ]
        }
      ],
      "source": [
        "logger = 'Comet'\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "54c9a0c0-7b49-4b4b-a990-c1f898d10093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'yolo11s.pt'...\n",
            "Ultralytics 8.3.95 ðŸš€ Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "YOLO11s summary (fused): 100 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.6.0+cu118...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 3.7s, saved as 'yolo11s.torchscript' (36.5 MB)\n",
            "\n",
            "Export complete (4.7s)\n",
            "Results saved to \u001b[1mC:\\Users\\Audrick\\Desktop\\Final Project\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11s.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11s.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
            "Visualize:       https://netron.app\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/18.4M [00:00<?, ?B/s]\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 9.00M/18.4M [00:00<00:00, 89.9MB/s]\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 17.6M/18.4M [00:00<00:00, 45.3MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.4M/18.4M [00:00<00:00, 49.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!yolo export model=yolo11n.pt format=torchscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFVX0udzfqgv"
      },
      "source": [
        "# YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLXpLj8fqgv"
      },
      "source": [
        "## Attempt 1: Base Model (YOLOv11 Nano)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUpdwuckfqgw",
        "outputId": "a0d77fee-ef1b-4ce0-ad13-5e443c5cd26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.116 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.95  Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Anaconda\\envs\\CPE313_Cuadra\\Lib\\site-packages\\torch\\cuda\\__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\data.yaml, epochs=15, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,590,620 parameters, 2,590,604 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/qazgcuadra/general/846954f4e09147b3868337ab6327a941\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'c:\\\\Users\\\\Audrick\\\\Desktop\\\\Final Project' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\train\\labels.cache... 1978 images, 302 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1978/1978 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\valid\\labels.cache... 262 images, 33 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/15         0G       1.42      2.801      1.491         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [17:45<00:00,  8.59s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:55<00:00,  6.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.667      0.519      0.657      0.414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/15         0G      1.375      1.933      1.446         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [17:10<00:00,  8.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:54<00:00,  6.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.478      0.746      0.677       0.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/15         0G      1.407      1.766      1.469         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:52<00:00,  8.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:48<00:00,  5.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.764      0.659       0.75      0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/15         0G      1.404      1.648      1.458         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:42<00:00,  8.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:48<00:00,  5.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.808      0.738      0.792      0.522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/15         0G      1.332      1.479      1.429         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:22<00:00,  7.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.782       0.79       0.83      0.529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/15         0G      1.415      1.504      1.516         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:39<00:00,  8.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:46<00:00,  5.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.874      0.783      0.848      0.564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/15         0G      1.356      1.309      1.471         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:30<00:00,  7.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  5.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.842      0.812      0.857      0.577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/15         0G      1.321       1.21      1.445         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:33<00:00,  8.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:46<00:00,  5.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.873      0.855      0.887      0.598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/15         0G      1.287      1.124      1.419         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:35<00:00,  8.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.841      0.838      0.877      0.616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/15         0G      1.249      1.078      1.399         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:35<00:00,  8.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  5.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.876       0.84      0.897      0.632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/15         0G      1.237     0.9878      1.385         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:39<00:00,  8.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:51<00:00,  5.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.889       0.87        0.9       0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/15         0G       1.19     0.9224       1.35         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [17:01<00:00,  8.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:48<00:00,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.907      0.846      0.917      0.643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/15         0G      1.171     0.8929      1.337         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:35<00:00,  8.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:46<00:00,  5.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.909      0.863      0.915      0.663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/15         0G      1.134     0.8472      1.315         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [16:37<00:00,  8.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  5.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.908      0.847      0.919      0.667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/15         0G      1.101      0.815      1.299         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [17:05<00:00,  8.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:51<00:00,  5.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655       0.92      0.863      0.923      0.676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "15 epochs completed in 4.406 hours.\n",
            "Optimizer stripped from runs\\detect\\train3\\weights\\last.pt, 5.5MB\n",
            "Optimizer stripped from runs\\detect\\train3\\weights\\best.pt, 5.5MB\n",
            "\n",
            "Validating runs\\detect\\train3\\weights\\best.pt...\n",
            "Ultralytics 8.3.95  Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:39<00:00,  4.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655       0.92      0.863      0.923      0.675\n",
            "                 Boots         75        148      0.906      0.865      0.917      0.574\n",
            "                Gloves        125        236      0.939       0.72      0.857      0.593\n",
            "                Helmet        142        151      0.954      0.947      0.981      0.813\n",
            "                  Vest         95        120       0.88       0.92      0.938      0.722\n",
            "Speed: 3.3ms preprocess, 137.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : surprised_purlin_3440\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/qazgcuadra/general/846954f4e09147b3868337ab6327a941\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg0 [31]               : (9.499999999999998e-05, 0.0010820833333333333)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg1 [31]               : (9.499999999999998e-05, 0.0010820833333333333)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg2 [31]               : (9.499999999999998e-05, 0.0010820833333333333)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50(B) [32]     : (0.65704, 0.9231437423087323)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95(B) [32]  : (0.41424, 0.6759)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision(B) [32] : (0.47769, 0.92024)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall(B) [32]    : (0.51851, 0.87021)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/GFLOPs              : 6.444\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/parameters          : 2590620\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/speed_PyTorch(ms)   : 159.67\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [30]       : (1.10102, 1.41974)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss [30]       : (0.81505, 2.80059)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/dfl_loss [30]       : (1.2988, 1.51588)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [30]         : (0.94864, 1.33389)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss [30]         : (0.92751, 2.91119)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/dfl_loss [30]         : (1.15376, 1.45072)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_logging_interval  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_confusion_matrix_on_eval : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_image_predictions        : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_image_predictions        : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     auto_augment    : randaugment\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bgr             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste_mode : flip\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crop_fraction   : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\data.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     embed           : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 15\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     erasing         : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 640\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : yolo11n.pt\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train3\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : runs\\detect\\train3\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time            : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 20\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 1 (5.21 MB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 2 metrics, params and output messages\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 7 file(s), remaining 3.75 MB/8.28 MB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\test\\images\\image_162_jpg.rf.3609856464b049d8fac7a9ad89742653.jpg: 640x640 2 Bootss, 2 Glovess, 1 Helmet, 1 Vest, 135.9ms\n",
            "Speed: 16.6ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11n.pt')\n",
        "model.train(data=r\"C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\data.yaml\", epochs=15)\n",
        "result1 = model(r\"C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\test\\images\\image_162_jpg.rf.3609856464b049d8fac7a9ad89742653.jpg\", conf=0.5)\n",
        "result1[0].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwrD0gI6fqgw"
      },
      "source": [
        "## Attempt 2: Base Model (YOLOv11 Small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tarxDvlYfqgw",
        "outputId": "1e90c47e-9b3d-43cd-e174-838773098360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.126 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.95  Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11s.pt, data=C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\data.yaml, epochs=15, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,429,340 parameters, 9,429,324 gradients, 21.6 GFLOPs\n",
            "\n",
            "Transferred 493/499 items from pretrained weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/qazgcuadra/general/774c40084e7d41cc9a59754e8ff2bfc8\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'c:\\\\Users\\\\Audrick\\\\Desktop\\\\Final Project' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\train\\labels.cache... 1978 images, 302 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1978/1978 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\valid\\labels.cache... 262 images, 33 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs\\detect\\train5\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns\\detect\\train5\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/15         0G      1.397      2.248      1.494         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [8:03:37<00:00, 234.01s/it]  \n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:11<00:00, 21.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        262        655      0.524      0.607      0.522      0.273\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/15         0G      1.456      1.757      1.506         76        640:  11%|â–ˆâ–        | 14/124 [55:50<7:51:52, 257.39s/it]"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo11s.pt')\n",
        "model.train(data=r\"C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\data.yaml\", epochs=15)\n",
        "result2 = model(r\"C:\\Users\\Audrick\\Desktop\\Final Project\\PPE Project.v3i.yolov11\\test\\images\\image_162_jpg.rf.3609856464b049d8fac7a9ad89742653.jpg\", conf=0.5)\n",
        "result2[0].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0-KNaO8fqgx"
      },
      "source": [
        "## Attempt 3: Base Model Yolov8n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4VB022Xfqgx",
        "outputId": "05beceea-3427-4b80-bb58-3c195d513626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.95 ðŸš€ Python-3.11.11 torch-2.6.0+cu118 CPU (Intel Core(TM) i5-7400 3.00GHz)\n",
            "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.6.0+cu118...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.9s, saved as 'yolov8n.torchscript' (12.4 MB)\n",
            "\n",
            "Export complete (4.1s)\n",
            "Results saved to \u001b[1mC:\\Users\\Audrick\\Desktop\\Final Project\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ],
      "source": [
        "!yolo export model=yolov8n.pt format=torchscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38egWR0Lfqgx",
        "outputId": "fdb33cec-7363-45de-938e-ef5abc860927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 97.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/PPE-Project-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 957.0Â±506.6 MB/s, size: 44.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/PPE-Project-1/train/labels.cache... 989 images, 142 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 989/989 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 379.4Â±83.8 MB/s, size: 37.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels.cache... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/15      3.22G       1.13      2.524      1.319         71        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:17<00:00,  3.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.996       0.15      0.673      0.423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/15      3.24G      1.096      1.514      1.239         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:19<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.598      0.688      0.644      0.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/15      3.24G      1.111      1.428      1.259         67        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.791      0.739       0.81      0.537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/15      3.26G      1.081      1.376      1.257         86        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:16<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.763      0.769      0.802      0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/15      3.28G      1.046      1.232      1.216         73        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.797       0.81      0.831      0.589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/15      3.28G       1.06      1.327       1.28         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:16<00:00,  3.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.828      0.783      0.839      0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/15      3.28G      1.012      1.193      1.235         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.784      0.817      0.838      0.594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/15      3.29G     0.9902      1.105      1.231         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.848      0.799      0.868      0.635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/15      3.29G     0.9542      1.027      1.197         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:14<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.869      0.843      0.899      0.668\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/15      3.31G     0.9293      0.971       1.18         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.875       0.84      0.907      0.672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/15      3.31G     0.8811     0.9061      1.138         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:14<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.885      0.848      0.912      0.685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/15      3.31G     0.8839     0.8571      1.132         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796       0.82       0.89      0.896      0.679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/15      3.33G     0.8425     0.8237      1.123         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:14<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.877      0.834      0.908      0.684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/15      3.33G     0.8225     0.7817      1.115         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:14<00:00,  4.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.845      0.882      0.918        0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/15      3.33G     0.8083     0.7663      1.098         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796       0.85      0.877      0.913      0.703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15 epochs completed in 0.079 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:04<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.839      0.889      0.913      0.703\n",
            "                 Boots         75        148      0.646      0.899       0.84      0.558\n",
            "                Gloves        125        236      0.831      0.797      0.852      0.586\n",
            "                Helmet        142        151       0.96      0.974      0.982      0.833\n",
            "                Person        113        141      0.908      0.858      0.958      0.822\n",
            "                  Vest         95        120      0.851      0.917      0.932      0.714\n",
            "Speed: 0.4ms preprocess, 2.2ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f785cc13e90>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0055102,   0.0027551,           0],\n",
              "       [          1,           1,           1, ...,   0.0030408,   0.0015204,           0],\n",
              "       [          1,           1,           1, ...,     0.48089,     0.48089,           0],\n",
              "       [          1,           1,           1, ...,     0.43925,     0.43925,           0],\n",
              "       [          1,           1,           1, ...,    0.022283,    0.011141,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.10557,     0.10557,     0.19604, ...,           0,           0,           0],\n",
              "       [   0.086106,    0.086106,     0.18713, ...,           0,           0,           0],\n",
              "       [   0.018098,    0.018098,     0.28354, ...,    0.014465,           0,           0],\n",
              "       [  0.0096661,   0.0096661,     0.17396, ...,           0,           0,           0],\n",
              "       [    0.16964,     0.16964,     0.34298, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.055791,    0.055791,       0.109, ...,           1,           1,           1],\n",
              "       [   0.045052,    0.045052,     0.10365, ...,           1,           1,           1],\n",
              "       [  0.0091316,   0.0091316,     0.16519, ...,           1,           1,           1],\n",
              "       [  0.0048565,   0.0048565,    0.095264, ...,           1,           1,           1],\n",
              "       [   0.092751,    0.092751,     0.20771, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97973,     0.97973,     0.97297, ...,           0,           0,           0],\n",
              "       [    0.97034,     0.97034,     0.96186, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,   0.0072854,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0],\n",
              "       [    0.99167,     0.99167,     0.98333, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7236716382522802)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([     0.5579,     0.58604,     0.83348,     0.82183,     0.71419])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.839478523517157), 'metrics/recall(B)': np.float64(0.888718289390094), 'metrics/mAP50(B)': np.float64(0.9125236023363362), 'metrics/mAP50-95(B)': np.float64(0.7026880866873851), 'fitness': np.float64(0.7236716382522802)}\n",
              "save_dir: PosixPath('runs/detect/train3')\n",
              "speed: {'preprocess': 0.4469788587822632, 'inference': 2.215257866412803, 'loss': 0.0008636259541398923, 'postprocess': 5.004838114504629}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "model.train(data=r\"/content/PPE-Project-1/data.yaml\", epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lUxNdJfqgy"
      },
      "source": [
        "## Attempt 4: Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3orGeQQZfqgy",
        "outputId": "ed6500af-ba4e-4c60-b74a-0dd57882adb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=runs/detect/tune'\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mðŸ’¡ Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'iou': 0.7, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m1/10 iterations complete âœ… (318.21s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.72388 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.85006, 'metrics/recall(B)': 0.87685, 'metrics/mAP50(B)': 0.91255, 'metrics/mAP50-95(B)': 0.70291, 'val/box_loss': 0.88663, 'val/cls_loss': 0.92306, 'val/dfl_loss': 1.11814, 'fitness': 0.72388}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train4\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 2/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.96998, 'weight_decay': 0.00052, 'iou': 0.72075, 'hsv_h': 0.01472, 'hsv_s': 0.70012, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.10327, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m2/10 iterations complete âœ… (630.20s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.72388 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.85006, 'metrics/recall(B)': 0.87685, 'metrics/mAP50(B)': 0.91255, 'metrics/mAP50-95(B)': 0.70291, 'val/box_loss': 0.88663, 'val/cls_loss': 0.92306, 'val/dfl_loss': 1.11814, 'fitness': 0.72388}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train4\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 3/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.97742, 'weight_decay': 0.0005, 'iou': 0.69298, 'hsv_h': 0.01317, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.08613, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m3/10 iterations complete âœ… (937.55s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73159 observed at iteration 3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.88164, 'metrics/recall(B)': 0.87074, 'metrics/mAP50(B)': 0.9206, 'metrics/mAP50-95(B)': 0.71059, 'val/box_loss': 0.86581, 'val/cls_loss': 0.88153, 'val/dfl_loss': 1.11345, 'fitness': 0.73159}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.97742\n",
            "weight_decay: 0.0005\n",
            "iou: 0.69298\n",
            "hsv_h: 0.01317\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08613\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 4/10 with hyperparameters: {'lr0': 0.00671, 'momentum': 0.8, 'weight_decay': 0.0005, 'iou': 0.67977, 'hsv_h': 0.00894, 'hsv_s': 0.8, 'hsv_v': 0.46674, 'degrees': 0.0, 'translate': 0.09339, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m4/10 iterations complete âœ… (1250.67s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73159 observed at iteration 3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.88164, 'metrics/recall(B)': 0.87074, 'metrics/mAP50(B)': 0.9206, 'metrics/mAP50-95(B)': 0.71059, 'val/box_loss': 0.86581, 'val/cls_loss': 0.88153, 'val/dfl_loss': 1.11345, 'fitness': 0.73159}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.97742\n",
            "weight_decay: 0.0005\n",
            "iou: 0.69298\n",
            "hsv_h: 0.01317\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08613\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 5/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.97742, 'weight_decay': 0.00052, 'iou': 0.66377, 'hsv_h': 0.01239, 'hsv_s': 0.66495, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.08307, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m5/10 iterations complete âœ… (1550.08s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73159 observed at iteration 3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.88164, 'metrics/recall(B)': 0.87074, 'metrics/mAP50(B)': 0.9206, 'metrics/mAP50-95(B)': 0.71059, 'val/box_loss': 0.86581, 'val/cls_loss': 0.88153, 'val/dfl_loss': 1.11345, 'fitness': 0.73159}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.97742\n",
            "weight_decay: 0.0005\n",
            "iou: 0.69298\n",
            "hsv_h: 0.01317\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08613\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 6/10 with hyperparameters: {'lr0': 0.00934, 'momentum': 0.87293, 'weight_decay': 0.00056, 'iou': 0.78859, 'hsv_h': 0.01402, 'hsv_s': 0.66495, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.08142, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m6/10 iterations complete âœ… (1857.77s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73271 observed at iteration 6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.89925, 'metrics/recall(B)': 0.83494, 'metrics/mAP50(B)': 0.9169, 'metrics/mAP50-95(B)': 0.71224, 'val/box_loss': 0.87203, 'val/cls_loss': 0.92414, 'val/dfl_loss': 1.10375, 'fitness': 0.73271}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train9\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.00934\n",
            "momentum: 0.87293\n",
            "weight_decay: 0.00056\n",
            "iou: 0.78859\n",
            "hsv_h: 0.01402\n",
            "hsv_s: 0.66495\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08142\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 7/10 with hyperparameters: {'lr0': 0.00934, 'momentum': 0.87173, 'weight_decay': 0.00056, 'iou': 0.78621, 'hsv_h': 0.01398, 'hsv_s': 0.66661, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.08142, 'scale': 0.50335, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m7/10 iterations complete âœ… (2155.25s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73271 observed at iteration 6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.89925, 'metrics/recall(B)': 0.83494, 'metrics/mAP50(B)': 0.9169, 'metrics/mAP50-95(B)': 0.71224, 'val/box_loss': 0.87203, 'val/cls_loss': 0.92414, 'val/dfl_loss': 1.10375, 'fitness': 0.73271}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train9\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.00934\n",
            "momentum: 0.87293\n",
            "weight_decay: 0.00056\n",
            "iou: 0.78859\n",
            "hsv_h: 0.01402\n",
            "hsv_s: 0.66495\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08142\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 8/10 with hyperparameters: {'lr0': 0.00934, 'momentum': 0.82805, 'weight_decay': 0.00055, 'iou': 0.79813, 'hsv_h': 0.01346, 'hsv_s': 0.7066, 'hsv_v': 0.47665, 'degrees': 0.0, 'translate': 0.08617, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m8/10 iterations complete âœ… (2453.41s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73271 observed at iteration 6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.89925, 'metrics/recall(B)': 0.83494, 'metrics/mAP50(B)': 0.9169, 'metrics/mAP50-95(B)': 0.71224, 'val/box_loss': 0.87203, 'val/cls_loss': 0.92414, 'val/dfl_loss': 1.10375, 'fitness': 0.73271}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train9\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.00934\n",
            "momentum: 0.87293\n",
            "weight_decay: 0.00056\n",
            "iou: 0.78859\n",
            "hsv_h: 0.01402\n",
            "hsv_s: 0.66495\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08142\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 9/10 with hyperparameters: {'lr0': 0.0094, 'momentum': 0.87293, 'weight_decay': 0.00057, 'iou': 0.78859, 'hsv_h': 0.01403, 'hsv_s': 0.65456, 'hsv_v': 0.40147, 'degrees': 0.0, 'translate': 0.08142, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m9/10 iterations complete âœ… (2753.93s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73271 observed at iteration 6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.89925, 'metrics/recall(B)': 0.83494, 'metrics/mAP50(B)': 0.9169, 'metrics/mAP50-95(B)': 0.71224, 'val/box_loss': 0.87203, 'val/cls_loss': 0.92414, 'val/dfl_loss': 1.10375, 'fitness': 0.73271}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train9\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.00934\n",
            "momentum: 0.87293\n",
            "weight_decay: 0.00056\n",
            "iou: 0.78859\n",
            "hsv_h: 0.01402\n",
            "hsv_s: 0.66495\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08142\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 10/10 with hyperparameters: {'lr0': 0.00934, 'momentum': 0.89058, 'weight_decay': 0.00056, 'iou': 0.78859, 'hsv_h': 0.01402, 'hsv_s': 0.65785, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.08075, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m10/10 iterations complete âœ… (3053.96s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.73271 observed at iteration 6\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.89925, 'metrics/recall(B)': 0.83494, 'metrics/mAP50(B)': 0.9169, 'metrics/mAP50-95(B)': 0.71224, 'val/box_loss': 0.87203, 'val/cls_loss': 0.92414, 'val/dfl_loss': 1.10375, 'fitness': 0.73271}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train9\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.00934\n",
            "momentum: 0.87293\n",
            "weight_decay: 0.00056\n",
            "iou: 0.78859\n",
            "hsv_h: 0.01402\n",
            "hsv_s: 0.66495\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.08142\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "model.tune(\n",
        "    data=r\"/content/PPE-Project-1/data.yaml\",\n",
        "    epochs=15,\n",
        "    iterations=10,\n",
        "    val=True,\n",
        "    space={\n",
        "        'lr0': (1e-4, 1e-2),\n",
        "        'momentum': (0.8, 0.98),\n",
        "        'weight_decay': (0.0001, 0.01),\n",
        "        'iou': (0.1, 0.95),\n",
        "        'hsv_h': (0.0, 0.015),\n",
        "        'hsv_s': (0.4, 0.8),\n",
        "        'hsv_v': (0.4, 0.8),\n",
        "        'degrees': (0.0, 10.0),\n",
        "        'translate': (0.0, 0.2),\n",
        "        'scale': (0.5, 1.5),\n",
        "        'shear': (0.0, 2.0),\n",
        "        'perspective': (0.0, 0.001),\n",
        "    }\n",
        ") 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG8YlbFMfqgy"
      },
      "source": [
        "## Attempt 5: Applying the best parameters (changing the epoch to 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5EvSesifqgy",
        "outputId": "fb3afe27-6685-4da6-92dc-72883439be68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/PPE-Project-1/data.yaml, degrees=0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.01402, hsv_s=0.66495, hsv_v=0.4, imgsz=640, int8=False, iou=0.78859, keras=False, kobj=1.0, line_width=None, lr0=0.00934, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.87293, mosaic=1.0, multi_scale=False, name=train14, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train14, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.08142, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.00056, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1368.7Â±534.0 MB/s, size: 44.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/PPE-Project-1/train/labels.cache... 989 images, 142 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 989/989 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 347.1Â±98.3 MB/s, size: 37.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels.cache... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train14/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.00934' and 'momentum=0.87293' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.00056), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/15       2.9G      1.136      2.555      1.329         70        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:20<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.954      0.103       0.72      0.451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/15      3.22G      1.081      1.524      1.244         61        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:17<00:00,  3.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.635      0.656      0.667      0.447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/15      3.22G      1.114      1.421      1.267         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.553       0.68      0.631      0.426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/15      3.22G      1.067      1.366       1.25         87        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:17<00:00,  3.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.784      0.722      0.795      0.553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/15      3.22G      1.041      1.234      1.217         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.802      0.725      0.789      0.567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/15      3.22G      1.062      1.334      1.274         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:18<00:00,  3.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.818       0.79      0.854      0.617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/15      3.22G       1.02      1.201      1.224         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:16<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.749      0.794      0.821      0.592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/15      3.22G     0.9711      1.093      1.213         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.856      0.764      0.849      0.629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/15      3.22G     0.9457      1.019      1.185         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.848      0.799      0.863      0.648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/15      3.24G     0.9239     0.9584      1.177         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.892       0.82      0.896       0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/15      3.24G     0.8825      0.904      1.136         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.899      0.811      0.903      0.689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/15      3.24G     0.8706     0.8472      1.122         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  4.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.829      0.851       0.88      0.678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/15      3.26G     0.8357     0.8142      1.118         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:16<00:00,  3.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.867      0.856      0.901      0.697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/15      3.26G     0.8223     0.7702      1.107         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:14<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  4.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.899      0.835      0.917      0.712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/15      3.26G     0.8027     0.7539        1.1         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:15<00:00,  3.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.876      0.868      0.906      0.707\n",
            "\n",
            "15 epochs completed in 0.080 hours.\n",
            "Optimizer stripped from runs/detect/train14/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train14/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train14/weights/best.pt...\n",
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.898      0.835      0.917      0.712\n",
            "                 Boots         75        148      0.789       0.77      0.849      0.577\n",
            "                Gloves        125        236      0.963      0.691      0.849      0.599\n",
            "                Helmet        142        151      0.952      0.967      0.979       0.85\n",
            "                Person        113        141      0.924      0.864      0.961      0.811\n",
            "                  Vest         95        120      0.863      0.883      0.948      0.725\n",
            "Speed: 0.2ms preprocess, 2.3ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train14\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f77b3b137d0>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0049795,   0.0024897,           0],\n",
              "       [          1,           1,           1, ...,   0.0037007,   0.0018504,           0],\n",
              "       [          1,           1,           1, ...,    0.023277,    0.023277,           0],\n",
              "       [          1,           1,           1, ...,    0.036921,    0.036921,           0],\n",
              "       [          1,           1,           1, ...,    0.024925,    0.012462,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.095899,    0.095899,     0.17286, ...,           0,           0,           0],\n",
              "       [   0.089669,    0.089669,     0.19577, ...,           0,           0,           0],\n",
              "       [   0.017325,    0.017325,     0.22572, ...,     0.52033,     0.39404,           0],\n",
              "       [  0.0078806,   0.0078806,     0.15615, ...,           0,           0,           0],\n",
              "       [    0.18785,     0.18785,     0.35814, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.050417,    0.050417,    0.094856, ...,           1,           1,           1],\n",
              "       [   0.046996,    0.046996,     0.10897, ...,           1,           1,           1],\n",
              "       [  0.0087384,   0.0087384,     0.12732, ...,           1,           1,           1],\n",
              "       [  0.0039559,   0.0039559,    0.084741, ...,           1,           1,           1],\n",
              "       [    0.10375,     0.10375,     0.21853, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97973,     0.97973,     0.97297, ...,           0,           0,           0],\n",
              "       [    0.97458,     0.97458,     0.96186, ...,           0,           0,           0],\n",
              "       [          1,           1,     0.99338, ...,     0.35166,     0.24536,           0],\n",
              "       [          1,           1,     0.99291, ...,           0,           0,           0],\n",
              "       [    0.99167,     0.99167,     0.99167, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7327591226244281)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.57669,     0.59886,     0.84974,     0.81088,     0.72526])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.8981896233643584), 'metrics/recall(B)': np.float64(0.8349747507403789), 'metrics/mAP50(B)': np.float64(0.9170270197336304), 'metrics/mAP50-95(B)': np.float64(0.7122849118345167), 'fitness': np.float64(0.7327591226244281)}\n",
              "save_dir: PosixPath('runs/detect/train14')\n",
              "speed: {'preprocess': 0.22553043130172934, 'inference': 2.3428724122184312, 'loss': 0.0007400114541026436, 'postprocess': 4.55609893511264}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "model.train(data=r\"/content/PPE-Project-1/data.yaml\",\n",
        "            epochs=15,\n",
        "            lr0=0.00934,\n",
        "            momentum=0.87293,\n",
        "            weight_decay=0.00056,\n",
        "            iou=0.78859,\n",
        "            hsv_h=0.01402,\n",
        "            hsv_s=0.66495,\n",
        "            hsv_v=0.4,\n",
        "            degrees=0,\n",
        "            translate=0.08142,\n",
        "            scale=0.5,\n",
        "            shear=0.0,\n",
        "            perspective=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5XDj7wBfqgz"
      },
      "source": [
        "# Rt-Detr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 1: Utilizing Ultralytics\n"
      ],
      "metadata": {
        "id": "crxWLhDM8ohZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"sXoFafsVvntFkKTy4Z5x\")\n",
        "project = rf.workspace(\"randomlangyan\").project(\"ppe-project-7brvs\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhDvegvu9Mqa",
        "outputId": "31c28ea7-cf23-4eeb-cc01-d9ff52f0e32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.64-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "Downloading roboflow-1.1.64-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.64\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in PPE-Project-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64893/64893 [00:02<00:00, 26626.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to PPE-Project-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2810/2810 [00:01<00:00, 2279.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ultralytics\n",
        "!pip install torch torchvision matplotlib seaborn opencv-python pandas tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVeUAzeCDVy-",
        "outputId": "82b7248a-1fed-4d40-d56d-00213376a781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.136-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.136-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.136 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import RTDETR\n",
        "\n",
        "model_2 = RTDETR(\"rtdetr-l.pt\")\n",
        "model_2.train(data=\"/content/PPE-Project-1/data.yaml\", epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JofG1n9Z8v3S",
        "outputId": "2a093be0-98af-472d-b14e-342546737e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/rtdetr-l.pt to 'rtdetr-l.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.4M/63.4M [00:00<00:00, 100MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/PPE-Project-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=rtdetr-l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 21.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=5\n",
            "WARNING âš ï¸ no model scale passed. Assuming scale='l'.\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
            "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
            "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
            "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
            "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
            "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
            "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
            "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
            " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
            " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
            " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
            " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 28        [21, 24, 27]  1   7312127  ultralytics.nn.modules.head.RTDETRDecoder    [5, [256, 256, 256]]          \n",
            "rt-detr-l summary: 457 layers, 32,816,351 parameters, 32,816,351 gradients, 108.0 GFLOPs\n",
            "\n",
            "Transferred 926/941 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 102MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 20.1Â±6.4 MB/s, size: 44.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/PPE-Project-1/train/labels... 989 images, 142 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 989/989 [00:01<00:00, 794.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/PPE-Project-1/train/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 11.1Â±11.6 MB/s, size: 37.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<00:00, 573.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/PPE-Project-1/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       1/10      12.3G     0.6623      5.399     0.4222         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:12<00:00,  1.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:07<00:00,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796       0.66      0.798      0.769      0.536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       2/10      12.6G     0.4313     0.6732     0.2355         85        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:10<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.495      0.417      0.366      0.249\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       3/10      12.5G     0.4041     0.6307     0.2127         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.829       0.75       0.81      0.583\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       4/10      12.7G     0.3774     0.5509     0.2047         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.824      0.811      0.849      0.614\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       5/10      12.5G     0.3442     0.5048      0.181         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.845      0.845      0.885      0.658\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       6/10      12.5G     0.3192     0.4844     0.1648         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.868      0.839      0.899       0.67\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       7/10      12.5G     0.3065     0.4296     0.1503         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.881      0.863      0.913      0.685\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       8/10      12.5G      0.289      0.402     0.1364         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.902      0.874       0.93      0.706\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       9/10      12.5G     0.2823     0.3837     0.1329         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796       0.87      0.893      0.938      0.719\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      10/10      12.5G     0.2691     0.3725     0.1258         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.913      0.865      0.939      0.727\n",
            "\n",
            "10 epochs completed in 0.224 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 66.1MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 66.1MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,994,015 parameters, 0 gradients, 103.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.912      0.863      0.939      0.726\n",
            "                 Boots         75        148      0.837      0.901      0.926      0.592\n",
            "                Gloves        125        236      0.946      0.742      0.893      0.624\n",
            "                Helmet        142        151       0.98      0.967      0.978      0.829\n",
            "                Person        113        141       0.91      0.823      0.954      0.834\n",
            "                  Vest         95        120      0.888      0.883      0.945      0.751\n",
            "Speed: 0.3ms preprocess, 15.9ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e80885905d0>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0016011,  0.00080056,           0],\n",
              "       [          1,           1,           1, ...,     0.00295,    0.001475,           0],\n",
              "       [          1,           1,           1, ...,   0.0016797,  0.00083985,           0],\n",
              "       [          1,           1,           1, ...,     0.01116,     0.01116,           0],\n",
              "       [          1,           1,           1, ...,    0.020339,    0.020339,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.042291,    0.042291,    0.042291, ...,           0,           0,           0],\n",
              "       [   0.024664,    0.024664,    0.024664, ...,           0,           0,           0],\n",
              "       [   0.021978,    0.021978,    0.021978, ...,           0,           0,           0],\n",
              "       [  0.0097047,   0.0097047,   0.0097047, ...,           0,           0,           0],\n",
              "       [    0.02183,     0.02183,     0.02183, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.021615,    0.021615,    0.021615, ...,           1,           1,           1],\n",
              "       [   0.012487,    0.012487,    0.012487, ...,           1,           1,           1],\n",
              "       [   0.011113,    0.011113,    0.011113, ...,           1,           1,           1],\n",
              "       [   0.004876,    0.004876,    0.004876, ...,           1,           1,           1],\n",
              "       [   0.011035,    0.011035,    0.011035, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97297,     0.97297,     0.97297, ...,           0,           0,           0],\n",
              "       [    0.99153,     0.99153,     0.99153, ...,           0,           0,           0],\n",
              "       [    0.98675,     0.98675,     0.98675, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7474164746437102)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.59241,     0.62442,     0.82852,     0.83396,      0.7511])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.9122395493655973), 'metrics/recall(B)': np.float64(0.8631103559542342), 'metrics/mAP50(B)': np.float64(0.9394358488894937), 'metrics/mAP50-95(B)': np.float64(0.7260809886164009), 'fitness': np.float64(0.7474164746437102)}\n",
              "save_dir: PosixPath('runs/detect/train')\n",
              "speed: {'preprocess': 0.2937886297704886, 'inference': 15.918959145037766, 'loss': 0.0006444885498969168, 'postprocess': 0.6607593511454826}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.val(data=\"/content/PPE-Project-1/data.yaml\", epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXtVXwVLKGFR",
        "outputId": "9660443c-3fc9-4a12-98bc-1e434c12fe1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,994,015 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1356.0Â±469.4 MB/s, size: 42.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels.cache... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:11<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.913      0.864      0.938      0.726\n",
            "                 Boots         75        148      0.837      0.905      0.926      0.594\n",
            "                Gloves        125        236      0.946      0.741      0.889      0.626\n",
            "                Helmet        142        151      0.979      0.967      0.978      0.828\n",
            "                Person        113        141      0.914      0.826      0.954      0.833\n",
            "                  Vest         95        120      0.889      0.883      0.945      0.749\n",
            "Speed: 2.2ms preprocess, 34.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e808826d890>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0016186,  0.00080931,           0],\n",
              "       [          1,           1,           1, ...,   0.0029604,   0.0014802,           0],\n",
              "       [          1,           1,           1, ...,   0.0016904,  0.00084521,           0],\n",
              "       [          1,           1,           1, ...,    0.012544,    0.012544,           0],\n",
              "       [          1,           1,           1, ...,    0.020267,    0.020267,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.042743,    0.042743,    0.042743, ...,           0,           0,           0],\n",
              "       [    0.02475,     0.02475,     0.02475, ...,           0,           0,           0],\n",
              "       [   0.022117,    0.022117,    0.022117, ...,           0,           0,           0],\n",
              "       [  0.0096844,   0.0096844,   0.0096844, ...,           0,           0,           0],\n",
              "       [   0.021513,    0.021513,    0.021513, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.021851,    0.021851,    0.021851, ...,           1,           1,           1],\n",
              "       [   0.012531,    0.012531,    0.012531, ...,           1,           1,           1],\n",
              "       [   0.011184,    0.011184,    0.011184, ...,           1,           1,           1],\n",
              "       [  0.0048658,   0.0048658,   0.0048658, ...,           1,           1,           1],\n",
              "       [   0.010874,    0.010874,    0.010874, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97297,     0.97297,     0.97297, ...,           0,           0,           0],\n",
              "       [    0.99153,     0.99153,     0.99153, ...,           0,           0,           0],\n",
              "       [    0.98675,     0.98675,     0.98675, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7472693899948186)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.59426,     0.62583,     0.82807,     0.83322,     0.74874])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.9130858480048761), 'metrics/recall(B)': np.float64(0.8644968323295125), 'metrics/mAP50(B)': np.float64(0.9384770183428623), 'metrics/mAP50-95(B)': np.float64(0.7260240979561471), 'fitness': np.float64(0.7472693899948186)}\n",
              "save_dir: PosixPath('runs/detect/train2')\n",
              "speed: {'preprocess': 2.1988441603058746, 'inference': 34.051901958013545, 'loss': 0.0007868473269857928, 'postprocess': 1.1558018320626013}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = RTDETR('/content/rtdetr-l.pt')\n",
        "\n",
        "model_2.tune(\n",
        "    data=r\"/content/PPE-Project-1/data.yaml\",\n",
        "    epochs=15,\n",
        "    iterations=10,\n",
        "    val=True,\n",
        "    space={\n",
        "        'lr0': (1e-4, 1e-2),\n",
        "        'momentum': (0.8, 0.98),\n",
        "        'weight_decay': (0.0001, 0.01),\n",
        "        'iou': (0.1, 0.95),\n",
        "        'hsv_h': (0.0, 0.015),\n",
        "        'hsv_s': (0.4, 0.8),\n",
        "        'hsv_v': (0.4, 0.8),\n",
        "        'degrees': (0.0, 10.0),\n",
        "        'translate': (0.0, 0.2),\n",
        "        'scale': (0.5, 1.5),\n",
        "        'shear': (0.0, 2.0),\n",
        "        'perspective': (0.0, 0.001),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9yTm-6LO93m",
        "outputId": "b299e454-0ebd-4107-e4fe-fd1e942319ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=runs/detect/tune'\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mðŸ’¡ Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'iou': 0.7, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 1\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.7', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.01', 'lrf=0.01', 'momentum=0.937', 'weight_decay=0.0005', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.7', 'hsv_v=0.4', 'degrees=0.0', 'translate=0.1', 'scale=0.5', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m1/10 iterations complete âœ… (19.38s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 2/10 with hyperparameters: {'lr0': 0.00991, 'momentum': 0.8, 'weight_decay': 0.00049, 'iou': 0.77274, 'hsv_h': 0.01455, 'hsv_s': 0.63422, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.12281, 'scale': 0.5349, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 2\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.77274', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00991', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.00049', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.01455', 'hsv_s=0.63422', 'hsv_v=0.4', 'degrees=0.0', 'translate=0.12281', 'scale=0.5349', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m2/10 iterations complete âœ… (36.90s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 3/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.8, 'weight_decay': 0.00042, 'iou': 0.68708, 'hsv_h': 0.015, 'hsv_s': 0.71918, 'hsv_v': 0.49972, 'degrees': 0.0, 'translate': 0.0934, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 3\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.68708', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.01', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.00042', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.71918', 'hsv_v=0.49972', 'degrees=0.0', 'translate=0.0934', 'scale=0.5', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m3/10 iterations complete âœ… (54.88s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 4/10 with hyperparameters: {'lr0': 0.00958, 'momentum': 0.8, 'weight_decay': 0.0005, 'iou': 0.58037, 'hsv_h': 0.015, 'hsv_s': 0.76305, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.09779, 'scale': 0.55646, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 4\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.58037', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00958', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.0005', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.76305', 'hsv_v=0.4', 'degrees=0.0', 'translate=0.09779', 'scale=0.55646', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m4/10 iterations complete âœ… (73.71s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 5/10 with hyperparameters: {'lr0': 0.00936, 'momentum': 0.8, 'weight_decay': 0.0005, 'iou': 0.58037, 'hsv_h': 0.01492, 'hsv_s': 0.76305, 'hsv_v': 0.40872, 'degrees': 0.0, 'translate': 0.09707, 'scale': 0.55512, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 5\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.58037', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00936', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.0005', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.01492', 'hsv_s=0.76305', 'hsv_v=0.40872', 'degrees=0.0', 'translate=0.09707', 'scale=0.55512', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m5/10 iterations complete âœ… (90.81s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 6/10 with hyperparameters: {'lr0': 0.00866, 'momentum': 0.8, 'weight_decay': 0.00042, 'iou': 0.49341, 'hsv_h': 0.01328, 'hsv_s': 0.8, 'hsv_v': 0.40872, 'degrees': 0.0, 'translate': 0.10173, 'scale': 0.65815, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 6\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.49341', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00866', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.00042', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.01328', 'hsv_s=0.8', 'hsv_v=0.40872', 'degrees=0.0', 'translate=0.10173', 'scale=0.65815', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m6/10 iterations complete âœ… (109.47s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 7/10 with hyperparameters: {'lr0': 0.00931, 'momentum': 0.85451, 'weight_decay': 0.00045, 'iou': 0.68869, 'hsv_h': 0.015, 'hsv_s': 0.8, 'hsv_v': 0.50406, 'degrees': 0.0, 'translate': 0.0934, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 7\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.68869', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00931', 'lrf=0.01', 'momentum=0.85451', 'weight_decay=0.00045', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.8', 'hsv_v=0.50406', 'degrees=0.0', 'translate=0.0934', 'scale=0.5', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m7/10 iterations complete âœ… (126.09s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 8/10 with hyperparameters: {'lr0': 0.00916, 'momentum': 0.98, 'weight_decay': 0.00036, 'iou': 0.56181, 'hsv_h': 0.015, 'hsv_s': 0.8, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.09097, 'scale': 0.57956, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 8\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.56181', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.00916', 'lrf=0.01', 'momentum=0.98', 'weight_decay=0.00036', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.8', 'hsv_v=0.4', 'degrees=0.0', 'translate=0.09097', 'scale=0.57956', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m8/10 iterations complete âœ… (146.79s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 9/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.8, 'weight_decay': 0.00051, 'iou': 0.53619, 'hsv_h': 0.015, 'hsv_s': 0.71742, 'hsv_v': 0.41028, 'degrees': 0.0, 'translate': 0.10203, 'scale': 0.52512, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 9\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.53619', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.01', 'lrf=0.01', 'momentum=0.8', 'weight_decay=0.00051', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.71742', 'hsv_v=0.41028', 'degrees=0.0', 'translate=0.10203', 'scale=0.52512', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m9/10 iterations complete âœ… (163.43s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 10/10 with hyperparameters: {'lr0': 0.01, 'momentum': 0.98, 'weight_decay': 0.0005, 'iou': 0.64923, 'hsv_h': 0.015, 'hsv_s': 0.62416, 'hsv_v': 0.40218, 'degrees': 0.0, 'translate': 0.09485, 'scale': 0.5614, 'shear': 0.0, 'perspective': 0.0}\n",
            "ERROR âŒ training failure for hyperparameter tuning iteration 10\n",
            "Command '['/usr/bin/python3', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/content/rtdetr-l.pt', 'data=/content/PPE-Project-1/data.yaml', 'epochs=15', 'time=None', 'patience=100', 'batch=16', 'imgsz=640', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=auto', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.64923', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.01', 'lrf=0.01', 'momentum=0.98', 'weight_decay=0.0005', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.62416', 'hsv_v=0.40218', 'degrees=0.0', 'translate=0.09485', 'scale=0.5614', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune/tune_scatter_plots.png\n",
            "Saved runs/detect/tune/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m10/10 iterations complete âœ… (180.93s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train3\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "momentum: 0.937\n",
            "weight_decay: 0.0005\n",
            "iou: 0.7\n",
            "hsv_h: 0.015\n",
            "hsv_s: 0.7\n",
            "hsv_v: 0.4\n",
            "degrees: 0.0\n",
            "translate: 0.1\n",
            "scale: 0.5\n",
            "shear: 0.0\n",
            "perspective: 0.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = RTDETR('/content/rtdetr-l.pt')\n",
        "model_2.train(data=r\"/content/PPE-Project-1/data.yaml\",\n",
        "            epochs=15,\n",
        "            lr0=0.01,\n",
        "            momentum=0.937,\n",
        "            weight_decay=0.0005,\n",
        "            iou=0.7,\n",
        "            hsv_h=0.015,\n",
        "            hsv_s=0.7,\n",
        "            hsv_v=0.4,\n",
        "            degrees=0.0,\n",
        "            translate=0.1,\n",
        "            scale=0.5,\n",
        "            shear=0.0,\n",
        "            perspective=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odSPFU54seOX",
        "outputId": "1a98d721-f231-459e-da29-e597675caf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/PPE-Project-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/rtdetr-l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train13, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train13, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "WARNING âš ï¸ no model scale passed. Assuming scale='l'.\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
            "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
            "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
            "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
            "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
            "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
            "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
            "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
            " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
            " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
            " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
            " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
            " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 28        [21, 24, 27]  1   7312127  ultralytics.nn.modules.head.RTDETRDecoder    [5, [256, 256, 256]]          \n",
            "rt-detr-l summary: 457 layers, 32,816,351 parameters, 32,816,351 gradients, 108.0 GFLOPs\n",
            "\n",
            "Transferred 926/941 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1444.1Â±543.8 MB/s, size: 44.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/PPE-Project-1/train/labels.cache... 989 images, 142 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 989/989 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 339.0Â±130.0 MB/s, size: 37.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels.cache... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train13/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train13\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       1/15      12.5G     0.7422      3.791     0.5268         71        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:11<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.654      0.769      0.756       0.53\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       2/15      12.5G     0.4134     0.6259     0.2129         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:10<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.858      0.787      0.845      0.615\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       3/15      12.4G     0.3998     0.5819     0.1996         67        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.855      0.844      0.893      0.642\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       4/15      12.4G     0.4051     0.5434     0.2197         86        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.824      0.833      0.873      0.634\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       5/15      12.5G      0.375      0.546     0.1823         73        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.818      0.807      0.869      0.638\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       6/15      12.5G     0.3496     0.5331     0.1908         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:09<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.883      0.799      0.854      0.624\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       7/15      12.5G     0.3307     0.4797     0.1734         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.898      0.796      0.889      0.659\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       8/15      12.5G     0.3228     0.4464     0.1691         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.883      0.872       0.91      0.684\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       9/15      12.5G     0.3012     0.4291     0.1461         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.886      0.876      0.919      0.704\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      10/15      12.5G     0.2932     0.4181     0.1454         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.884      0.887      0.934      0.708\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      11/15      12.5G     0.2769      0.392     0.1321         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.892      0.907      0.938      0.722\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      12/15      12.5G     0.2755     0.3698     0.1298         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.903      0.893      0.937      0.719\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      13/15      12.5G     0.2676     0.3547     0.1263         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.914      0.892      0.939      0.724\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      14/15      12.5G     0.2625     0.3472     0.1251         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.906        0.9      0.938      0.728\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/62 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      15/15      12.7G     0.2583     0.3357     0.1205         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:08<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.919      0.883      0.942      0.734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15 epochs completed in 0.350 hours.\n",
            "Optimizer stripped from runs/detect/train13/weights/last.pt, 66.1MB\n",
            "Optimizer stripped from runs/detect/train13/weights/best.pt, 66.1MB\n",
            "\n",
            "Validating runs/detect/train13/weights/best.pt...\n",
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,994,015 parameters, 0 gradients, 103.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.919      0.883      0.942      0.734\n",
            "                 Boots         75        148       0.88      0.912      0.919      0.582\n",
            "                Gloves        125        236      0.945      0.729      0.899      0.649\n",
            "                Helmet        142        151      0.948      0.967      0.976      0.838\n",
            "                Person        113        141      0.942      0.872      0.968      0.847\n",
            "                  Vest         95        120      0.879      0.933      0.947      0.752\n",
            "Speed: 0.3ms preprocess, 17.3ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train13\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e8116837ed0>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0018032,   0.0009016,           0],\n",
              "       [          1,           1,           1, ...,  0.00079304,  0.00039652,           0],\n",
              "       [          1,           1,           1, ...,   0.0018541,  0.00092704,           0],\n",
              "       [          1,           1,           1, ...,   0.0060492,   0.0030246,           0],\n",
              "       [          1,           1,           1, ...,    0.094414,    0.094414,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.035847,    0.035847,    0.035847, ...,           0,           0,           0],\n",
              "       [   0.013337,    0.013337,    0.013337, ...,           0,           0,           0],\n",
              "       [   0.024232,    0.024232,    0.024232, ...,           0,           0,           0],\n",
              "       [   0.041954,    0.041954,    0.041954, ...,           0,           0,           0],\n",
              "       [   0.013681,    0.013681,    0.013681, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.018257,    0.018257,    0.018257, ...,           1,           1,           1],\n",
              "       [  0.0067139,   0.0067139,   0.0067139, ...,           1,           1,           1],\n",
              "       [   0.012266,    0.012266,    0.012266, ...,           1,           1,           1],\n",
              "       [    0.02143,     0.02143,     0.02143, ...,           1,           1,           1],\n",
              "       [  0.0068874,   0.0068874,   0.0068874, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97973,     0.97973,     0.97973, ...,           0,           0,           0],\n",
              "       [    0.98305,     0.98305,     0.98305, ...,           0,           0,           0],\n",
              "       [    0.98675,     0.98675,     0.98675, ...,           0,           0,           0],\n",
              "       [    0.99291,     0.99291,     0.99291, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7545447485453652)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.58216,     0.64942,     0.83807,     0.84731,     0.75168])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.9185820392021133), 'metrics/recall(B)': np.float64(0.8826635220968587), 'metrics/mAP50(B)': np.float64(0.9418827536354201), 'metrics/mAP50-95(B)': np.float64(0.7337294146464702), 'fitness': np.float64(0.7545447485453652)}\n",
              "save_dir: PosixPath('runs/detect/train13')\n",
              "speed: {'preprocess': 0.26705943893295825, 'inference': 17.324900832058233, 'loss': 0.0003885458060534705, 'postprocess': 1.073854610688037}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = RTDETR('/content/runs/detect/train13/weights/best.pt')\n",
        "model_2.val(data=\"/content/PPE-Project-1/data.yaml\", epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsw8LhgbD4iI",
        "outputId": "4cb47ae0-1aea-4f83-baba-433d1233eff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.136 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,994,015 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1249.2Â±387.3 MB/s, size: 33.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/PPE-Project-1/valid/labels.cache... 262 images, 30 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262/262 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:11<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        262        796      0.918      0.883      0.942      0.734\n",
            "                 Boots         75        148      0.883      0.912      0.919      0.587\n",
            "                Gloves        125        236      0.945      0.729      0.899      0.648\n",
            "                Helmet        142        151      0.948      0.967      0.976      0.836\n",
            "                Person        113        141      0.938      0.872      0.968      0.846\n",
            "                  Vest         95        120      0.879      0.933      0.947      0.752\n",
            "Speed: 1.0ms preprocess, 36.0ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 3, 4])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e81166df850>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0018197,  0.00090985,           0],\n",
              "       [          1,           1,           1, ...,   0.0007946,   0.0003973,           0],\n",
              "       [          1,           1,           1, ...,   0.0018535,  0.00092674,           0],\n",
              "       [          1,           1,           1, ...,   0.0060308,   0.0030154,           0],\n",
              "       [          1,           1,           1, ...,    0.092664,    0.092664,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.036169,    0.036169,    0.036169, ...,           0,           0,           0],\n",
              "       [   0.013363,    0.013363,    0.013363, ...,           0,           0,           0],\n",
              "       [   0.024224,    0.024224,    0.024224, ...,           0,           0,           0],\n",
              "       [   0.041829,    0.041829,    0.041829, ...,           0,           0,           0],\n",
              "       [   0.013591,    0.013591,    0.013591, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.018424,    0.018424,    0.018424, ...,           1,           1,           1],\n",
              "       [  0.0067272,   0.0067272,   0.0067272, ...,           1,           1,           1],\n",
              "       [   0.012262,    0.012262,    0.012262, ...,           1,           1,           1],\n",
              "       [   0.021364,    0.021364,    0.021364, ...,           1,           1,           1],\n",
              "       [  0.0068419,   0.0068419,   0.0068419, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97973,     0.97973,     0.97973, ...,           0,           0,           0],\n",
              "       [    0.98305,     0.98305,     0.98305, ...,           0,           0,           0],\n",
              "       [    0.98675,     0.98675,     0.98675, ...,           0,           0,           0],\n",
              "       [    0.99291,     0.99291,     0.99291, ...,           0,           0,           0],\n",
              "       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.7545603508367447)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.58724,     0.64775,     0.83564,     0.84584,     0.75223])\n",
              "names: {0: 'Boots', 1: 'Gloves', 2: 'Helmet', 3: 'Person', 4: 'Vest'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.9184547939239293), 'metrics/recall(B)': np.float64(0.8826718089334553), 'metrics/mAP50(B)': np.float64(0.9419595422975109), 'metrics/mAP50-95(B)': np.float64(0.733738218452215), 'fitness': np.float64(0.7545603508367447)}\n",
              "save_dir: PosixPath('runs/detect/val2')\n",
              "speed: {'preprocess': 1.0164103396962547, 'inference': 36.03215899618361, 'loss': 0.0007421488534356557, 'postprocess': 0.5726467786227645}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W46ShdDxfqgz"
      },
      "source": [
        "# RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miscellaneous"
      ],
      "metadata": {
        "id": "VPnkGXGbyqkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtQq7l-Mfqgz",
        "outputId": "ee87c57b-cd1f-49b6-9867-fc8618c9188b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-retinanet'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Total 232 (delta 0), reused 0 (delta 0), pack-reused 232 (from 2)\u001b[K\n",
            "Receiving objects: 100% (232/232), 1.02 MiB | 3.18 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yhenon/pytorch-retinanet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"sXoFafsVvntFkKTy4Z5x\")\n",
        "project = rf.workspace(\"randomlangyan\").project(\"ppe-project-7brvs\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ],
      "metadata": {
        "id": "bTdB4JziotIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813e417e-3092-4565-911d-cf86e611a0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.64)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in PPE-Project-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64893/64893 [00:01<00:00, 47341.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to PPE-Project-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2810/2810 [00:00<00:00, 7217.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"sXoFafsVvntFkKTy4Z5x\")\n",
        "project = rf.workspace(\"randomlangyan\").project(\"ppe-project-7brvs\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco-mmdetection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIvDydL6e2r8",
        "outputId": "5073e646-277e-4834-ac2d-c40bd3f7f0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.64-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "Downloading roboflow-1.1.64-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.64\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in PPE-Project-1 to coco-mmdetection:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64564/64564 [00:05<00:00, 12895.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to PPE-Project-1 in coco-mmdetection:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1407/1407 [00:00<00:00, 3115.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ3ORObMfqg1"
      },
      "source": [
        "## Attempt 1: Base Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "%cd /content/pytorch-retinanet\n",
        "!python train.py --dataset coco --coco_path \"/content/PPE-Project-1\" --depth 50 --epochs 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8C451p3kDwo",
        "outputId": "4c65fb05-15c5-4096-8732-3127b1ad40d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 0 | Iteration: 188 | Classification loss: 0.02802 | Regression loss: 0.09448 | Running loss: 1.49952\n",
            "Epoch: 0 | Iteration: 189 | Classification loss: 0.22616 | Regression loss: 0.38245 | Running loss: 1.49483\n",
            "Epoch: 0 | Iteration: 190 | Classification loss: 0.70193 | Regression loss: 1.06145 | Running loss: 1.49623\n",
            "Epoch: 0 | Iteration: 191 | Classification loss: 0.26807 | Regression loss: 0.66634 | Running loss: 1.49331\n",
            "Epoch: 0 | Iteration: 192 | Classification loss: 0.28740 | Regression loss: 0.00000 | Running loss: 1.48706\n",
            "Epoch: 0 | Iteration: 193 | Classification loss: 0.43226 | Regression loss: 0.78286 | Running loss: 1.48566\n",
            "Epoch: 0 | Iteration: 194 | Classification loss: 0.78047 | Regression loss: 0.93185 | Running loss: 1.48682\n",
            "Epoch: 0 | Iteration: 195 | Classification loss: 0.40633 | Regression loss: 0.61128 | Running loss: 1.48443\n",
            "Epoch: 0 | Iteration: 196 | Classification loss: 0.16290 | Regression loss: 0.00000 | Running loss: 1.47772\n",
            "Epoch: 0 | Iteration: 197 | Classification loss: 0.28010 | Regression loss: 0.57274 | Running loss: 1.47456\n",
            "Epoch: 0 | Iteration: 198 | Classification loss: 0.39646 | Regression loss: 0.76054 | Running loss: 1.47297\n",
            "Epoch: 0 | Iteration: 199 | Classification loss: 0.65057 | Regression loss: 0.95377 | Running loss: 1.47362\n",
            "Epoch: 0 | Iteration: 200 | Classification loss: 0.39590 | Regression loss: 0.62679 | Running loss: 1.47138\n",
            "Epoch: 0 | Iteration: 201 | Classification loss: 0.07435 | Regression loss: 0.26638 | Running loss: 1.46578\n",
            "Epoch: 0 | Iteration: 202 | Classification loss: 0.71368 | Regression loss: 0.92630 | Running loss: 1.46664\n",
            "Epoch: 0 | Iteration: 203 | Classification loss: 0.31363 | Regression loss: 0.44989 | Running loss: 1.46319\n",
            "Epoch: 0 | Iteration: 204 | Classification loss: 0.47995 | Regression loss: 0.85843 | Running loss: 1.46259\n",
            "Epoch: 0 | Iteration: 205 | Classification loss: 0.66229 | Regression loss: 0.94744 | Running loss: 1.46330\n",
            "Epoch: 0 | Iteration: 206 | Classification loss: 0.57314 | Regression loss: 0.00000 | Running loss: 1.45900\n",
            "Epoch: 0 | Iteration: 207 | Classification loss: 0.28032 | Regression loss: 0.69845 | Running loss: 1.45669\n",
            "Epoch: 0 | Iteration: 208 | Classification loss: 0.26153 | Regression loss: 0.55240 | Running loss: 1.45362\n",
            "Epoch: 0 | Iteration: 209 | Classification loss: 0.65692 | Regression loss: 0.44234 | Running loss: 1.45193\n",
            "Epoch: 0 | Iteration: 210 | Classification loss: 0.19879 | Regression loss: 0.54033 | Running loss: 1.44855\n",
            "Epoch: 0 | Iteration: 211 | Classification loss: 0.79217 | Regression loss: 0.90286 | Running loss: 1.44971\n",
            "Epoch: 0 | Iteration: 212 | Classification loss: 0.31475 | Regression loss: 0.54335 | Running loss: 1.44694\n",
            "Epoch: 0 | Iteration: 213 | Classification loss: 0.21198 | Regression loss: 0.48084 | Running loss: 1.44341\n",
            "Epoch: 0 | Iteration: 214 | Classification loss: 0.29072 | Regression loss: 0.68550 | Running loss: 1.44124\n",
            "Epoch: 0 | Iteration: 215 | Classification loss: 0.66284 | Regression loss: 0.91136 | Running loss: 1.44185\n",
            "Epoch: 0 | Iteration: 216 | Classification loss: 0.47561 | Regression loss: 0.56747 | Running loss: 1.44002\n",
            "Epoch: 0 | Iteration: 217 | Classification loss: 0.48875 | Regression loss: 0.56719 | Running loss: 1.43825\n",
            "Epoch: 0 | Iteration: 218 | Classification loss: 0.50967 | Regression loss: 0.85255 | Running loss: 1.43791\n",
            "Epoch: 0 | Iteration: 219 | Classification loss: 0.16218 | Regression loss: 0.48184 | Running loss: 1.43430\n",
            "Epoch: 0 | Iteration: 220 | Classification loss: 0.06645 | Regression loss: 0.09982 | Running loss: 1.42856\n",
            "Epoch: 0 | Iteration: 221 | Classification loss: 0.28779 | Regression loss: 0.45595 | Running loss: 1.42548\n",
            "Epoch: 0 | Iteration: 222 | Classification loss: 0.36241 | Regression loss: 0.75436 | Running loss: 1.42409\n",
            "Epoch: 0 | Iteration: 223 | Classification loss: 0.40007 | Regression loss: 0.83302 | Running loss: 1.42324\n",
            "Epoch: 0 | Iteration: 224 | Classification loss: 0.74216 | Regression loss: 0.97114 | Running loss: 1.42453\n",
            "Epoch: 0 | Iteration: 225 | Classification loss: 1.43894 | Regression loss: 0.35106 | Running loss: 1.42615\n",
            "Epoch: 0 | Iteration: 226 | Classification loss: 0.29830 | Regression loss: 0.50685 | Running loss: 1.42341\n",
            "Epoch: 0 | Iteration: 227 | Classification loss: 0.60903 | Regression loss: 0.57364 | Running loss: 1.42235\n",
            "Epoch: 0 | Iteration: 228 | Classification loss: 0.77624 | Regression loss: 1.02559 | Running loss: 1.42401\n",
            "Epoch: 0 | Iteration: 229 | Classification loss: 0.28253 | Regression loss: 0.48796 | Running loss: 1.42117\n",
            "Epoch: 0 | Iteration: 230 | Classification loss: 0.01834 | Regression loss: 0.12307 | Running loss: 1.41563\n",
            "Epoch: 0 | Iteration: 231 | Classification loss: 0.24127 | Regression loss: 0.46187 | Running loss: 1.41256\n",
            "Epoch: 0 | Iteration: 232 | Classification loss: 0.20264 | Regression loss: 0.44309 | Running loss: 1.40927\n",
            "Epoch: 0 | Iteration: 233 | Classification loss: 0.44048 | Regression loss: 0.63063 | Running loss: 1.40782\n",
            "Epoch: 0 | Iteration: 234 | Classification loss: 0.46943 | Regression loss: 0.59259 | Running loss: 1.40635\n",
            "Epoch: 0 | Iteration: 235 | Classification loss: 0.82922 | Regression loss: 0.71400 | Running loss: 1.40693\n",
            "Epoch: 0 | Iteration: 236 | Classification loss: 0.52035 | Regression loss: 0.75709 | Running loss: 1.40638\n",
            "Epoch: 0 | Iteration: 237 | Classification loss: 1.08230 | Regression loss: 0.50513 | Running loss: 1.40714\n",
            "Epoch: 0 | Iteration: 238 | Classification loss: 0.58341 | Regression loss: 1.02012 | Running loss: 1.40797\n",
            "Epoch: 0 | Iteration: 239 | Classification loss: 0.70519 | Regression loss: 0.34739 | Running loss: 1.40649\n",
            "Epoch: 0 | Iteration: 240 | Classification loss: 0.32135 | Regression loss: 0.52294 | Running loss: 1.40415\n",
            "Epoch: 0 | Iteration: 241 | Classification loss: 0.52151 | Regression loss: 0.85690 | Running loss: 1.40405\n",
            "Epoch: 0 | Iteration: 242 | Classification loss: 0.38152 | Regression loss: 0.71926 | Running loss: 1.40280\n",
            "Epoch: 0 | Iteration: 243 | Classification loss: 0.35952 | Regression loss: 0.43170 | Running loss: 1.40029\n",
            "Epoch: 0 | Iteration: 244 | Classification loss: 0.97772 | Regression loss: 0.10205 | Running loss: 1.39898\n",
            "Epoch: 0 | Iteration: 245 | Classification loss: 0.30275 | Regression loss: 0.46366 | Running loss: 1.39641\n",
            "Epoch: 0 | Iteration: 246 | Classification loss: 0.72511 | Regression loss: 1.00464 | Running loss: 1.39776\n",
            "Epoch: 0 | Iteration: 247 | Classification loss: 1.25105 | Regression loss: 0.10968 | Running loss: 1.39761\n",
            "Epoch: 0 | Iteration: 248 | Classification loss: 0.38052 | Regression loss: 0.75505 | Running loss: 1.39656\n",
            "Epoch: 0 | Iteration: 249 | Classification loss: 0.67469 | Regression loss: 0.58472 | Running loss: 1.39601\n",
            "Epoch: 0 | Iteration: 250 | Classification loss: 0.13437 | Regression loss: 0.00000 | Running loss: 1.39099\n",
            "Epoch: 0 | Iteration: 251 | Classification loss: 0.22324 | Regression loss: 0.42787 | Running loss: 1.38805\n",
            "Epoch: 0 | Iteration: 252 | Classification loss: 0.35098 | Regression loss: 0.56756 | Running loss: 1.38619\n",
            "Epoch: 0 | Iteration: 253 | Classification loss: 0.53059 | Regression loss: 0.89117 | Running loss: 1.38633\n",
            "Epoch: 0 | Iteration: 254 | Classification loss: 0.06668 | Regression loss: 0.00000 | Running loss: 1.38116\n",
            "Epoch: 0 | Iteration: 255 | Classification loss: 0.31252 | Regression loss: 0.36779 | Running loss: 1.37842\n",
            "Epoch: 0 | Iteration: 256 | Classification loss: 0.13930 | Regression loss: 0.08312 | Running loss: 1.37392\n",
            "Epoch: 0 | Iteration: 257 | Classification loss: 0.33610 | Regression loss: 0.76547 | Running loss: 1.37287\n",
            "Epoch: 0 | Iteration: 258 | Classification loss: 0.70315 | Regression loss: 0.45260 | Running loss: 1.37203\n",
            "Epoch: 0 | Iteration: 259 | Classification loss: 0.44089 | Regression loss: 0.73134 | Running loss: 1.37126\n",
            "Epoch: 0 | Iteration: 260 | Classification loss: 0.23018 | Regression loss: 0.32780 | Running loss: 1.36814\n",
            "Epoch: 0 | Iteration: 261 | Classification loss: 0.72826 | Regression loss: 0.75318 | Running loss: 1.36858\n",
            "Epoch: 0 | Iteration: 262 | Classification loss: 0.33967 | Regression loss: 0.58021 | Running loss: 1.36687\n",
            "Epoch: 0 | Iteration: 263 | Classification loss: 0.40390 | Regression loss: 0.93512 | Running loss: 1.36677\n",
            "Epoch: 0 | Iteration: 264 | Classification loss: 0.36263 | Regression loss: 0.82685 | Running loss: 1.36610\n",
            "Epoch: 0 | Iteration: 265 | Classification loss: 0.26616 | Regression loss: 0.60901 | Running loss: 1.36425\n",
            "Epoch: 0 | Iteration: 266 | Classification loss: 0.40636 | Regression loss: 0.54513 | Running loss: 1.36270\n",
            "Epoch: 0 | Iteration: 267 | Classification loss: 1.12950 | Regression loss: 0.87828 | Running loss: 1.36511\n",
            "Epoch: 0 | Iteration: 268 | Classification loss: 0.37441 | Regression loss: 0.50593 | Running loss: 1.36331\n",
            "Epoch: 0 | Iteration: 269 | Classification loss: 0.39171 | Regression loss: 0.67891 | Running loss: 1.36223\n",
            "Epoch: 0 | Iteration: 270 | Classification loss: 0.64346 | Regression loss: 0.89999 | Running loss: 1.36289\n",
            "Epoch: 0 | Iteration: 271 | Classification loss: 0.41284 | Regression loss: 0.69987 | Running loss: 1.36197\n",
            "Epoch: 0 | Iteration: 272 | Classification loss: 0.55609 | Regression loss: 0.74317 | Running loss: 1.36174\n",
            "Epoch: 0 | Iteration: 273 | Classification loss: 0.21125 | Regression loss: 0.40519 | Running loss: 1.35902\n",
            "Epoch: 0 | Iteration: 274 | Classification loss: 0.23793 | Regression loss: 0.65203 | Running loss: 1.35732\n",
            "Epoch: 0 | Iteration: 275 | Classification loss: 0.07089 | Regression loss: 0.05682 | Running loss: 1.35286\n",
            "Epoch: 0 | Iteration: 276 | Classification loss: 0.48534 | Regression loss: 0.64542 | Running loss: 1.35206\n",
            "Epoch: 0 | Iteration: 277 | Classification loss: 0.60639 | Regression loss: 0.77212 | Running loss: 1.35216\n",
            "Epoch: 0 | Iteration: 278 | Classification loss: 1.55489 | Regression loss: 0.75073 | Running loss: 1.35557\n",
            "Epoch: 0 | Iteration: 279 | Classification loss: 0.28833 | Regression loss: 0.07881 | Running loss: 1.35204\n",
            "Epoch: 0 | Iteration: 280 | Classification loss: 0.49053 | Regression loss: 1.05816 | Running loss: 1.35274\n",
            "Epoch: 0 | Iteration: 281 | Classification loss: 0.21474 | Regression loss: 0.35683 | Running loss: 1.34997\n",
            "Epoch: 0 | Iteration: 282 | Classification loss: 0.67990 | Regression loss: 0.83627 | Running loss: 1.35056\n",
            "Epoch: 0 | Iteration: 283 | Classification loss: 0.02641 | Regression loss: 0.09525 | Running loss: 1.34623\n",
            "Epoch: 0 | Iteration: 284 | Classification loss: 0.18154 | Regression loss: 0.37299 | Running loss: 1.34346\n",
            "Epoch: 0 | Iteration: 285 | Classification loss: 0.04694 | Regression loss: 0.02777 | Running loss: 1.33902\n",
            "Epoch: 0 | Iteration: 286 | Classification loss: 0.61820 | Regression loss: 0.86958 | Running loss: 1.33954\n",
            "Epoch: 0 | Iteration: 287 | Classification loss: 0.73624 | Regression loss: 0.91955 | Running loss: 1.34064\n",
            "Epoch: 0 | Iteration: 288 | Classification loss: 0.40417 | Regression loss: 0.72559 | Running loss: 1.33991\n",
            "Epoch: 0 | Iteration: 289 | Classification loss: 0.85392 | Regression loss: 0.08139 | Running loss: 1.33851\n",
            "Epoch: 0 | Iteration: 290 | Classification loss: 0.32671 | Regression loss: 0.69793 | Running loss: 1.33743\n",
            "Epoch: 0 | Iteration: 291 | Classification loss: 0.05809 | Regression loss: 0.24069 | Running loss: 1.33388\n",
            "Epoch: 0 | Iteration: 292 | Classification loss: 0.24473 | Regression loss: 0.48828 | Running loss: 1.33183\n",
            "Epoch: 0 | Iteration: 293 | Classification loss: 0.38896 | Regression loss: 0.73852 | Running loss: 1.33113\n",
            "Epoch: 0 | Iteration: 294 | Classification loss: 0.82590 | Regression loss: 1.10841 | Running loss: 1.33318\n",
            "Epoch: 0 | Iteration: 295 | Classification loss: 0.23274 | Regression loss: 0.33256 | Running loss: 1.33058\n",
            "Epoch: 0 | Iteration: 296 | Classification loss: 0.80513 | Regression loss: 0.57075 | Running loss: 1.33073\n",
            "Epoch: 0 | Iteration: 297 | Classification loss: 0.17161 | Regression loss: 0.51187 | Running loss: 1.32856\n",
            "Epoch: 0 | Iteration: 298 | Classification loss: 0.48400 | Regression loss: 0.78885 | Running loss: 1.32838\n",
            "Epoch: 0 | Iteration: 299 | Classification loss: 0.26488 | Regression loss: 0.64033 | Running loss: 1.32696\n",
            "Epoch: 0 | Iteration: 300 | Classification loss: 0.44560 | Regression loss: 0.80253 | Running loss: 1.32670\n",
            "Epoch: 0 | Iteration: 301 | Classification loss: 0.47224 | Regression loss: 0.78352 | Running loss: 1.32647\n",
            "Epoch: 0 | Iteration: 302 | Classification loss: 0.53141 | Regression loss: 0.88956 | Running loss: 1.32678\n",
            "Epoch: 0 | Iteration: 303 | Classification loss: 0.35522 | Regression loss: 0.79533 | Running loss: 1.32620\n",
            "Epoch: 0 | Iteration: 304 | Classification loss: 0.01779 | Regression loss: 0.07628 | Running loss: 1.32216\n",
            "Epoch: 0 | Iteration: 305 | Classification loss: 0.33555 | Regression loss: 0.60737 | Running loss: 1.32092\n",
            "Epoch: 0 | Iteration: 306 | Classification loss: 0.38321 | Regression loss: 0.47998 | Running loss: 1.31943\n",
            "Epoch: 0 | Iteration: 307 | Classification loss: 0.34161 | Regression loss: 0.64730 | Running loss: 1.31836\n",
            "Epoch: 0 | Iteration: 308 | Classification loss: 0.78335 | Regression loss: 1.00074 | Running loss: 1.31986\n",
            "Epoch: 0 | Iteration: 309 | Classification loss: 0.14365 | Regression loss: 0.33496 | Running loss: 1.31715\n",
            "Epoch: 0 | Iteration: 310 | Classification loss: 0.35348 | Regression loss: 0.74381 | Running loss: 1.31644\n",
            "Epoch: 0 | Iteration: 311 | Classification loss: 0.42748 | Regression loss: 0.87153 | Running loss: 1.31639\n",
            "Epoch: 0 | Iteration: 312 | Classification loss: 0.63410 | Regression loss: 0.39009 | Running loss: 1.31545\n",
            "Epoch: 0 | Iteration: 313 | Classification loss: 0.19103 | Regression loss: 0.34500 | Running loss: 1.31297\n",
            "Epoch: 0 | Iteration: 314 | Classification loss: 0.31197 | Regression loss: 0.64675 | Running loss: 1.31185\n",
            "Epoch: 0 | Iteration: 315 | Classification loss: 0.84251 | Regression loss: 0.43599 | Running loss: 1.31174\n",
            "Epoch: 0 | Iteration: 316 | Classification loss: 0.28929 | Regression loss: 0.60689 | Running loss: 1.31043\n",
            "Epoch: 0 | Iteration: 317 | Classification loss: 0.66780 | Regression loss: 0.91012 | Running loss: 1.31127\n",
            "Epoch: 0 | Iteration: 318 | Classification loss: 0.00787 | Regression loss: 0.07442 | Running loss: 1.30742\n",
            "Epoch: 0 | Iteration: 319 | Classification loss: 0.61718 | Regression loss: 0.85503 | Running loss: 1.30793\n",
            "Epoch: 0 | Iteration: 320 | Classification loss: 0.48487 | Regression loss: 0.41190 | Running loss: 1.30665\n",
            "Epoch: 0 | Iteration: 321 | Classification loss: 0.49576 | Regression loss: 0.78452 | Running loss: 1.30657\n",
            "Epoch: 0 | Iteration: 322 | Classification loss: 0.26246 | Regression loss: 0.06017 | Running loss: 1.30353\n",
            "Epoch: 0 | Iteration: 323 | Classification loss: 0.36425 | Regression loss: 0.48465 | Running loss: 1.30212\n",
            "Epoch: 0 | Iteration: 324 | Classification loss: 0.69803 | Regression loss: 0.80821 | Running loss: 1.30275\n",
            "Epoch: 0 | Iteration: 325 | Classification loss: 0.49237 | Regression loss: 0.86899 | Running loss: 1.30293\n",
            "Epoch: 0 | Iteration: 326 | Classification loss: 0.37747 | Regression loss: 0.86213 | Running loss: 1.30274\n",
            "Epoch: 0 | Iteration: 327 | Classification loss: 0.24605 | Regression loss: 0.61614 | Running loss: 1.30139\n",
            "Epoch: 0 | Iteration: 328 | Classification loss: 0.30265 | Regression loss: 0.67627 | Running loss: 1.30041\n",
            "Epoch: 0 | Iteration: 329 | Classification loss: 0.52523 | Regression loss: 0.43808 | Running loss: 1.29939\n",
            "Epoch: 0 | Iteration: 330 | Classification loss: 0.35960 | Regression loss: 0.73540 | Running loss: 1.29877\n",
            "Epoch: 0 | Iteration: 331 | Classification loss: 0.27563 | Regression loss: 0.65891 | Running loss: 1.29768\n",
            "Epoch: 0 | Iteration: 332 | Classification loss: 0.22404 | Regression loss: 0.00000 | Running loss: 1.29445\n",
            "Epoch: 0 | Iteration: 333 | Classification loss: 0.46983 | Regression loss: 0.53650 | Running loss: 1.29359\n",
            "Epoch: 0 | Iteration: 334 | Classification loss: 0.64304 | Regression loss: 0.92418 | Running loss: 1.29441\n",
            "Epoch: 0 | Iteration: 335 | Classification loss: 0.37605 | Regression loss: 0.76816 | Running loss: 1.29396\n",
            "Epoch: 0 | Iteration: 336 | Classification loss: 0.16121 | Regression loss: 0.32645 | Running loss: 1.29157\n",
            "Epoch: 0 | Iteration: 337 | Classification loss: 0.68013 | Regression loss: 1.05753 | Running loss: 1.29289\n",
            "Epoch: 0 | Iteration: 338 | Classification loss: 0.66821 | Regression loss: 0.80321 | Running loss: 1.29341\n",
            "Epoch: 0 | Iteration: 339 | Classification loss: 0.14494 | Regression loss: 0.30576 | Running loss: 1.29094\n",
            "Epoch: 0 | Iteration: 340 | Classification loss: 0.72675 | Regression loss: 0.95666 | Running loss: 1.29209\n",
            "Epoch: 0 | Iteration: 341 | Classification loss: 0.18682 | Regression loss: 0.33113 | Running loss: 1.28982\n",
            "Epoch: 0 | Iteration: 342 | Classification loss: 0.25893 | Regression loss: 0.41029 | Running loss: 1.28801\n",
            "Epoch: 0 | Iteration: 343 | Classification loss: 0.62462 | Regression loss: 0.50124 | Running loss: 1.28754\n",
            "Epoch: 0 | Iteration: 344 | Classification loss: 0.32575 | Regression loss: 0.77781 | Running loss: 1.28701\n",
            "Epoch: 0 | Iteration: 345 | Classification loss: 0.36244 | Regression loss: 0.34700 | Running loss: 1.28534\n",
            "Epoch: 0 | Iteration: 346 | Classification loss: 0.50049 | Regression loss: 0.64748 | Running loss: 1.28494\n",
            "Epoch: 0 | Iteration: 347 | Classification loss: 0.03551 | Regression loss: 0.00000 | Running loss: 1.28135\n",
            "Epoch: 0 | Iteration: 348 | Classification loss: 0.61044 | Regression loss: 0.95473 | Running loss: 1.28217\n",
            "Epoch: 0 | Iteration: 349 | Classification loss: 0.17285 | Regression loss: 0.37742 | Running loss: 1.28008\n",
            "Epoch: 0 | Iteration: 350 | Classification loss: 0.54548 | Regression loss: 0.48969 | Running loss: 1.27938\n",
            "Epoch: 0 | Iteration: 351 | Classification loss: 0.02669 | Regression loss: 0.08821 | Running loss: 1.27607\n",
            "Epoch: 0 | Iteration: 352 | Classification loss: 0.36781 | Regression loss: 0.63467 | Running loss: 1.27529\n",
            "Epoch: 0 | Iteration: 353 | Classification loss: 0.01806 | Regression loss: 0.12499 | Running loss: 1.27210\n",
            "Epoch: 0 | Iteration: 354 | Classification loss: 0.51462 | Regression loss: 1.01693 | Running loss: 1.27283\n",
            "Epoch: 0 | Iteration: 355 | Classification loss: 0.38872 | Regression loss: 0.63367 | Running loss: 1.27212\n",
            "Epoch: 0 | Iteration: 356 | Classification loss: 0.33953 | Regression loss: 0.69808 | Running loss: 1.27147\n",
            "Epoch: 0 | Iteration: 357 | Classification loss: 0.40354 | Regression loss: 0.66494 | Running loss: 1.27090\n",
            "Epoch: 0 | Iteration: 358 | Classification loss: 0.56434 | Regression loss: 0.89858 | Running loss: 1.27143\n",
            "Epoch: 0 | Iteration: 359 | Classification loss: 0.52128 | Regression loss: 0.77627 | Running loss: 1.27151\n",
            "Epoch: 0 | Iteration: 360 | Classification loss: 0.30479 | Regression loss: 0.56406 | Running loss: 1.27039\n",
            "Epoch: 0 | Iteration: 361 | Classification loss: 1.14980 | Regression loss: 0.26800 | Running loss: 1.27080\n",
            "Epoch: 0 | Iteration: 362 | Classification loss: 0.31330 | Regression loss: 0.69773 | Running loss: 1.27008\n",
            "Epoch: 0 | Iteration: 363 | Classification loss: 0.64058 | Regression loss: 0.87150 | Running loss: 1.27075\n",
            "Epoch: 0 | Iteration: 364 | Classification loss: 0.23716 | Regression loss: 0.57153 | Running loss: 1.26948\n",
            "Epoch: 0 | Iteration: 365 | Classification loss: 0.60119 | Regression loss: 0.84700 | Running loss: 1.26997\n",
            "Epoch: 0 | Iteration: 366 | Classification loss: 0.35159 | Regression loss: 0.69450 | Running loss: 1.26936\n",
            "Epoch: 0 | Iteration: 367 | Classification loss: 0.42716 | Regression loss: 0.46792 | Running loss: 1.26834\n",
            "Epoch: 0 | Iteration: 368 | Classification loss: 0.29381 | Regression loss: 0.62186 | Running loss: 1.26739\n",
            "Epoch: 0 | Iteration: 369 | Classification loss: 0.57901 | Regression loss: 0.87597 | Running loss: 1.26789\n",
            "Epoch: 0 | Iteration: 370 | Classification loss: 0.24095 | Regression loss: 0.44857 | Running loss: 1.26634\n",
            "Epoch: 0 | Iteration: 371 | Classification loss: 0.18099 | Regression loss: 0.64781 | Running loss: 1.26516\n",
            "Epoch: 0 | Iteration: 372 | Classification loss: 0.25997 | Regression loss: 0.59195 | Running loss: 1.26405\n",
            "Epoch: 0 | Iteration: 373 | Classification loss: 0.33380 | Regression loss: 0.81473 | Running loss: 1.26374\n",
            "Epoch: 0 | Iteration: 374 | Classification loss: 0.35131 | Regression loss: 0.54565 | Running loss: 1.26276\n",
            "Epoch: 0 | Iteration: 375 | Classification loss: 0.24803 | Regression loss: 0.40720 | Running loss: 1.26115\n",
            "Epoch: 0 | Iteration: 376 | Classification loss: 0.07155 | Regression loss: 0.09637 | Running loss: 1.25825\n",
            "Epoch: 0 | Iteration: 377 | Classification loss: 0.33891 | Regression loss: 0.62796 | Running loss: 1.25748\n",
            "Epoch: 0 | Iteration: 378 | Classification loss: 0.52957 | Regression loss: 0.83165 | Running loss: 1.25775\n",
            "Epoch: 0 | Iteration: 379 | Classification loss: 0.56696 | Regression loss: 0.78985 | Running loss: 1.25801\n",
            "Epoch: 0 | Iteration: 380 | Classification loss: 0.36728 | Regression loss: 0.72492 | Running loss: 1.25758\n",
            "Epoch: 0 | Iteration: 381 | Classification loss: 0.86163 | Regression loss: 1.01819 | Running loss: 1.25921\n",
            "Epoch: 0 | Iteration: 382 | Classification loss: 0.44586 | Regression loss: 0.77290 | Running loss: 1.25910\n",
            "Epoch: 0 | Iteration: 383 | Classification loss: 0.42456 | Regression loss: 0.71486 | Running loss: 1.25879\n",
            "Epoch: 0 | Iteration: 384 | Classification loss: 0.42592 | Regression loss: 0.71712 | Running loss: 1.25849\n",
            "Epoch: 0 | Iteration: 385 | Classification loss: 0.29868 | Regression loss: 0.53120 | Running loss: 1.25738\n",
            "Epoch: 0 | Iteration: 386 | Classification loss: 0.50495 | Regression loss: 0.77635 | Running loss: 1.25744\n",
            "Epoch: 0 | Iteration: 387 | Classification loss: 0.49205 | Regression loss: 0.00000 | Running loss: 1.25547\n",
            "Epoch: 0 | Iteration: 388 | Classification loss: 0.30592 | Regression loss: 0.56003 | Running loss: 1.25447\n",
            "Epoch: 0 | Iteration: 389 | Classification loss: 0.54349 | Regression loss: 0.13887 | Running loss: 1.25300\n",
            "Epoch: 0 | Iteration: 390 | Classification loss: 0.19522 | Regression loss: 0.47081 | Running loss: 1.25150\n",
            "Epoch: 0 | Iteration: 391 | Classification loss: 0.40039 | Regression loss: 0.71303 | Running loss: 1.25115\n",
            "Epoch: 0 | Iteration: 392 | Classification loss: 0.01668 | Regression loss: 0.12522 | Running loss: 1.24832\n",
            "Epoch: 0 | Iteration: 393 | Classification loss: 0.40850 | Regression loss: 0.66277 | Running loss: 1.24787\n",
            "Epoch: 0 | Iteration: 394 | Classification loss: 0.70870 | Regression loss: 0.84230 | Running loss: 1.24864\n",
            "Epoch: 0 | Iteration: 395 | Classification loss: 0.53593 | Regression loss: 0.84122 | Running loss: 1.24897\n",
            "Epoch: 0 | Iteration: 396 | Classification loss: 0.33426 | Regression loss: 0.59972 | Running loss: 1.24817\n",
            "Epoch: 0 | Iteration: 397 | Classification loss: 0.69717 | Regression loss: 0.98008 | Running loss: 1.24925\n",
            "Epoch: 0 | Iteration: 398 | Classification loss: 0.38839 | Regression loss: 0.69223 | Running loss: 1.24883\n",
            "Epoch: 0 | Iteration: 399 | Classification loss: 0.16465 | Regression loss: 0.40929 | Running loss: 1.24714\n",
            "Epoch: 0 | Iteration: 400 | Classification loss: 0.76749 | Regression loss: 0.94921 | Running loss: 1.24831\n",
            "Epoch: 0 | Iteration: 401 | Classification loss: 0.43342 | Regression loss: 0.54822 | Running loss: 1.24765\n",
            "Epoch: 0 | Iteration: 402 | Classification loss: 0.25054 | Regression loss: 0.41729 | Running loss: 1.24621\n",
            "Epoch: 0 | Iteration: 403 | Classification loss: 0.82346 | Regression loss: 0.90743 | Running loss: 1.24741\n",
            "Epoch: 0 | Iteration: 404 | Classification loss: 0.60857 | Regression loss: 0.91207 | Running loss: 1.24808\n",
            "Epoch: 0 | Iteration: 405 | Classification loss: 0.49487 | Regression loss: 0.80778 | Running loss: 1.24822\n",
            "Epoch: 0 | Iteration: 406 | Classification loss: 0.86142 | Regression loss: 0.51134 | Running loss: 1.24852\n",
            "Epoch: 0 | Iteration: 407 | Classification loss: 0.17811 | Regression loss: 0.37549 | Running loss: 1.24682\n",
            "Epoch: 0 | Iteration: 408 | Classification loss: 0.21484 | Regression loss: 0.41759 | Running loss: 1.24532\n",
            "Epoch: 0 | Iteration: 409 | Classification loss: 1.04470 | Regression loss: 0.00000 | Running loss: 1.24483\n",
            "Epoch: 0 | Iteration: 410 | Classification loss: 0.79038 | Regression loss: 0.95141 | Running loss: 1.24604\n",
            "Epoch: 0 | Iteration: 411 | Classification loss: 0.13460 | Regression loss: 0.04188 | Running loss: 1.24344\n",
            "Epoch: 0 | Iteration: 412 | Classification loss: 0.09216 | Regression loss: 0.03975 | Running loss: 1.24075\n",
            "Epoch: 0 | Iteration: 413 | Classification loss: 0.44163 | Regression loss: 0.42606 | Running loss: 1.23985\n",
            "Epoch: 0 | Iteration: 414 | Classification loss: 0.73534 | Regression loss: 0.76723 | Running loss: 1.24048\n",
            "Epoch: 0 | Iteration: 415 | Classification loss: 0.77178 | Regression loss: 0.95788 | Running loss: 1.24166\n",
            "Epoch: 0 | Iteration: 416 | Classification loss: 0.50783 | Regression loss: 0.40204 | Running loss: 1.24086\n",
            "Epoch: 0 | Iteration: 417 | Classification loss: 0.67749 | Regression loss: 0.80493 | Running loss: 1.24144\n",
            "Epoch: 0 | Iteration: 418 | Classification loss: 0.40218 | Regression loss: 0.61057 | Running loss: 1.24089\n",
            "Epoch: 0 | Iteration: 419 | Classification loss: 0.86155 | Regression loss: 0.84688 | Running loss: 1.24201\n",
            "Epoch: 0 | Iteration: 420 | Classification loss: 0.60257 | Regression loss: 0.74423 | Running loss: 1.24226\n",
            "Epoch: 0 | Iteration: 421 | Classification loss: 0.31431 | Regression loss: 0.30277 | Running loss: 1.24078\n",
            "Epoch: 0 | Iteration: 422 | Classification loss: 0.33307 | Regression loss: 0.38679 | Running loss: 1.23954\n",
            "Epoch: 0 | Iteration: 423 | Classification loss: 0.70644 | Regression loss: 0.78883 | Running loss: 1.24015\n",
            "Epoch: 0 | Iteration: 424 | Classification loss: 0.12768 | Regression loss: 0.35457 | Running loss: 1.23836\n",
            "Epoch: 0 | Iteration: 425 | Classification loss: 0.20019 | Regression loss: 0.42141 | Running loss: 1.23692\n",
            "Epoch: 0 | Iteration: 426 | Classification loss: 0.27678 | Regression loss: 0.27708 | Running loss: 1.23532\n",
            "Epoch: 0 | Iteration: 427 | Classification loss: 0.48182 | Regression loss: 0.59347 | Running loss: 1.23494\n",
            "Epoch: 0 | Iteration: 428 | Classification loss: 0.43205 | Regression loss: 0.77581 | Running loss: 1.23488\n",
            "Epoch: 0 | Iteration: 429 | Classification loss: 0.46122 | Regression loss: 0.35880 | Running loss: 1.23391\n",
            "Epoch: 0 | Iteration: 430 | Classification loss: 0.63467 | Regression loss: 0.94000 | Running loss: 1.23471\n",
            "Epoch: 0 | Iteration: 431 | Classification loss: 0.06952 | Regression loss: 0.09157 | Running loss: 1.23222\n",
            "Epoch: 0 | Iteration: 432 | Classification loss: 0.60443 | Regression loss: 0.82516 | Running loss: 1.23268\n",
            "Epoch: 0 | Iteration: 433 | Classification loss: 0.06344 | Regression loss: 0.06546 | Running loss: 1.23013\n",
            "Epoch: 0 | Iteration: 434 | Classification loss: 0.22215 | Regression loss: 0.45549 | Running loss: 1.22886\n",
            "Epoch: 0 | Iteration: 435 | Classification loss: 0.02626 | Regression loss: 0.07322 | Running loss: 1.22627\n",
            "Epoch: 0 | Iteration: 436 | Classification loss: 0.14180 | Regression loss: 0.29838 | Running loss: 1.22447\n",
            "Epoch: 0 | Iteration: 437 | Classification loss: 0.25628 | Regression loss: 0.41813 | Running loss: 1.22322\n",
            "Epoch: 0 | Iteration: 438 | Classification loss: 0.24858 | Regression loss: 0.32400 | Running loss: 1.22174\n",
            "Epoch: 0 | Iteration: 439 | Classification loss: 0.01568 | Regression loss: 0.36193 | Running loss: 1.21982\n",
            "Epoch: 0 | Iteration: 440 | Classification loss: 0.76585 | Regression loss: 1.10603 | Running loss: 1.22130\n",
            "Epoch: 0 | Iteration: 441 | Classification loss: 0.59010 | Regression loss: 0.85151 | Running loss: 1.22179\n",
            "Epoch: 0 | Iteration: 442 | Classification loss: 0.21486 | Regression loss: 0.20355 | Running loss: 1.21998\n",
            "Epoch: 0 | Iteration: 443 | Classification loss: 0.52464 | Regression loss: 0.51189 | Running loss: 1.21957\n",
            "Epoch: 0 | Iteration: 444 | Classification loss: 0.70944 | Regression loss: 0.85813 | Running loss: 1.22035\n",
            "Epoch: 0 | Iteration: 445 | Classification loss: 0.37498 | Regression loss: 0.68814 | Running loss: 1.22000\n",
            "Epoch: 0 | Iteration: 446 | Classification loss: 0.47423 | Regression loss: 0.84207 | Running loss: 1.22021\n",
            "Epoch: 0 | Iteration: 447 | Classification loss: 0.18640 | Regression loss: 0.42301 | Running loss: 1.21885\n",
            "Epoch: 0 | Iteration: 448 | Classification loss: 0.29279 | Regression loss: 0.36733 | Running loss: 1.21760\n",
            "Epoch: 0 | Iteration: 449 | Classification loss: 0.32208 | Regression loss: 0.61764 | Running loss: 1.21699\n",
            "Epoch: 0 | Iteration: 450 | Classification loss: 0.23104 | Regression loss: 0.50773 | Running loss: 1.21593\n",
            "Epoch: 0 | Iteration: 451 | Classification loss: 0.08039 | Regression loss: 0.00000 | Running loss: 1.21341\n",
            "Epoch: 0 | Iteration: 452 | Classification loss: 0.27996 | Regression loss: 0.62394 | Running loss: 1.21273\n",
            "Epoch: 0 | Iteration: 453 | Classification loss: 0.40605 | Regression loss: 0.47594 | Running loss: 1.21200\n",
            "Epoch: 0 | Iteration: 454 | Classification loss: 0.61843 | Regression loss: 0.94475 | Running loss: 1.21277\n",
            "Epoch: 0 | Iteration: 455 | Classification loss: 0.03846 | Regression loss: 0.00000 | Running loss: 1.21020\n",
            "Epoch: 0 | Iteration: 456 | Classification loss: 0.48179 | Regression loss: 0.78441 | Running loss: 1.21032\n",
            "Epoch: 0 | Iteration: 457 | Classification loss: 0.14549 | Regression loss: 0.13820 | Running loss: 1.20830\n",
            "Epoch: 0 | Iteration: 458 | Classification loss: 0.15772 | Regression loss: 0.30810 | Running loss: 1.20668\n",
            "Epoch: 0 | Iteration: 459 | Classification loss: 0.50769 | Regression loss: 0.26719 | Running loss: 1.20574\n",
            "Epoch: 0 | Iteration: 460 | Classification loss: 0.21421 | Regression loss: 0.55672 | Running loss: 1.20480\n",
            "Epoch: 0 | Iteration: 461 | Classification loss: 0.20213 | Regression loss: 0.39245 | Running loss: 1.20348\n",
            "Epoch: 0 | Iteration: 462 | Classification loss: 0.42870 | Regression loss: 0.78857 | Running loss: 1.20351\n",
            "Epoch: 0 | Iteration: 463 | Classification loss: 0.55944 | Regression loss: 0.81908 | Running loss: 1.20389\n",
            "Epoch: 0 | Iteration: 464 | Classification loss: 0.20749 | Regression loss: 0.35868 | Running loss: 1.20251\n",
            "Epoch: 0 | Iteration: 465 | Classification loss: 0.57835 | Regression loss: 0.70026 | Running loss: 1.20268\n",
            "Epoch: 0 | Iteration: 466 | Classification loss: 0.00977 | Regression loss: 0.00000 | Running loss: 1.20012\n",
            "Epoch: 0 | Iteration: 467 | Classification loss: 0.00916 | Regression loss: 0.09301 | Running loss: 1.19778\n",
            "Epoch: 0 | Iteration: 468 | Classification loss: 0.00449 | Regression loss: 0.01894 | Running loss: 1.19527\n",
            "Epoch: 0 | Iteration: 469 | Classification loss: 0.10382 | Regression loss: 0.32586 | Running loss: 1.19364\n",
            "Epoch: 0 | Iteration: 470 | Classification loss: 0.37707 | Regression loss: 0.51661 | Running loss: 1.19301\n",
            "Epoch: 0 | Iteration: 471 | Classification loss: 0.00649 | Regression loss: 0.03373 | Running loss: 1.19056\n",
            "Epoch: 0 | Iteration: 472 | Classification loss: 0.65439 | Regression loss: 0.65256 | Running loss: 1.19081\n",
            "Epoch: 0 | Iteration: 473 | Classification loss: 0.03598 | Regression loss: 0.13896 | Running loss: 1.18867\n",
            "Epoch: 0 | Iteration: 474 | Classification loss: 0.21068 | Regression loss: 0.37150 | Running loss: 1.18739\n",
            "Epoch: 0 | Iteration: 475 | Classification loss: 0.76390 | Regression loss: 0.96872 | Running loss: 1.18854\n",
            "Epoch: 0 | Iteration: 476 | Classification loss: 0.79119 | Regression loss: 0.78727 | Running loss: 1.18935\n",
            "Epoch: 0 | Iteration: 477 | Classification loss: 0.00547 | Regression loss: 0.15918 | Running loss: 1.18721\n",
            "Epoch: 0 | Iteration: 478 | Classification loss: 0.19532 | Regression loss: 0.38864 | Running loss: 1.18595\n",
            "Epoch: 0 | Iteration: 479 | Classification loss: 0.45091 | Regression loss: 0.78970 | Running loss: 1.18606\n",
            "Epoch: 0 | Iteration: 480 | Classification loss: 0.15413 | Regression loss: 0.33559 | Running loss: 1.18462\n",
            "Epoch: 0 | Iteration: 481 | Classification loss: 0.31425 | Regression loss: 0.47470 | Running loss: 1.18380\n",
            "Epoch: 0 | Iteration: 482 | Classification loss: 0.14698 | Regression loss: 0.41558 | Running loss: 1.18251\n",
            "Epoch: 0 | Iteration: 483 | Classification loss: 0.01480 | Regression loss: 0.11057 | Running loss: 1.18033\n",
            "Epoch: 0 | Iteration: 484 | Classification loss: 1.18617 | Regression loss: 0.60304 | Running loss: 1.18158\n",
            "Epoch: 0 | Iteration: 485 | Classification loss: 0.28447 | Regression loss: 0.73971 | Running loss: 1.18126\n",
            "Epoch: 0 | Iteration: 486 | Classification loss: 0.68205 | Regression loss: 0.77428 | Running loss: 1.18182\n",
            "Epoch: 0 | Iteration: 487 | Classification loss: 0.15563 | Regression loss: 0.00000 | Running loss: 1.17972\n",
            "Epoch: 0 | Iteration: 488 | Classification loss: 0.24449 | Regression loss: 0.63050 | Running loss: 1.17910\n",
            "Epoch: 0 | Iteration: 489 | Classification loss: 0.39359 | Regression loss: 0.75214 | Running loss: 1.17903\n",
            "Epoch: 0 | Iteration: 490 | Classification loss: 0.01374 | Regression loss: 0.13026 | Running loss: 1.17692\n",
            "Epoch: 0 | Iteration: 491 | Classification loss: 0.31527 | Regression loss: 0.64277 | Running loss: 1.17647\n",
            "Epoch: 0 | Iteration: 492 | Classification loss: 0.03439 | Regression loss: 0.10443 | Running loss: 1.17437\n",
            "Epoch: 0 | Iteration: 493 | Classification loss: 0.44392 | Regression loss: 0.64130 | Running loss: 1.17419\n",
            "Epoch: 0 | Iteration: 494 | Classification loss: 0.56891 | Regression loss: 0.81493 | Running loss: 1.17461\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.21s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.12s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.24s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.541\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.171\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.124\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.296\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.258\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 1 | Iteration: 0 | Classification loss: 0.09681 | Regression loss: 0.39178 | Running loss: 1.17323\n",
            "Epoch: 1 | Iteration: 1 | Classification loss: 0.42171 | Regression loss: 0.44432 | Running loss: 1.17261\n",
            "Epoch: 1 | Iteration: 2 | Classification loss: 0.17555 | Regression loss: 0.31651 | Running loss: 1.17125\n",
            "Epoch: 1 | Iteration: 3 | Classification loss: 0.29670 | Regression loss: 0.54138 | Running loss: 1.17058\n",
            "Epoch: 1 | Iteration: 4 | Classification loss: 0.02152 | Regression loss: 0.05826 | Running loss: 1.16840\n",
            "Epoch: 1 | Iteration: 5 | Classification loss: 0.30404 | Regression loss: 0.51500 | Running loss: 1.16562\n",
            "Epoch: 1 | Iteration: 6 | Classification loss: 0.18897 | Regression loss: 0.52489 | Running loss: 1.16065\n",
            "Epoch: 1 | Iteration: 7 | Classification loss: 0.04711 | Regression loss: 0.12888 | Running loss: 1.15500\n",
            "Epoch: 1 | Iteration: 8 | Classification loss: 0.29562 | Regression loss: 0.57570 | Running loss: 1.15199\n",
            "Epoch: 1 | Iteration: 9 | Classification loss: 0.37038 | Regression loss: 0.76632 | Running loss: 1.14984\n",
            "Epoch: 1 | Iteration: 10 | Classification loss: 0.02882 | Regression loss: 0.00000 | Running loss: 1.14531\n",
            "Epoch: 1 | Iteration: 11 | Classification loss: 0.30125 | Regression loss: 0.43238 | Running loss: 1.14247\n",
            "Epoch: 1 | Iteration: 12 | Classification loss: 0.08630 | Regression loss: 0.27754 | Running loss: 1.13891\n",
            "Epoch: 1 | Iteration: 13 | Classification loss: 0.31836 | Regression loss: 0.43335 | Running loss: 1.13659\n",
            "Epoch: 1 | Iteration: 14 | Classification loss: 0.47689 | Regression loss: 0.66432 | Running loss: 1.13459\n",
            "Epoch: 1 | Iteration: 15 | Classification loss: 0.39081 | Regression loss: 0.81022 | Running loss: 1.13284\n",
            "Epoch: 1 | Iteration: 16 | Classification loss: 0.35208 | Regression loss: 0.55169 | Running loss: 1.13122\n",
            "Epoch: 1 | Iteration: 17 | Classification loss: 0.26001 | Regression loss: 0.61796 | Running loss: 1.12888\n",
            "Epoch: 1 | Iteration: 18 | Classification loss: 0.30066 | Regression loss: 0.59607 | Running loss: 1.12705\n",
            "Epoch: 1 | Iteration: 19 | Classification loss: 0.50428 | Regression loss: 0.66067 | Running loss: 1.12536\n",
            "Epoch: 1 | Iteration: 20 | Classification loss: 0.25923 | Regression loss: 0.62270 | Running loss: 1.12314\n",
            "Epoch: 1 | Iteration: 21 | Classification loss: 0.50257 | Regression loss: 0.82281 | Running loss: 1.12181\n",
            "Epoch: 1 | Iteration: 22 | Classification loss: 0.49233 | Regression loss: 0.91881 | Running loss: 1.12087\n",
            "Epoch: 1 | Iteration: 23 | Classification loss: 0.46806 | Regression loss: 0.67626 | Running loss: 1.11910\n",
            "Epoch: 1 | Iteration: 24 | Classification loss: 0.36393 | Regression loss: 0.74739 | Running loss: 1.11750\n",
            "Epoch: 1 | Iteration: 25 | Classification loss: 0.68113 | Regression loss: 0.75208 | Running loss: 1.11665\n",
            "Epoch: 1 | Iteration: 26 | Classification loss: 0.15818 | Regression loss: 0.47805 | Running loss: 1.11435\n",
            "Epoch: 1 | Iteration: 27 | Classification loss: 0.10937 | Regression loss: 0.02504 | Running loss: 1.11095\n",
            "Epoch: 1 | Iteration: 28 | Classification loss: 0.40474 | Regression loss: 0.75658 | Running loss: 1.11001\n",
            "Epoch: 1 | Iteration: 29 | Classification loss: 0.45462 | Regression loss: 0.51614 | Running loss: 1.10793\n",
            "Epoch: 1 | Iteration: 30 | Classification loss: 0.39276 | Regression loss: 0.56400 | Running loss: 1.08069\n",
            "Epoch: 1 | Iteration: 31 | Classification loss: 0.41189 | Regression loss: 0.73943 | Running loss: 1.07889\n",
            "Epoch: 1 | Iteration: 32 | Classification loss: 0.02416 | Regression loss: 0.09953 | Running loss: 1.07564\n",
            "Epoch: 1 | Iteration: 33 | Classification loss: 0.33824 | Regression loss: 0.57085 | Running loss: 1.06951\n",
            "Epoch: 1 | Iteration: 34 | Classification loss: 0.12455 | Regression loss: 0.45214 | Running loss: 1.06749\n",
            "Epoch: 1 | Iteration: 35 | Classification loss: 0.01066 | Regression loss: 0.05032 | Running loss: 1.06366\n",
            "Epoch: 1 | Iteration: 36 | Classification loss: 0.41019 | Regression loss: 0.72030 | Running loss: 1.06225\n",
            "Epoch: 1 | Iteration: 37 | Classification loss: 0.13074 | Regression loss: 0.20137 | Running loss: 1.05964\n",
            "Epoch: 1 | Iteration: 38 | Classification loss: 0.28680 | Regression loss: 0.50169 | Running loss: 1.05758\n",
            "Epoch: 1 | Iteration: 39 | Classification loss: 0.00974 | Regression loss: 0.06937 | Running loss: 1.05392\n",
            "Epoch: 1 | Iteration: 40 | Classification loss: 0.65541 | Regression loss: 0.77723 | Running loss: 1.05302\n",
            "Epoch: 1 | Iteration: 41 | Classification loss: 0.18918 | Regression loss: 0.48505 | Running loss: 1.05119\n",
            "Epoch: 1 | Iteration: 42 | Classification loss: 0.62808 | Regression loss: 0.51033 | Running loss: 1.04978\n",
            "Epoch: 1 | Iteration: 43 | Classification loss: 0.28986 | Regression loss: 0.53178 | Running loss: 1.04789\n",
            "Epoch: 1 | Iteration: 44 | Classification loss: 0.51472 | Regression loss: 0.99258 | Running loss: 1.04773\n",
            "Epoch: 1 | Iteration: 45 | Classification loss: 0.02436 | Regression loss: 0.06192 | Running loss: 1.04512\n",
            "Epoch: 1 | Iteration: 46 | Classification loss: 0.52083 | Regression loss: 0.59589 | Running loss: 1.04412\n",
            "Epoch: 1 | Iteration: 47 | Classification loss: 0.55100 | Regression loss: 0.84378 | Running loss: 1.04342\n",
            "Epoch: 1 | Iteration: 48 | Classification loss: 0.34205 | Regression loss: 0.67649 | Running loss: 1.04182\n",
            "Epoch: 1 | Iteration: 49 | Classification loss: 0.34184 | Regression loss: 0.77648 | Running loss: 1.04083\n",
            "Epoch: 1 | Iteration: 50 | Classification loss: 0.26271 | Regression loss: 0.64563 | Running loss: 1.04134\n",
            "Epoch: 1 | Iteration: 51 | Classification loss: 0.20438 | Regression loss: 0.51849 | Running loss: 1.03998\n",
            "Epoch: 1 | Iteration: 52 | Classification loss: 0.37241 | Regression loss: 0.78814 | Running loss: 1.04116\n",
            "Epoch: 1 | Iteration: 53 | Classification loss: 0.01933 | Regression loss: 0.22649 | Running loss: 1.03652\n",
            "Epoch: 1 | Iteration: 54 | Classification loss: 0.28522 | Regression loss: 0.50726 | Running loss: 1.03455\n",
            "Epoch: 1 | Iteration: 55 | Classification loss: 0.39310 | Regression loss: 0.45430 | Running loss: 1.03436\n",
            "Epoch: 1 | Iteration: 56 | Classification loss: 0.23024 | Regression loss: 0.53902 | Running loss: 1.03264\n",
            "Epoch: 1 | Iteration: 57 | Classification loss: 0.01437 | Regression loss: 0.10231 | Running loss: 1.02873\n",
            "Epoch: 1 | Iteration: 58 | Classification loss: 0.01389 | Regression loss: 0.00000 | Running loss: 1.02627\n",
            "Epoch: 1 | Iteration: 59 | Classification loss: 0.10979 | Regression loss: 0.30744 | Running loss: 1.02315\n",
            "Epoch: 1 | Iteration: 60 | Classification loss: 0.06712 | Regression loss: 0.03487 | Running loss: 1.02242\n",
            "Epoch: 1 | Iteration: 61 | Classification loss: 0.11262 | Regression loss: 0.26451 | Running loss: 1.01987\n",
            "Epoch: 1 | Iteration: 62 | Classification loss: 0.33804 | Regression loss: 0.45139 | Running loss: 1.01915\n",
            "Epoch: 1 | Iteration: 63 | Classification loss: 0.02109 | Regression loss: 0.06636 | Running loss: 1.01862\n",
            "Epoch: 1 | Iteration: 64 | Classification loss: 0.02871 | Regression loss: 0.03493 | Running loss: 1.01535\n",
            "Epoch: 1 | Iteration: 65 | Classification loss: 0.27574 | Regression loss: 0.54593 | Running loss: 1.01477\n",
            "Epoch: 1 | Iteration: 66 | Classification loss: 0.45481 | Regression loss: 0.47419 | Running loss: 1.01414\n",
            "Epoch: 1 | Iteration: 67 | Classification loss: 0.27130 | Regression loss: 0.43934 | Running loss: 1.01495\n",
            "Epoch: 1 | Iteration: 68 | Classification loss: 0.17334 | Regression loss: 0.41340 | Running loss: 1.01258\n",
            "Epoch: 1 | Iteration: 69 | Classification loss: 0.19575 | Regression loss: 0.00000 | Running loss: 1.01181\n",
            "Epoch: 1 | Iteration: 70 | Classification loss: 0.11327 | Regression loss: 0.24230 | Running loss: 1.01201\n",
            "Epoch: 1 | Iteration: 71 | Classification loss: 0.17279 | Regression loss: 0.49127 | Running loss: 1.00984\n",
            "Epoch: 1 | Iteration: 72 | Classification loss: 0.69105 | Regression loss: 0.45844 | Running loss: 1.00934\n",
            "Epoch: 1 | Iteration: 73 | Classification loss: 0.52614 | Regression loss: 0.73619 | Running loss: 1.01030\n",
            "Epoch: 1 | Iteration: 74 | Classification loss: 0.33674 | Regression loss: 0.63048 | Running loss: 1.00919\n",
            "Epoch: 1 | Iteration: 75 | Classification loss: 0.07245 | Regression loss: 0.37426 | Running loss: 1.00816\n",
            "Epoch: 1 | Iteration: 76 | Classification loss: 0.21333 | Regression loss: 0.33173 | Running loss: 1.00622\n",
            "Epoch: 1 | Iteration: 77 | Classification loss: 0.29496 | Regression loss: 0.73586 | Running loss: 1.00467\n",
            "Epoch: 1 | Iteration: 78 | Classification loss: 0.25881 | Regression loss: 0.35325 | Running loss: 1.00356\n",
            "Epoch: 1 | Iteration: 79 | Classification loss: 0.00719 | Regression loss: 0.07848 | Running loss: 1.00110\n",
            "Epoch: 1 | Iteration: 80 | Classification loss: 0.11982 | Regression loss: 0.22858 | Running loss: 0.99833\n",
            "Epoch: 1 | Iteration: 81 | Classification loss: 0.13094 | Regression loss: 0.41304 | Running loss: 0.99728\n",
            "Epoch: 1 | Iteration: 82 | Classification loss: 0.14473 | Regression loss: 0.00000 | Running loss: 0.99662\n",
            "Epoch: 1 | Iteration: 83 | Classification loss: 0.53641 | Regression loss: 0.82114 | Running loss: 0.99736\n",
            "Epoch: 1 | Iteration: 84 | Classification loss: 0.62618 | Regression loss: 0.88781 | Running loss: 0.99927\n",
            "Epoch: 1 | Iteration: 85 | Classification loss: 0.49763 | Regression loss: 0.53496 | Running loss: 0.99831\n",
            "Epoch: 1 | Iteration: 86 | Classification loss: 0.59695 | Regression loss: 0.85485 | Running loss: 0.99847\n",
            "Epoch: 1 | Iteration: 87 | Classification loss: 0.01088 | Regression loss: 0.00000 | Running loss: 0.99624\n",
            "Epoch: 1 | Iteration: 88 | Classification loss: 0.26129 | Regression loss: 0.36801 | Running loss: 0.96714\n",
            "Epoch: 1 | Iteration: 89 | Classification loss: 0.31818 | Regression loss: 0.72199 | Running loss: 0.96549\n",
            "Epoch: 1 | Iteration: 90 | Classification loss: 0.00420 | Regression loss: 0.08835 | Running loss: 0.96356\n",
            "Epoch: 1 | Iteration: 91 | Classification loss: 0.53858 | Regression loss: 0.70061 | Running loss: 0.96521\n",
            "Epoch: 1 | Iteration: 92 | Classification loss: 0.53690 | Regression loss: 0.73057 | Running loss: 0.96461\n",
            "Epoch: 1 | Iteration: 93 | Classification loss: 0.17713 | Regression loss: 0.32531 | Running loss: 0.96510\n",
            "Epoch: 1 | Iteration: 94 | Classification loss: 0.03765 | Regression loss: 0.07446 | Running loss: 0.96342\n",
            "Epoch: 1 | Iteration: 95 | Classification loss: 0.11741 | Regression loss: 0.25616 | Running loss: 0.96202\n",
            "Epoch: 1 | Iteration: 96 | Classification loss: 0.67813 | Regression loss: 0.54543 | Running loss: 0.96064\n",
            "Epoch: 1 | Iteration: 97 | Classification loss: 0.00321 | Regression loss: 0.00000 | Running loss: 0.95730\n",
            "Epoch: 1 | Iteration: 98 | Classification loss: 0.28852 | Regression loss: 0.43835 | Running loss: 0.95526\n",
            "Epoch: 1 | Iteration: 99 | Classification loss: 0.17700 | Regression loss: 0.52759 | Running loss: 0.95448\n",
            "Epoch: 1 | Iteration: 100 | Classification loss: 0.22004 | Regression loss: 0.55710 | Running loss: 0.95302\n",
            "Epoch: 1 | Iteration: 101 | Classification loss: 0.32119 | Regression loss: 0.59694 | Running loss: 0.95191\n",
            "Epoch: 1 | Iteration: 102 | Classification loss: 0.33230 | Regression loss: 0.46295 | Running loss: 0.95075\n",
            "Epoch: 1 | Iteration: 103 | Classification loss: 0.29354 | Regression loss: 0.28734 | Running loss: 0.95046\n",
            "Epoch: 1 | Iteration: 104 | Classification loss: 0.40875 | Regression loss: 0.68288 | Running loss: 0.95078\n",
            "Epoch: 1 | Iteration: 105 | Classification loss: 0.00072 | Regression loss: 0.00000 | Running loss: 0.94982\n",
            "Epoch: 1 | Iteration: 106 | Classification loss: 0.16177 | Regression loss: 0.37789 | Running loss: 0.95000\n",
            "Epoch: 1 | Iteration: 107 | Classification loss: 0.20081 | Regression loss: 0.28685 | Running loss: 0.94767\n",
            "Epoch: 1 | Iteration: 108 | Classification loss: 0.51172 | Regression loss: 0.77628 | Running loss: 0.94980\n",
            "Epoch: 1 | Iteration: 109 | Classification loss: 0.20045 | Regression loss: 0.38268 | Running loss: 0.94865\n",
            "Epoch: 1 | Iteration: 110 | Classification loss: 0.49309 | Regression loss: 0.24233 | Running loss: 0.94860\n",
            "Epoch: 1 | Iteration: 111 | Classification loss: 0.25761 | Regression loss: 0.65504 | Running loss: 0.94772\n",
            "Epoch: 1 | Iteration: 112 | Classification loss: 0.27004 | Regression loss: 0.52892 | Running loss: 0.94674\n",
            "Epoch: 1 | Iteration: 113 | Classification loss: 0.21329 | Regression loss: 0.12190 | Running loss: 0.94486\n",
            "Epoch: 1 | Iteration: 114 | Classification loss: 0.83226 | Regression loss: 0.90454 | Running loss: 0.94554\n",
            "Epoch: 1 | Iteration: 115 | Classification loss: 0.26258 | Regression loss: 0.48484 | Running loss: 0.94436\n",
            "Epoch: 1 | Iteration: 116 | Classification loss: 0.47471 | Regression loss: 0.80043 | Running loss: 0.94638\n",
            "Epoch: 1 | Iteration: 117 | Classification loss: 0.33994 | Regression loss: 0.26834 | Running loss: 0.94587\n",
            "Epoch: 1 | Iteration: 118 | Classification loss: 0.11047 | Regression loss: 0.31927 | Running loss: 0.94380\n",
            "Epoch: 1 | Iteration: 119 | Classification loss: 0.11918 | Regression loss: 0.31905 | Running loss: 0.94279\n",
            "Epoch: 1 | Iteration: 120 | Classification loss: 0.19077 | Regression loss: 0.26465 | Running loss: 0.94225\n",
            "Epoch: 1 | Iteration: 121 | Classification loss: 0.00065 | Regression loss: 0.00000 | Running loss: 0.94185\n",
            "Epoch: 1 | Iteration: 122 | Classification loss: 0.16935 | Regression loss: 0.39038 | Running loss: 0.94096\n",
            "Epoch: 1 | Iteration: 123 | Classification loss: 0.16382 | Regression loss: 0.27280 | Running loss: 0.94156\n",
            "Epoch: 1 | Iteration: 124 | Classification loss: 0.20949 | Regression loss: 0.35610 | Running loss: 0.94037\n",
            "Epoch: 1 | Iteration: 125 | Classification loss: 0.39221 | Regression loss: 0.68358 | Running loss: 0.94024\n",
            "Epoch: 1 | Iteration: 126 | Classification loss: 0.11866 | Regression loss: 0.24283 | Running loss: 0.93864\n",
            "Epoch: 1 | Iteration: 127 | Classification loss: 0.02865 | Regression loss: 0.03783 | Running loss: 0.93567\n",
            "Epoch: 1 | Iteration: 128 | Classification loss: 0.20656 | Regression loss: 0.26674 | Running loss: 0.93607\n",
            "Epoch: 1 | Iteration: 129 | Classification loss: 0.53844 | Regression loss: 0.78957 | Running loss: 0.93577\n",
            "Epoch: 1 | Iteration: 130 | Classification loss: 0.60898 | Regression loss: 0.78286 | Running loss: 0.93527\n",
            "Epoch: 1 | Iteration: 131 | Classification loss: 0.32974 | Regression loss: 0.56284 | Running loss: 0.93565\n",
            "Epoch: 1 | Iteration: 132 | Classification loss: 0.00928 | Regression loss: 0.08989 | Running loss: 0.93334\n",
            "Epoch: 1 | Iteration: 133 | Classification loss: 0.21068 | Regression loss: 0.45298 | Running loss: 0.93335\n",
            "Epoch: 1 | Iteration: 134 | Classification loss: 0.20179 | Regression loss: 0.49997 | Running loss: 0.93145\n",
            "Epoch: 1 | Iteration: 135 | Classification loss: 0.25976 | Regression loss: 0.47980 | Running loss: 0.93227\n",
            "Epoch: 1 | Iteration: 136 | Classification loss: 0.23188 | Regression loss: 0.40317 | Running loss: 0.93054\n",
            "Epoch: 1 | Iteration: 137 | Classification loss: 0.43144 | Regression loss: 0.58601 | Running loss: 0.91719\n",
            "Epoch: 1 | Iteration: 138 | Classification loss: 0.51063 | Regression loss: 1.05070 | Running loss: 0.91649\n",
            "Epoch: 1 | Iteration: 139 | Classification loss: 0.40988 | Regression loss: 0.42288 | Running loss: 0.91480\n",
            "Epoch: 1 | Iteration: 140 | Classification loss: 0.10042 | Regression loss: 0.35409 | Running loss: 0.91333\n",
            "Epoch: 1 | Iteration: 141 | Classification loss: 0.08125 | Regression loss: 0.31283 | Running loss: 0.91201\n",
            "Epoch: 1 | Iteration: 142 | Classification loss: 0.17516 | Regression loss: 0.28437 | Running loss: 0.91045\n",
            "Epoch: 1 | Iteration: 143 | Classification loss: 0.26233 | Regression loss: 0.58124 | Running loss: 0.91036\n",
            "Epoch: 1 | Iteration: 144 | Classification loss: 0.56683 | Regression loss: 0.81724 | Running loss: 0.90961\n",
            "Epoch: 1 | Iteration: 145 | Classification loss: 0.02179 | Regression loss: 0.00000 | Running loss: 0.90629\n",
            "Epoch: 1 | Iteration: 146 | Classification loss: 0.13788 | Regression loss: 0.31756 | Running loss: 0.90362\n",
            "Epoch: 1 | Iteration: 147 | Classification loss: 0.12920 | Regression loss: 0.41458 | Running loss: 0.90198\n",
            "Epoch: 1 | Iteration: 148 | Classification loss: 0.00335 | Regression loss: 0.07708 | Running loss: 0.90077\n",
            "Epoch: 1 | Iteration: 149 | Classification loss: 0.27978 | Regression loss: 0.56312 | Running loss: 0.89981\n",
            "Epoch: 1 | Iteration: 150 | Classification loss: 0.49029 | Regression loss: 0.87162 | Running loss: 0.90176\n",
            "Epoch: 1 | Iteration: 151 | Classification loss: 0.19857 | Regression loss: 0.29254 | Running loss: 0.89981\n",
            "Epoch: 1 | Iteration: 152 | Classification loss: 0.00771 | Regression loss: 0.06878 | Running loss: 0.89764\n",
            "Epoch: 1 | Iteration: 153 | Classification loss: 0.00044 | Regression loss: 0.00000 | Running loss: 0.89481\n",
            "Epoch: 1 | Iteration: 154 | Classification loss: 0.27103 | Regression loss: 0.36831 | Running loss: 0.89557\n",
            "Epoch: 1 | Iteration: 155 | Classification loss: 0.00183 | Regression loss: 0.08091 | Running loss: 0.89535\n",
            "Epoch: 1 | Iteration: 156 | Classification loss: 0.39139 | Regression loss: 0.56652 | Running loss: 0.89505\n",
            "Epoch: 1 | Iteration: 157 | Classification loss: 0.19158 | Regression loss: 0.22603 | Running loss: 0.89355\n",
            "Epoch: 1 | Iteration: 158 | Classification loss: 0.60042 | Regression loss: 0.87520 | Running loss: 0.89404\n",
            "Epoch: 1 | Iteration: 159 | Classification loss: 0.14344 | Regression loss: 0.56004 | Running loss: 0.89202\n",
            "Epoch: 1 | Iteration: 160 | Classification loss: 0.00199 | Regression loss: 0.04379 | Running loss: 0.88861\n",
            "Epoch: 1 | Iteration: 161 | Classification loss: 1.32231 | Regression loss: 1.00379 | Running loss: 0.89094\n",
            "Epoch: 1 | Iteration: 162 | Classification loss: 0.47749 | Regression loss: 0.81056 | Running loss: 0.89325\n",
            "Epoch: 1 | Iteration: 163 | Classification loss: 0.42477 | Regression loss: 0.49292 | Running loss: 0.89463\n",
            "Epoch: 1 | Iteration: 164 | Classification loss: 0.17049 | Regression loss: 0.03126 | Running loss: 0.89283\n",
            "Epoch: 1 | Iteration: 165 | Classification loss: 0.83597 | Regression loss: 0.74781 | Running loss: 0.89301\n",
            "Epoch: 1 | Iteration: 166 | Classification loss: 0.48066 | Regression loss: 0.71253 | Running loss: 0.89226\n",
            "Epoch: 1 | Iteration: 167 | Classification loss: 0.41255 | Regression loss: 0.66729 | Running loss: 0.89221\n",
            "Epoch: 1 | Iteration: 168 | Classification loss: 0.27521 | Regression loss: 0.72635 | Running loss: 0.89389\n",
            "Epoch: 1 | Iteration: 169 | Classification loss: 0.38559 | Regression loss: 0.76793 | Running loss: 0.89325\n",
            "Epoch: 1 | Iteration: 170 | Classification loss: 0.00272 | Regression loss: 0.10197 | Running loss: 0.89113\n",
            "Epoch: 1 | Iteration: 171 | Classification loss: 0.16001 | Regression loss: 0.31921 | Running loss: 0.88914\n",
            "Epoch: 1 | Iteration: 172 | Classification loss: 0.35361 | Regression loss: 0.59151 | Running loss: 0.88885\n",
            "Epoch: 1 | Iteration: 173 | Classification loss: 0.28444 | Regression loss: 0.65945 | Running loss: 0.88861\n",
            "Epoch: 1 | Iteration: 174 | Classification loss: 0.32311 | Regression loss: 0.63510 | Running loss: 0.89021\n",
            "Epoch: 1 | Iteration: 175 | Classification loss: 0.26451 | Regression loss: 0.49503 | Running loss: 0.88851\n",
            "Epoch: 1 | Iteration: 176 | Classification loss: 0.44767 | Regression loss: 0.66773 | Running loss: 0.88987\n",
            "Epoch: 1 | Iteration: 177 | Classification loss: 0.17953 | Regression loss: 0.52239 | Running loss: 0.88699\n",
            "Epoch: 1 | Iteration: 178 | Classification loss: 0.46796 | Regression loss: 0.58820 | Running loss: 0.88659\n",
            "Epoch: 1 | Iteration: 179 | Classification loss: 0.12495 | Regression loss: 0.26779 | Running loss: 0.88599\n",
            "Epoch: 1 | Iteration: 180 | Classification loss: 0.00163 | Regression loss: 0.00000 | Running loss: 0.88563\n",
            "Epoch: 1 | Iteration: 181 | Classification loss: 0.09255 | Regression loss: 0.33917 | Running loss: 0.88490\n",
            "Epoch: 1 | Iteration: 182 | Classification loss: 0.00906 | Regression loss: 0.12235 | Running loss: 0.88208\n",
            "Epoch: 1 | Iteration: 183 | Classification loss: 0.22556 | Regression loss: 0.51906 | Running loss: 0.88045\n",
            "Epoch: 1 | Iteration: 184 | Classification loss: 0.14877 | Regression loss: 0.19001 | Running loss: 0.87889\n",
            "Epoch: 1 | Iteration: 185 | Classification loss: 0.48018 | Regression loss: 0.66300 | Running loss: 0.88037\n",
            "Epoch: 1 | Iteration: 186 | Classification loss: 0.00278 | Regression loss: 0.04873 | Running loss: 0.87838\n",
            "Epoch: 1 | Iteration: 187 | Classification loss: 0.10074 | Regression loss: 0.06174 | Running loss: 0.87507\n",
            "Epoch: 1 | Iteration: 188 | Classification loss: 0.46975 | Regression loss: 0.62605 | Running loss: 0.87490\n",
            "Epoch: 1 | Iteration: 189 | Classification loss: 0.53733 | Regression loss: 0.73014 | Running loss: 0.87628\n",
            "Epoch: 1 | Iteration: 190 | Classification loss: 0.00202 | Regression loss: 0.00000 | Running loss: 0.87450\n",
            "Epoch: 1 | Iteration: 191 | Classification loss: 0.10696 | Regression loss: 0.24312 | Running loss: 0.87466\n",
            "Epoch: 1 | Iteration: 192 | Classification loss: 0.00951 | Regression loss: 0.08550 | Running loss: 0.87221\n",
            "Epoch: 1 | Iteration: 193 | Classification loss: 1.09239 | Regression loss: 1.05829 | Running loss: 0.87627\n",
            "Epoch: 1 | Iteration: 194 | Classification loss: 0.13010 | Regression loss: 0.31919 | Running loss: 0.87595\n",
            "Epoch: 1 | Iteration: 195 | Classification loss: 0.09809 | Regression loss: 0.32024 | Running loss: 0.87326\n",
            "Epoch: 1 | Iteration: 196 | Classification loss: 0.58283 | Regression loss: 0.74629 | Running loss: 0.87405\n",
            "Epoch: 1 | Iteration: 197 | Classification loss: 0.17322 | Regression loss: 0.37118 | Running loss: 0.87456\n",
            "Epoch: 1 | Iteration: 198 | Classification loss: 0.00835 | Regression loss: 0.06409 | Running loss: 0.87227\n",
            "Epoch: 1 | Iteration: 199 | Classification loss: 0.49950 | Regression loss: 0.65682 | Running loss: 0.87116\n",
            "Epoch: 1 | Iteration: 200 | Classification loss: 0.18628 | Regression loss: 0.01628 | Running loss: 0.86953\n",
            "Epoch: 1 | Iteration: 201 | Classification loss: 0.33225 | Regression loss: 0.35939 | Running loss: 0.87059\n",
            "Epoch: 1 | Iteration: 202 | Classification loss: 0.46678 | Regression loss: 0.63987 | Running loss: 0.87110\n",
            "Epoch: 1 | Iteration: 203 | Classification loss: 0.42087 | Regression loss: 0.56023 | Running loss: 0.87075\n",
            "Epoch: 1 | Iteration: 204 | Classification loss: 0.25720 | Regression loss: 0.56940 | Running loss: 0.86919\n",
            "Epoch: 1 | Iteration: 205 | Classification loss: 0.72691 | Regression loss: 0.04250 | Running loss: 0.86868\n",
            "Epoch: 1 | Iteration: 206 | Classification loss: 0.22804 | Regression loss: 0.58763 | Running loss: 0.86963\n",
            "Epoch: 1 | Iteration: 207 | Classification loss: 0.58811 | Regression loss: 0.47092 | Running loss: 0.86847\n",
            "Epoch: 1 | Iteration: 208 | Classification loss: 0.04894 | Regression loss: 0.24965 | Running loss: 0.86754\n",
            "Epoch: 1 | Iteration: 209 | Classification loss: 0.13440 | Regression loss: 0.27912 | Running loss: 0.86569\n",
            "Epoch: 1 | Iteration: 210 | Classification loss: 0.10446 | Regression loss: 0.25913 | Running loss: 0.86320\n",
            "Epoch: 1 | Iteration: 211 | Classification loss: 0.38742 | Regression loss: 0.48072 | Running loss: 0.86379\n",
            "Epoch: 1 | Iteration: 212 | Classification loss: 0.10782 | Regression loss: 0.27240 | Running loss: 0.86259\n",
            "Epoch: 1 | Iteration: 213 | Classification loss: 0.21373 | Regression loss: 0.44088 | Running loss: 0.86227\n",
            "Epoch: 1 | Iteration: 214 | Classification loss: 0.24900 | Regression loss: 0.58014 | Running loss: 0.86173\n",
            "Epoch: 1 | Iteration: 215 | Classification loss: 0.00174 | Regression loss: 0.06635 | Running loss: 0.86039\n",
            "Epoch: 1 | Iteration: 216 | Classification loss: 0.00166 | Regression loss: 0.07210 | Running loss: 0.85715\n",
            "Epoch: 1 | Iteration: 217 | Classification loss: 0.32493 | Regression loss: 0.57698 | Running loss: 0.85724\n",
            "Epoch: 1 | Iteration: 218 | Classification loss: 0.28637 | Regression loss: 0.45753 | Running loss: 0.85734\n",
            "Epoch: 1 | Iteration: 219 | Classification loss: 0.24757 | Regression loss: 0.45621 | Running loss: 0.85679\n",
            "Epoch: 1 | Iteration: 220 | Classification loss: 0.14270 | Regression loss: 0.01174 | Running loss: 0.85395\n",
            "Epoch: 1 | Iteration: 221 | Classification loss: 0.39365 | Regression loss: 0.25221 | Running loss: 0.85316\n",
            "Epoch: 1 | Iteration: 222 | Classification loss: 0.30583 | Regression loss: 0.71658 | Running loss: 0.85309\n",
            "Epoch: 1 | Iteration: 223 | Classification loss: 0.06283 | Regression loss: 0.26141 | Running loss: 0.85102\n",
            "Epoch: 1 | Iteration: 224 | Classification loss: 0.34814 | Regression loss: 0.70867 | Running loss: 0.85184\n",
            "Epoch: 1 | Iteration: 225 | Classification loss: 0.42602 | Regression loss: 0.68279 | Running loss: 0.85373\n",
            "Epoch: 1 | Iteration: 226 | Classification loss: 0.00360 | Regression loss: 0.08064 | Running loss: 0.85241\n",
            "Epoch: 1 | Iteration: 227 | Classification loss: 0.50432 | Regression loss: 0.73040 | Running loss: 0.85264\n",
            "Epoch: 1 | Iteration: 228 | Classification loss: 0.07874 | Regression loss: 0.00000 | Running loss: 0.85034\n",
            "Epoch: 1 | Iteration: 229 | Classification loss: 0.17109 | Regression loss: 0.50791 | Running loss: 0.84827\n",
            "Epoch: 1 | Iteration: 230 | Classification loss: 0.64964 | Regression loss: 0.95052 | Running loss: 0.84789\n",
            "Epoch: 1 | Iteration: 231 | Classification loss: 0.16070 | Regression loss: 0.37172 | Running loss: 0.84734\n",
            "Epoch: 1 | Iteration: 232 | Classification loss: 0.00238 | Regression loss: 0.05186 | Running loss: 0.84509\n",
            "Epoch: 1 | Iteration: 233 | Classification loss: 0.00096 | Regression loss: 0.01790 | Running loss: 0.84152\n",
            "Epoch: 1 | Iteration: 234 | Classification loss: 0.27439 | Regression loss: 0.56195 | Running loss: 0.84165\n",
            "Epoch: 1 | Iteration: 235 | Classification loss: 0.00222 | Regression loss: 0.06698 | Running loss: 0.84151\n",
            "Epoch: 1 | Iteration: 236 | Classification loss: 0.19513 | Regression loss: 0.37464 | Running loss: 0.84124\n",
            "Epoch: 1 | Iteration: 237 | Classification loss: 0.22470 | Regression loss: 0.50095 | Running loss: 0.84140\n",
            "Epoch: 1 | Iteration: 238 | Classification loss: 0.21146 | Regression loss: 0.37474 | Running loss: 0.84043\n",
            "Epoch: 1 | Iteration: 239 | Classification loss: 0.22825 | Regression loss: 0.62890 | Running loss: 0.84002\n",
            "Epoch: 1 | Iteration: 240 | Classification loss: 0.15947 | Regression loss: 0.36890 | Running loss: 0.83799\n",
            "Epoch: 1 | Iteration: 241 | Classification loss: 0.31860 | Regression loss: 0.76031 | Running loss: 0.83759\n",
            "Epoch: 1 | Iteration: 242 | Classification loss: 0.30792 | Regression loss: 0.67129 | Running loss: 0.83638\n",
            "Epoch: 1 | Iteration: 243 | Classification loss: 0.00456 | Regression loss: 0.17634 | Running loss: 0.83353\n",
            "Epoch: 1 | Iteration: 244 | Classification loss: 0.50909 | Regression loss: 0.53256 | Running loss: 0.83351\n",
            "Epoch: 1 | Iteration: 245 | Classification loss: 0.02572 | Regression loss: 0.00000 | Running loss: 0.83187\n",
            "Epoch: 1 | Iteration: 246 | Classification loss: 0.00335 | Regression loss: 0.12569 | Running loss: 0.82937\n",
            "Epoch: 1 | Iteration: 247 | Classification loss: 0.43396 | Regression loss: 0.75956 | Running loss: 0.82956\n",
            "Epoch: 1 | Iteration: 248 | Classification loss: 0.17783 | Regression loss: 0.30880 | Running loss: 0.82895\n",
            "Epoch: 1 | Iteration: 249 | Classification loss: 0.19807 | Regression loss: 0.55565 | Running loss: 0.82830\n",
            "Epoch: 1 | Iteration: 250 | Classification loss: 0.33096 | Regression loss: 0.52215 | Running loss: 0.82847\n",
            "Epoch: 1 | Iteration: 251 | Classification loss: 0.18717 | Regression loss: 0.46431 | Running loss: 0.82632\n",
            "Epoch: 1 | Iteration: 252 | Classification loss: 0.49811 | Regression loss: 0.35896 | Running loss: 0.82531\n",
            "Epoch: 1 | Iteration: 253 | Classification loss: 0.68526 | Regression loss: 0.83550 | Running loss: 0.82608\n",
            "Epoch: 1 | Iteration: 254 | Classification loss: 0.11059 | Regression loss: 0.38132 | Running loss: 0.82454\n",
            "Epoch: 1 | Iteration: 255 | Classification loss: 0.23925 | Regression loss: 0.49605 | Running loss: 0.82575\n",
            "Epoch: 1 | Iteration: 256 | Classification loss: 0.06956 | Regression loss: 0.14042 | Running loss: 0.82486\n",
            "Epoch: 1 | Iteration: 257 | Classification loss: 0.33402 | Regression loss: 0.53991 | Running loss: 0.82477\n",
            "Epoch: 1 | Iteration: 258 | Classification loss: 0.36282 | Regression loss: 0.57575 | Running loss: 0.82381\n",
            "Epoch: 1 | Iteration: 259 | Classification loss: 0.09051 | Regression loss: 0.24285 | Running loss: 0.82434\n",
            "Epoch: 1 | Iteration: 260 | Classification loss: 0.23975 | Regression loss: 0.43123 | Running loss: 0.82432\n",
            "Epoch: 1 | Iteration: 261 | Classification loss: 0.37966 | Regression loss: 0.67493 | Running loss: 0.82599\n",
            "Epoch: 1 | Iteration: 262 | Classification loss: 0.15676 | Regression loss: 0.41523 | Running loss: 0.82493\n",
            "Epoch: 1 | Iteration: 263 | Classification loss: 0.00380 | Regression loss: 0.10745 | Running loss: 0.82284\n",
            "Epoch: 1 | Iteration: 264 | Classification loss: 0.20100 | Regression loss: 0.50598 | Running loss: 0.82191\n",
            "Epoch: 1 | Iteration: 265 | Classification loss: 0.43322 | Regression loss: 0.78680 | Running loss: 0.82323\n",
            "Epoch: 1 | Iteration: 266 | Classification loss: 0.16305 | Regression loss: 0.28356 | Running loss: 0.82116\n",
            "Epoch: 1 | Iteration: 267 | Classification loss: 0.02164 | Regression loss: 0.07022 | Running loss: 0.81951\n",
            "Epoch: 1 | Iteration: 268 | Classification loss: 0.54039 | Regression loss: 0.95689 | Running loss: 0.81982\n",
            "Epoch: 1 | Iteration: 269 | Classification loss: 0.20581 | Regression loss: 0.56128 | Running loss: 0.81898\n",
            "Epoch: 1 | Iteration: 270 | Classification loss: 0.00342 | Regression loss: 0.00000 | Running loss: 0.81723\n",
            "Epoch: 1 | Iteration: 271 | Classification loss: 0.58117 | Regression loss: 0.87483 | Running loss: 0.81824\n",
            "Epoch: 1 | Iteration: 272 | Classification loss: 0.69855 | Regression loss: 0.85719 | Running loss: 0.81734\n",
            "Epoch: 1 | Iteration: 273 | Classification loss: 0.45210 | Regression loss: 0.70411 | Running loss: 0.81789\n",
            "Epoch: 1 | Iteration: 274 | Classification loss: 0.01518 | Regression loss: 0.10229 | Running loss: 0.81598\n",
            "Epoch: 1 | Iteration: 275 | Classification loss: 0.40142 | Regression loss: 0.48763 | Running loss: 0.81468\n",
            "Epoch: 1 | Iteration: 276 | Classification loss: 0.19250 | Regression loss: 0.45037 | Running loss: 0.81374\n",
            "Epoch: 1 | Iteration: 277 | Classification loss: 0.36159 | Regression loss: 0.66586 | Running loss: 0.81319\n",
            "Epoch: 1 | Iteration: 278 | Classification loss: 0.59719 | Regression loss: 0.86601 | Running loss: 0.81489\n",
            "Epoch: 1 | Iteration: 279 | Classification loss: 0.18027 | Regression loss: 0.38721 | Running loss: 0.81424\n",
            "Epoch: 1 | Iteration: 280 | Classification loss: 0.63986 | Regression loss: 0.68178 | Running loss: 0.81663\n",
            "Epoch: 1 | Iteration: 281 | Classification loss: 0.39392 | Regression loss: 0.73463 | Running loss: 0.81662\n",
            "Epoch: 1 | Iteration: 282 | Classification loss: 0.39942 | Regression loss: 0.78711 | Running loss: 0.81624\n",
            "Epoch: 1 | Iteration: 283 | Classification loss: 0.25932 | Regression loss: 0.60839 | Running loss: 0.81336\n",
            "Epoch: 1 | Iteration: 284 | Classification loss: 0.01115 | Regression loss: 0.07729 | Running loss: 0.81281\n",
            "Epoch: 1 | Iteration: 285 | Classification loss: 0.00709 | Regression loss: 0.05198 | Running loss: 0.80983\n",
            "Epoch: 1 | Iteration: 286 | Classification loss: 0.57074 | Regression loss: 0.86355 | Running loss: 0.81155\n",
            "Epoch: 1 | Iteration: 287 | Classification loss: 0.40450 | Regression loss: 0.25299 | Running loss: 0.80984\n",
            "Epoch: 1 | Iteration: 288 | Classification loss: 0.32468 | Regression loss: 0.63753 | Running loss: 0.81152\n",
            "Epoch: 1 | Iteration: 289 | Classification loss: 0.20411 | Regression loss: 0.40038 | Running loss: 0.81162\n",
            "Epoch: 1 | Iteration: 290 | Classification loss: 0.38435 | Regression loss: 0.68027 | Running loss: 0.81360\n",
            "Epoch: 1 | Iteration: 291 | Classification loss: 0.18332 | Regression loss: 0.47584 | Running loss: 0.81194\n",
            "Epoch: 1 | Iteration: 292 | Classification loss: 0.57760 | Regression loss: 0.80404 | Running loss: 0.81139\n",
            "Epoch: 1 | Iteration: 293 | Classification loss: 15.65515 | Regression loss: 0.10260 | Running loss: 0.84065\n",
            "Epoch: 1 | Iteration: 294 | Classification loss: 0.45961 | Regression loss: 0.87795 | Running loss: 0.84145\n",
            "Epoch: 1 | Iteration: 295 | Classification loss: 0.30385 | Regression loss: 0.65910 | Running loss: 0.84133\n",
            "Epoch: 1 | Iteration: 296 | Classification loss: 0.50771 | Regression loss: 0.80171 | Running loss: 0.84335\n",
            "Epoch: 1 | Iteration: 297 | Classification loss: 0.49020 | Regression loss: 0.77747 | Running loss: 0.84442\n",
            "Epoch: 1 | Iteration: 298 | Classification loss: 0.24506 | Regression loss: 0.56215 | Running loss: 0.84378\n",
            "Epoch: 1 | Iteration: 299 | Classification loss: 0.21716 | Regression loss: 0.50368 | Running loss: 0.84135\n",
            "Epoch: 1 | Iteration: 300 | Classification loss: 0.92149 | Regression loss: 0.08758 | Running loss: 0.84224\n",
            "Epoch: 1 | Iteration: 301 | Classification loss: 0.34822 | Regression loss: 0.61529 | Running loss: 0.84141\n",
            "Epoch: 1 | Iteration: 302 | Classification loss: 0.13880 | Regression loss: 0.08372 | Running loss: 0.84049\n",
            "Epoch: 1 | Iteration: 303 | Classification loss: 0.17785 | Regression loss: 0.47842 | Running loss: 0.83926\n",
            "Epoch: 1 | Iteration: 304 | Classification loss: 0.17311 | Regression loss: 0.47227 | Running loss: 0.83874\n",
            "Epoch: 1 | Iteration: 305 | Classification loss: 0.36531 | Regression loss: 0.74319 | Running loss: 0.83846\n",
            "Epoch: 1 | Iteration: 306 | Classification loss: 0.04323 | Regression loss: 0.05796 | Running loss: 0.83615\n",
            "Epoch: 1 | Iteration: 307 | Classification loss: 0.32567 | Regression loss: 0.55915 | Running loss: 0.83508\n",
            "Epoch: 1 | Iteration: 308 | Classification loss: 0.25232 | Regression loss: 0.45978 | Running loss: 0.83420\n",
            "Epoch: 1 | Iteration: 309 | Classification loss: 0.44744 | Regression loss: 0.92634 | Running loss: 0.83676\n",
            "Epoch: 1 | Iteration: 310 | Classification loss: 0.18619 | Regression loss: 0.40770 | Running loss: 0.83606\n",
            "Epoch: 1 | Iteration: 311 | Classification loss: 0.26194 | Regression loss: 0.67161 | Running loss: 0.83620\n",
            "Epoch: 1 | Iteration: 312 | Classification loss: 0.34804 | Regression loss: 0.67291 | Running loss: 0.83627\n",
            "Epoch: 1 | Iteration: 313 | Classification loss: 0.21542 | Regression loss: 0.52353 | Running loss: 0.83418\n",
            "Epoch: 1 | Iteration: 314 | Classification loss: 0.11780 | Regression loss: 0.31243 | Running loss: 0.83408\n",
            "Epoch: 1 | Iteration: 315 | Classification loss: 0.00779 | Regression loss: 0.06845 | Running loss: 0.83204\n",
            "Epoch: 1 | Iteration: 316 | Classification loss: 0.13329 | Regression loss: 0.36413 | Running loss: 0.83044\n",
            "Epoch: 1 | Iteration: 317 | Classification loss: 0.37491 | Regression loss: 0.45867 | Running loss: 0.83005\n",
            "Epoch: 1 | Iteration: 318 | Classification loss: 0.00579 | Regression loss: 0.07062 | Running loss: 0.82914\n",
            "Epoch: 1 | Iteration: 319 | Classification loss: 0.11629 | Regression loss: 0.22704 | Running loss: 0.82790\n",
            "Epoch: 1 | Iteration: 320 | Classification loss: 0.91849 | Regression loss: 0.27354 | Running loss: 0.82773\n",
            "Epoch: 1 | Iteration: 321 | Classification loss: 0.32967 | Regression loss: 0.46819 | Running loss: 0.82754\n",
            "Epoch: 1 | Iteration: 322 | Classification loss: 0.50465 | Regression loss: 0.75053 | Running loss: 0.82689\n",
            "Epoch: 1 | Iteration: 323 | Classification loss: 0.10509 | Regression loss: 0.29796 | Running loss: 0.82753\n",
            "Epoch: 1 | Iteration: 324 | Classification loss: 0.15215 | Regression loss: 0.47654 | Running loss: 0.82584\n",
            "Epoch: 1 | Iteration: 325 | Classification loss: 0.09595 | Regression loss: 0.45841 | Running loss: 0.82516\n",
            "Epoch: 1 | Iteration: 326 | Classification loss: 0.26445 | Regression loss: 0.38876 | Running loss: 0.82391\n",
            "Epoch: 1 | Iteration: 327 | Classification loss: 0.34922 | Regression loss: 0.40871 | Running loss: 0.82478\n",
            "Epoch: 1 | Iteration: 328 | Classification loss: 0.01486 | Regression loss: 0.02928 | Running loss: 0.82317\n",
            "Epoch: 1 | Iteration: 329 | Classification loss: 0.00789 | Regression loss: 0.00000 | Running loss: 0.82017\n",
            "Epoch: 1 | Iteration: 330 | Classification loss: 0.36333 | Regression loss: 0.55479 | Running loss: 0.81928\n",
            "Epoch: 1 | Iteration: 331 | Classification loss: 0.23097 | Regression loss: 0.52385 | Running loss: 0.81831\n",
            "Epoch: 1 | Iteration: 332 | Classification loss: 0.16867 | Regression loss: 0.33453 | Running loss: 0.81760\n",
            "Epoch: 1 | Iteration: 333 | Classification loss: 0.27611 | Regression loss: 0.54815 | Running loss: 0.81729\n",
            "Epoch: 1 | Iteration: 334 | Classification loss: 0.29650 | Regression loss: 0.43537 | Running loss: 0.81682\n",
            "Epoch: 1 | Iteration: 335 | Classification loss: 0.41685 | Regression loss: 0.87523 | Running loss: 0.81722\n",
            "Epoch: 1 | Iteration: 336 | Classification loss: 0.67556 | Regression loss: 0.76030 | Running loss: 0.81822\n",
            "Epoch: 1 | Iteration: 337 | Classification loss: 0.20303 | Regression loss: 0.27224 | Running loss: 0.81872\n",
            "Epoch: 1 | Iteration: 338 | Classification loss: 0.34106 | Regression loss: 0.26721 | Running loss: 0.81793\n",
            "Epoch: 1 | Iteration: 339 | Classification loss: 0.26053 | Regression loss: 0.49839 | Running loss: 0.81631\n",
            "Epoch: 1 | Iteration: 340 | Classification loss: 0.48503 | Regression loss: 0.62411 | Running loss: 0.81624\n",
            "Epoch: 1 | Iteration: 341 | Classification loss: 0.33087 | Regression loss: 0.41651 | Running loss: 0.81676\n",
            "Epoch: 1 | Iteration: 342 | Classification loss: 0.29420 | Regression loss: 0.41673 | Running loss: 0.81471\n",
            "Epoch: 1 | Iteration: 343 | Classification loss: 0.14667 | Regression loss: 0.26804 | Running loss: 0.81259\n",
            "Epoch: 1 | Iteration: 344 | Classification loss: 0.57074 | Regression loss: 0.61426 | Running loss: 0.81406\n",
            "Epoch: 1 | Iteration: 345 | Classification loss: 0.21090 | Regression loss: 0.35132 | Running loss: 0.81182\n",
            "Epoch: 1 | Iteration: 346 | Classification loss: 0.18963 | Regression loss: 0.48442 | Running loss: 0.81213\n",
            "Epoch: 1 | Iteration: 347 | Classification loss: 0.46371 | Regression loss: 0.64968 | Running loss: 0.81302\n",
            "Epoch: 1 | Iteration: 348 | Classification loss: 0.15455 | Regression loss: 0.28437 | Running loss: 0.81165\n",
            "Epoch: 1 | Iteration: 349 | Classification loss: 0.46961 | Regression loss: 0.64809 | Running loss: 0.81167\n",
            "Epoch: 1 | Iteration: 350 | Classification loss: 0.18276 | Regression loss: 0.02139 | Running loss: 0.81066\n",
            "Epoch: 1 | Iteration: 351 | Classification loss: 0.22986 | Regression loss: 0.55349 | Running loss: 0.80993\n",
            "Epoch: 1 | Iteration: 352 | Classification loss: 0.24743 | Regression loss: 0.32476 | Running loss: 0.81101\n",
            "Epoch: 1 | Iteration: 353 | Classification loss: 0.27628 | Regression loss: 0.46981 | Running loss: 0.80937\n",
            "Epoch: 1 | Iteration: 354 | Classification loss: 0.55800 | Regression loss: 0.78146 | Running loss: 0.81095\n",
            "Epoch: 1 | Iteration: 355 | Classification loss: 0.51258 | Regression loss: 0.79304 | Running loss: 0.81149\n",
            "Epoch: 1 | Iteration: 356 | Classification loss: 0.13451 | Regression loss: 0.26703 | Running loss: 0.81206\n",
            "Epoch: 1 | Iteration: 357 | Classification loss: 0.38991 | Regression loss: 0.85684 | Running loss: 0.81255\n",
            "Epoch: 1 | Iteration: 358 | Classification loss: 0.23321 | Regression loss: 0.42309 | Running loss: 0.81358\n",
            "Epoch: 1 | Iteration: 359 | Classification loss: 0.39323 | Regression loss: 0.51085 | Running loss: 0.81232\n",
            "Epoch: 1 | Iteration: 360 | Classification loss: 0.39670 | Regression loss: 0.66305 | Running loss: 0.81240\n",
            "Epoch: 1 | Iteration: 361 | Classification loss: 0.22911 | Regression loss: 0.54182 | Running loss: 0.81186\n",
            "Epoch: 1 | Iteration: 362 | Classification loss: 0.32103 | Regression loss: 0.74363 | Running loss: 0.81186\n",
            "Epoch: 1 | Iteration: 363 | Classification loss: 0.47256 | Regression loss: 0.68230 | Running loss: 0.81124\n",
            "Epoch: 1 | Iteration: 364 | Classification loss: 0.28414 | Regression loss: 0.66292 | Running loss: 0.81054\n",
            "Epoch: 1 | Iteration: 365 | Classification loss: 0.25430 | Regression loss: 0.38814 | Running loss: 0.81009\n",
            "Epoch: 1 | Iteration: 366 | Classification loss: 0.09375 | Regression loss: 0.23862 | Running loss: 0.80791\n",
            "Epoch: 1 | Iteration: 367 | Classification loss: 0.01026 | Regression loss: 0.02708 | Running loss: 0.80597\n",
            "Epoch: 1 | Iteration: 368 | Classification loss: 0.08945 | Regression loss: 0.20147 | Running loss: 0.80352\n",
            "Epoch: 1 | Iteration: 369 | Classification loss: 0.32179 | Regression loss: 0.36536 | Running loss: 0.80328\n",
            "Epoch: 1 | Iteration: 370 | Classification loss: 0.56151 | Regression loss: 0.66053 | Running loss: 0.80283\n",
            "Epoch: 1 | Iteration: 371 | Classification loss: 0.31977 | Regression loss: 0.56825 | Running loss: 0.80251\n",
            "Epoch: 1 | Iteration: 372 | Classification loss: 0.60204 | Regression loss: 0.54772 | Running loss: 0.80302\n",
            "Epoch: 1 | Iteration: 373 | Classification loss: 0.11810 | Regression loss: 0.05983 | Running loss: 0.80155\n",
            "Epoch: 1 | Iteration: 374 | Classification loss: 0.24715 | Regression loss: 0.67584 | Running loss: 0.80048\n",
            "Epoch: 1 | Iteration: 375 | Classification loss: 0.13097 | Regression loss: 0.42186 | Running loss: 0.80021\n",
            "Epoch: 1 | Iteration: 376 | Classification loss: 0.12879 | Regression loss: 0.31949 | Running loss: 0.79945\n",
            "Epoch: 1 | Iteration: 377 | Classification loss: 0.33901 | Regression loss: 0.58121 | Running loss: 0.79959\n",
            "Epoch: 1 | Iteration: 378 | Classification loss: 0.06581 | Regression loss: 0.06044 | Running loss: 0.79754\n",
            "Epoch: 1 | Iteration: 379 | Classification loss: 0.21451 | Regression loss: 0.29886 | Running loss: 0.79677\n",
            "Epoch: 1 | Iteration: 380 | Classification loss: 0.27196 | Regression loss: 0.65464 | Running loss: 0.79732\n",
            "Epoch: 1 | Iteration: 381 | Classification loss: 0.00315 | Regression loss: 0.04638 | Running loss: 0.79708\n",
            "Epoch: 1 | Iteration: 382 | Classification loss: 0.44212 | Regression loss: 0.50176 | Running loss: 0.79703\n",
            "Epoch: 1 | Iteration: 383 | Classification loss: 0.16874 | Regression loss: 0.33890 | Running loss: 0.79533\n",
            "Epoch: 1 | Iteration: 384 | Classification loss: 0.22370 | Regression loss: 0.38965 | Running loss: 0.79384\n",
            "Epoch: 1 | Iteration: 385 | Classification loss: 0.42715 | Regression loss: 0.82931 | Running loss: 0.79417\n",
            "Epoch: 1 | Iteration: 386 | Classification loss: 0.28778 | Regression loss: 0.63836 | Running loss: 0.79226\n",
            "Epoch: 1 | Iteration: 387 | Classification loss: 0.00282 | Regression loss: 0.07062 | Running loss: 0.78997\n",
            "Epoch: 1 | Iteration: 388 | Classification loss: 0.17307 | Regression loss: 0.52913 | Running loss: 0.78910\n",
            "Epoch: 1 | Iteration: 389 | Classification loss: 0.11278 | Regression loss: 0.29223 | Running loss: 0.78762\n",
            "Epoch: 1 | Iteration: 390 | Classification loss: 0.22744 | Regression loss: 0.60387 | Running loss: 0.78762\n",
            "Epoch: 1 | Iteration: 391 | Classification loss: 0.37915 | Regression loss: 0.75313 | Running loss: 0.78732\n",
            "Epoch: 1 | Iteration: 392 | Classification loss: 0.29110 | Regression loss: 0.42768 | Running loss: 0.78778\n",
            "Epoch: 1 | Iteration: 393 | Classification loss: 0.00263 | Regression loss: 0.04780 | Running loss: 0.78615\n",
            "Epoch: 1 | Iteration: 394 | Classification loss: 0.13261 | Regression loss: 0.56677 | Running loss: 0.78618\n",
            "Epoch: 1 | Iteration: 395 | Classification loss: 0.13972 | Regression loss: 0.41092 | Running loss: 0.78595\n",
            "Epoch: 1 | Iteration: 396 | Classification loss: 0.26399 | Regression loss: 0.41327 | Running loss: 0.78508\n",
            "Epoch: 1 | Iteration: 397 | Classification loss: 0.37344 | Regression loss: 0.70525 | Running loss: 0.78695\n",
            "Epoch: 1 | Iteration: 398 | Classification loss: 0.22176 | Regression loss: 0.55843 | Running loss: 0.78637\n",
            "Epoch: 1 | Iteration: 399 | Classification loss: 0.41037 | Regression loss: 0.69010 | Running loss: 0.78547\n",
            "Epoch: 1 | Iteration: 400 | Classification loss: 0.31932 | Regression loss: 0.56344 | Running loss: 0.78448\n",
            "Epoch: 1 | Iteration: 401 | Classification loss: 0.00210 | Regression loss: 0.03446 | Running loss: 0.78268\n",
            "Epoch: 1 | Iteration: 402 | Classification loss: 0.33906 | Regression loss: 0.61760 | Running loss: 0.78124\n",
            "Epoch: 1 | Iteration: 403 | Classification loss: 0.13796 | Regression loss: 0.33911 | Running loss: 0.78004\n",
            "Epoch: 1 | Iteration: 404 | Classification loss: 1.06071 | Regression loss: 0.03121 | Running loss: 0.78107\n",
            "Epoch: 1 | Iteration: 405 | Classification loss: 0.21423 | Regression loss: 0.55252 | Running loss: 0.77917\n",
            "Epoch: 1 | Iteration: 406 | Classification loss: 0.16839 | Regression loss: 0.48162 | Running loss: 0.77851\n",
            "Epoch: 1 | Iteration: 407 | Classification loss: 0.37166 | Regression loss: 0.70048 | Running loss: 0.77932\n",
            "Epoch: 1 | Iteration: 408 | Classification loss: 0.11016 | Regression loss: 0.28262 | Running loss: 0.77664\n",
            "Epoch: 1 | Iteration: 409 | Classification loss: 0.12828 | Regression loss: 0.38209 | Running loss: 0.77462\n",
            "Epoch: 1 | Iteration: 410 | Classification loss: 0.11282 | Regression loss: 0.33249 | Running loss: 0.77291\n",
            "Epoch: 1 | Iteration: 411 | Classification loss: 0.15126 | Regression loss: 0.28910 | Running loss: 0.77104\n",
            "Epoch: 1 | Iteration: 412 | Classification loss: 0.08760 | Regression loss: 0.29204 | Running loss: 0.77069\n",
            "Epoch: 1 | Iteration: 413 | Classification loss: 0.45061 | Regression loss: 0.70118 | Running loss: 0.77173\n",
            "Epoch: 1 | Iteration: 414 | Classification loss: 0.23589 | Regression loss: 0.81355 | Running loss: 0.77174\n",
            "Epoch: 1 | Iteration: 415 | Classification loss: 0.25988 | Regression loss: 0.69632 | Running loss: 0.77017\n",
            "Epoch: 1 | Iteration: 416 | Classification loss: 0.36583 | Regression loss: 0.36452 | Running loss: 0.77128\n",
            "Epoch: 1 | Iteration: 417 | Classification loss: 0.32157 | Regression loss: 0.63507 | Running loss: 0.77293\n",
            "Epoch: 1 | Iteration: 418 | Classification loss: 0.24585 | Regression loss: 0.65816 | Running loss: 0.77300\n",
            "Epoch: 1 | Iteration: 419 | Classification loss: 1.74710 | Regression loss: 0.35301 | Running loss: 0.77420\n",
            "Epoch: 1 | Iteration: 420 | Classification loss: 0.28105 | Regression loss: 0.35034 | Running loss: 0.77200\n",
            "Epoch: 1 | Iteration: 421 | Classification loss: 0.25385 | Regression loss: 0.64582 | Running loss: 0.77198\n",
            "Epoch: 1 | Iteration: 422 | Classification loss: 0.49389 | Regression loss: 0.85125 | Running loss: 0.77170\n",
            "Epoch: 1 | Iteration: 423 | Classification loss: 0.39642 | Regression loss: 0.63029 | Running loss: 0.77173\n",
            "Epoch: 1 | Iteration: 424 | Classification loss: 0.12315 | Regression loss: 0.09179 | Running loss: 0.76874\n",
            "Epoch: 1 | Iteration: 425 | Classification loss: 0.00482 | Regression loss: 0.17632 | Running loss: 0.76641\n",
            "Epoch: 1 | Iteration: 426 | Classification loss: 0.25073 | Regression loss: 0.52444 | Running loss: 0.76673\n",
            "Epoch: 1 | Iteration: 427 | Classification loss: 0.22330 | Regression loss: 0.51399 | Running loss: 0.76676\n",
            "Epoch: 1 | Iteration: 428 | Classification loss: 0.02156 | Regression loss: 0.05834 | Running loss: 0.76393\n",
            "Epoch: 1 | Iteration: 429 | Classification loss: 0.19684 | Regression loss: 0.45605 | Running loss: 0.76427\n",
            "Epoch: 1 | Iteration: 430 | Classification loss: 0.21029 | Regression loss: 0.48956 | Running loss: 0.76443\n",
            "Epoch: 1 | Iteration: 431 | Classification loss: 0.32586 | Regression loss: 0.37127 | Running loss: 0.76472\n",
            "Epoch: 1 | Iteration: 432 | Classification loss: 0.13213 | Regression loss: 0.24827 | Running loss: 0.76333\n",
            "Epoch: 1 | Iteration: 433 | Classification loss: 0.28044 | Regression loss: 0.50851 | Running loss: 0.76249\n",
            "Epoch: 1 | Iteration: 434 | Classification loss: 0.14696 | Regression loss: 0.20216 | Running loss: 0.76155\n",
            "Epoch: 1 | Iteration: 435 | Classification loss: 0.00226 | Regression loss: 0.06861 | Running loss: 0.75854\n",
            "Epoch: 1 | Iteration: 436 | Classification loss: 0.01411 | Regression loss: 0.06465 | Running loss: 0.75838\n",
            "Epoch: 1 | Iteration: 437 | Classification loss: 0.17691 | Regression loss: 0.33137 | Running loss: 0.75653\n",
            "Epoch: 1 | Iteration: 438 | Classification loss: 0.00345 | Regression loss: 0.08021 | Running loss: 0.75644\n",
            "Epoch: 1 | Iteration: 439 | Classification loss: 0.38151 | Regression loss: 0.55725 | Running loss: 0.75697\n",
            "Epoch: 1 | Iteration: 440 | Classification loss: 0.19087 | Regression loss: 0.43206 | Running loss: 0.75801\n",
            "Epoch: 1 | Iteration: 441 | Classification loss: 0.27754 | Regression loss: 0.32535 | Running loss: 0.75834\n",
            "Epoch: 1 | Iteration: 442 | Classification loss: 0.11007 | Regression loss: 0.29458 | Running loss: 0.75780\n",
            "Epoch: 1 | Iteration: 443 | Classification loss: 0.00561 | Regression loss: 0.08399 | Running loss: 0.75683\n",
            "Epoch: 1 | Iteration: 444 | Classification loss: 0.33659 | Regression loss: 0.75979 | Running loss: 0.75827\n",
            "Epoch: 1 | Iteration: 445 | Classification loss: 0.09531 | Regression loss: 0.34127 | Running loss: 0.75540\n",
            "Epoch: 1 | Iteration: 446 | Classification loss: 0.21194 | Regression loss: 0.40549 | Running loss: 0.75375\n",
            "Epoch: 1 | Iteration: 447 | Classification loss: 9.89257 | Regression loss: 0.33031 | Running loss: 0.77336\n",
            "Epoch: 1 | Iteration: 448 | Classification loss: 0.00173 | Regression loss: 0.05441 | Running loss: 0.77140\n",
            "Epoch: 1 | Iteration: 449 | Classification loss: 0.09147 | Regression loss: 0.29183 | Running loss: 0.76903\n",
            "Epoch: 1 | Iteration: 450 | Classification loss: 0.08817 | Regression loss: 0.01038 | Running loss: 0.76710\n",
            "Epoch: 1 | Iteration: 451 | Classification loss: 0.44297 | Regression loss: 0.64317 | Running loss: 0.76664\n",
            "Epoch: 1 | Iteration: 452 | Classification loss: 0.28498 | Regression loss: 0.36820 | Running loss: 0.76673\n",
            "Epoch: 1 | Iteration: 453 | Classification loss: 0.13125 | Regression loss: 0.28459 | Running loss: 0.76624\n",
            "Epoch: 1 | Iteration: 454 | Classification loss: 0.38336 | Regression loss: 0.68367 | Running loss: 0.76649\n",
            "Epoch: 1 | Iteration: 455 | Classification loss: 0.15214 | Regression loss: 0.24564 | Running loss: 0.76581\n",
            "Epoch: 1 | Iteration: 456 | Classification loss: 0.43509 | Regression loss: 0.65852 | Running loss: 0.76784\n",
            "Epoch: 1 | Iteration: 457 | Classification loss: 0.32716 | Regression loss: 0.61681 | Running loss: 0.76792\n",
            "Epoch: 1 | Iteration: 458 | Classification loss: 0.53625 | Regression loss: 0.70054 | Running loss: 0.76863\n",
            "Epoch: 1 | Iteration: 459 | Classification loss: 0.16858 | Regression loss: 0.51325 | Running loss: 0.76687\n",
            "Epoch: 1 | Iteration: 460 | Classification loss: 0.14013 | Regression loss: 0.52389 | Running loss: 0.76812\n",
            "Epoch: 1 | Iteration: 461 | Classification loss: 0.33844 | Regression loss: 0.80235 | Running loss: 0.76787\n",
            "Epoch: 1 | Iteration: 462 | Classification loss: 0.23094 | Regression loss: 0.54725 | Running loss: 0.76886\n",
            "Epoch: 1 | Iteration: 463 | Classification loss: 0.27089 | Regression loss: 0.56273 | Running loss: 0.76959\n",
            "Epoch: 1 | Iteration: 464 | Classification loss: 0.01379 | Regression loss: 0.05829 | Running loss: 0.76819\n",
            "Epoch: 1 | Iteration: 465 | Classification loss: 0.03874 | Regression loss: 0.10167 | Running loss: 0.76692\n",
            "Epoch: 1 | Iteration: 466 | Classification loss: 0.42499 | Regression loss: 0.67372 | Running loss: 0.76793\n",
            "Epoch: 1 | Iteration: 467 | Classification loss: 0.18932 | Regression loss: 0.45813 | Running loss: 0.76679\n",
            "Epoch: 1 | Iteration: 468 | Classification loss: 0.18437 | Regression loss: 0.51157 | Running loss: 0.76543\n",
            "Epoch: 1 | Iteration: 469 | Classification loss: 0.08841 | Regression loss: 0.22923 | Running loss: 0.76493\n",
            "Epoch: 1 | Iteration: 470 | Classification loss: 0.40369 | Regression loss: 0.78017 | Running loss: 0.76474\n",
            "Epoch: 1 | Iteration: 471 | Classification loss: 0.12539 | Regression loss: 0.01934 | Running loss: 0.76501\n",
            "Epoch: 1 | Iteration: 472 | Classification loss: 0.19424 | Regression loss: 0.27030 | Running loss: 0.76574\n",
            "Epoch: 1 | Iteration: 473 | Classification loss: 0.43411 | Regression loss: 0.73438 | Running loss: 0.76803\n",
            "Epoch: 1 | Iteration: 474 | Classification loss: 0.35005 | Regression loss: 0.68592 | Running loss: 0.76924\n",
            "Epoch: 1 | Iteration: 475 | Classification loss: 0.22169 | Regression loss: 0.00000 | Running loss: 0.76789\n",
            "Epoch: 1 | Iteration: 476 | Classification loss: 0.19340 | Regression loss: 0.53721 | Running loss: 0.76928\n",
            "Epoch: 1 | Iteration: 477 | Classification loss: 0.41117 | Regression loss: 0.54365 | Running loss: 0.76857\n",
            "Epoch: 1 | Iteration: 478 | Classification loss: 0.54373 | Regression loss: 0.74721 | Running loss: 0.77080\n",
            "Epoch: 1 | Iteration: 479 | Classification loss: 0.09875 | Regression loss: 0.28984 | Running loss: 0.77042\n",
            "Epoch: 1 | Iteration: 480 | Classification loss: 0.01142 | Regression loss: 0.00000 | Running loss: 0.76697\n",
            "Epoch: 1 | Iteration: 481 | Classification loss: 0.00260 | Regression loss: 0.02689 | Running loss: 0.76388\n",
            "Epoch: 1 | Iteration: 482 | Classification loss: 0.44920 | Regression loss: 0.68982 | Running loss: 0.76582\n",
            "Epoch: 1 | Iteration: 483 | Classification loss: 0.00224 | Regression loss: 0.05499 | Running loss: 0.76477\n",
            "Epoch: 1 | Iteration: 484 | Classification loss: 0.41903 | Regression loss: 0.65299 | Running loss: 0.76443\n",
            "Epoch: 1 | Iteration: 485 | Classification loss: 0.43826 | Regression loss: 0.66491 | Running loss: 0.76566\n",
            "Epoch: 1 | Iteration: 486 | Classification loss: 0.31304 | Regression loss: 0.48660 | Running loss: 0.76568\n",
            "Epoch: 1 | Iteration: 487 | Classification loss: 0.27120 | Regression loss: 0.36318 | Running loss: 0.76583\n",
            "Epoch: 1 | Iteration: 488 | Classification loss: 0.25160 | Regression loss: 0.52343 | Running loss: 0.76712\n",
            "Epoch: 1 | Iteration: 489 | Classification loss: 0.35227 | Regression loss: 0.44809 | Running loss: 0.76515\n",
            "Epoch: 1 | Iteration: 490 | Classification loss: 0.18715 | Regression loss: 0.37076 | Running loss: 0.76421\n",
            "Epoch: 1 | Iteration: 491 | Classification loss: 0.20987 | Regression loss: 0.30788 | Running loss: 0.76234\n",
            "Epoch: 1 | Iteration: 492 | Classification loss: 0.01691 | Regression loss: 0.05155 | Running loss: 0.76216\n",
            "Epoch: 1 | Iteration: 493 | Classification loss: 0.50680 | Regression loss: 0.47560 | Running loss: 0.76238\n",
            "Epoch: 1 | Iteration: 494 | Classification loss: 0.35605 | Regression loss: 0.58937 | Running loss: 0.76198\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.25s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.85s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.17s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.354\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.672\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.290\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.344\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.451\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 2 | Iteration: 0 | Classification loss: 0.15914 | Regression loss: 0.57952 | Running loss: 0.76317\n",
            "Epoch: 2 | Iteration: 1 | Classification loss: 0.00257 | Regression loss: 0.06263 | Running loss: 0.76138\n",
            "Epoch: 2 | Iteration: 2 | Classification loss: 0.45427 | Regression loss: 0.75924 | Running loss: 0.76353\n",
            "Epoch: 2 | Iteration: 3 | Classification loss: 0.12474 | Regression loss: 0.26339 | Running loss: 0.76214\n",
            "Epoch: 2 | Iteration: 4 | Classification loss: 0.37103 | Regression loss: 0.67213 | Running loss: 0.76145\n",
            "Epoch: 2 | Iteration: 5 | Classification loss: 0.08773 | Regression loss: 0.24559 | Running loss: 0.76114\n",
            "Epoch: 2 | Iteration: 6 | Classification loss: 0.37684 | Regression loss: 0.20038 | Running loss: 0.76057\n",
            "Epoch: 2 | Iteration: 7 | Classification loss: 0.50303 | Regression loss: 0.80129 | Running loss: 0.76219\n",
            "Epoch: 2 | Iteration: 8 | Classification loss: 0.00412 | Regression loss: 0.03453 | Running loss: 0.76059\n",
            "Epoch: 2 | Iteration: 9 | Classification loss: 0.25922 | Regression loss: 0.49666 | Running loss: 0.76194\n",
            "Epoch: 2 | Iteration: 10 | Classification loss: 0.29083 | Regression loss: 0.43806 | Running loss: 0.76176\n",
            "Epoch: 2 | Iteration: 11 | Classification loss: 0.00080 | Regression loss: 0.00000 | Running loss: 0.76034\n",
            "Epoch: 2 | Iteration: 12 | Classification loss: 0.47883 | Regression loss: 0.89099 | Running loss: 0.76273\n",
            "Epoch: 2 | Iteration: 13 | Classification loss: 0.41050 | Regression loss: 0.39268 | Running loss: 0.76259\n",
            "Epoch: 2 | Iteration: 14 | Classification loss: 0.00086 | Regression loss: 0.04224 | Running loss: 0.76040\n",
            "Epoch: 2 | Iteration: 15 | Classification loss: 0.00210 | Regression loss: 0.01973 | Running loss: 0.76039\n",
            "Epoch: 2 | Iteration: 16 | Classification loss: 0.12288 | Regression loss: 0.41399 | Running loss: 0.75999\n",
            "Epoch: 2 | Iteration: 17 | Classification loss: 0.23008 | Regression loss: 0.48396 | Running loss: 0.76070\n",
            "Epoch: 2 | Iteration: 18 | Classification loss: 0.01985 | Regression loss: 0.00000 | Running loss: 0.75923\n",
            "Epoch: 2 | Iteration: 19 | Classification loss: 0.51832 | Regression loss: 0.77788 | Running loss: 0.75954\n",
            "Epoch: 2 | Iteration: 20 | Classification loss: 0.10600 | Regression loss: 0.05030 | Running loss: 0.75745\n",
            "Epoch: 2 | Iteration: 21 | Classification loss: 0.47108 | Regression loss: 0.51202 | Running loss: 0.75761\n",
            "Epoch: 2 | Iteration: 22 | Classification loss: 0.32713 | Regression loss: 0.28023 | Running loss: 0.75707\n",
            "Epoch: 2 | Iteration: 23 | Classification loss: 0.26075 | Regression loss: 0.54759 | Running loss: 0.75689\n",
            "Epoch: 2 | Iteration: 24 | Classification loss: 0.22623 | Regression loss: 0.34507 | Running loss: 0.75571\n",
            "Epoch: 2 | Iteration: 25 | Classification loss: 0.46351 | Regression loss: 0.65342 | Running loss: 0.75618\n",
            "Epoch: 2 | Iteration: 26 | Classification loss: 0.20357 | Regression loss: 0.44103 | Running loss: 0.75481\n",
            "Epoch: 2 | Iteration: 27 | Classification loss: 0.26362 | Regression loss: 0.60404 | Running loss: 0.75373\n",
            "Epoch: 2 | Iteration: 28 | Classification loss: 0.29153 | Regression loss: 0.52774 | Running loss: 0.75308\n",
            "Epoch: 2 | Iteration: 29 | Classification loss: 0.17647 | Regression loss: 0.40628 | Running loss: 0.75202\n",
            "Epoch: 2 | Iteration: 30 | Classification loss: 0.38769 | Regression loss: 0.27402 | Running loss: 0.75048\n",
            "Epoch: 2 | Iteration: 31 | Classification loss: 0.22874 | Regression loss: 0.51507 | Running loss: 0.75069\n",
            "Epoch: 2 | Iteration: 32 | Classification loss: 0.16567 | Regression loss: 0.47598 | Running loss: 0.75171\n",
            "Epoch: 2 | Iteration: 33 | Classification loss: 0.88877 | Regression loss: 0.16329 | Running loss: 0.75149\n",
            "Epoch: 2 | Iteration: 34 | Classification loss: 0.18871 | Regression loss: 0.47635 | Running loss: 0.75088\n",
            "Epoch: 2 | Iteration: 35 | Classification loss: 0.00836 | Regression loss: 0.06861 | Running loss: 0.74912\n",
            "Epoch: 2 | Iteration: 36 | Classification loss: 0.31763 | Regression loss: 0.64588 | Running loss: 0.74874\n",
            "Epoch: 2 | Iteration: 37 | Classification loss: 0.13875 | Regression loss: 0.31314 | Running loss: 0.74940\n",
            "Epoch: 2 | Iteration: 38 | Classification loss: 0.18707 | Regression loss: 0.48023 | Running loss: 0.74891\n",
            "Epoch: 2 | Iteration: 39 | Classification loss: 0.11291 | Regression loss: 0.30619 | Running loss: 0.74860\n",
            "Epoch: 2 | Iteration: 40 | Classification loss: 0.24484 | Regression loss: 0.26394 | Running loss: 0.74949\n",
            "Epoch: 2 | Iteration: 41 | Classification loss: 0.18574 | Regression loss: 0.42601 | Running loss: 0.74846\n",
            "Epoch: 2 | Iteration: 42 | Classification loss: 0.37469 | Regression loss: 0.46002 | Running loss: 0.74946\n",
            "Epoch: 2 | Iteration: 43 | Classification loss: 0.00485 | Regression loss: 0.07209 | Running loss: 0.74804\n",
            "Epoch: 2 | Iteration: 44 | Classification loss: 0.26063 | Regression loss: 0.59544 | Running loss: 0.74959\n",
            "Epoch: 2 | Iteration: 45 | Classification loss: 0.12656 | Regression loss: 0.24857 | Running loss: 0.74748\n",
            "Epoch: 2 | Iteration: 46 | Classification loss: 0.20920 | Regression loss: 0.45467 | Running loss: 0.74746\n",
            "Epoch: 2 | Iteration: 47 | Classification loss: 0.10558 | Regression loss: 0.24315 | Running loss: 0.74588\n",
            "Epoch: 2 | Iteration: 48 | Classification loss: 0.00136 | Regression loss: 0.02535 | Running loss: 0.74429\n",
            "Epoch: 2 | Iteration: 49 | Classification loss: 0.11822 | Regression loss: 0.31965 | Running loss: 0.74215\n",
            "Epoch: 2 | Iteration: 50 | Classification loss: 0.26428 | Regression loss: 0.20770 | Running loss: 0.74292\n",
            "Epoch: 2 | Iteration: 51 | Classification loss: 0.00407 | Regression loss: 0.06840 | Running loss: 0.74083\n",
            "Epoch: 2 | Iteration: 52 | Classification loss: 0.19733 | Regression loss: 0.36419 | Running loss: 0.73917\n",
            "Epoch: 2 | Iteration: 53 | Classification loss: 0.18107 | Regression loss: 0.53537 | Running loss: 0.73856\n",
            "Epoch: 2 | Iteration: 54 | Classification loss: 0.23461 | Regression loss: 0.33882 | Running loss: 0.73747\n",
            "Epoch: 2 | Iteration: 55 | Classification loss: 0.16696 | Regression loss: 0.50608 | Running loss: 0.73700\n",
            "Epoch: 2 | Iteration: 56 | Classification loss: 0.00129 | Regression loss: 0.06787 | Running loss: 0.73569\n",
            "Epoch: 2 | Iteration: 57 | Classification loss: 0.23620 | Regression loss: 0.51403 | Running loss: 0.73487\n",
            "Epoch: 2 | Iteration: 58 | Classification loss: 0.41814 | Regression loss: 0.73549 | Running loss: 0.73669\n",
            "Epoch: 2 | Iteration: 59 | Classification loss: 0.55003 | Regression loss: 0.67832 | Running loss: 0.73756\n",
            "Epoch: 2 | Iteration: 60 | Classification loss: 0.22009 | Regression loss: 0.70349 | Running loss: 0.73771\n",
            "Epoch: 2 | Iteration: 61 | Classification loss: 0.70112 | Regression loss: 0.68791 | Running loss: 0.73895\n",
            "Epoch: 2 | Iteration: 62 | Classification loss: 0.18603 | Regression loss: 0.54295 | Running loss: 0.74018\n",
            "Epoch: 2 | Iteration: 63 | Classification loss: 0.26134 | Regression loss: 0.29739 | Running loss: 0.74127\n",
            "Epoch: 2 | Iteration: 64 | Classification loss: 0.09634 | Regression loss: 0.27580 | Running loss: 0.74118\n",
            "Epoch: 2 | Iteration: 65 | Classification loss: 0.02718 | Regression loss: 0.06737 | Running loss: 0.74116\n",
            "Epoch: 2 | Iteration: 66 | Classification loss: 0.15887 | Regression loss: 0.53776 | Running loss: 0.74180\n",
            "Epoch: 2 | Iteration: 67 | Classification loss: 0.00696 | Regression loss: 0.07388 | Running loss: 0.74038\n",
            "Epoch: 2 | Iteration: 68 | Classification loss: 0.21655 | Regression loss: 0.41735 | Running loss: 0.74148\n",
            "Epoch: 2 | Iteration: 69 | Classification loss: 0.04306 | Regression loss: 0.02080 | Running loss: 0.74148\n",
            "Epoch: 2 | Iteration: 70 | Classification loss: 0.00272 | Regression loss: 0.02780 | Running loss: 0.73989\n",
            "Epoch: 2 | Iteration: 71 | Classification loss: 0.18999 | Regression loss: 0.60041 | Running loss: 0.73962\n",
            "Epoch: 2 | Iteration: 72 | Classification loss: 0.00117 | Regression loss: 0.05966 | Running loss: 0.73832\n",
            "Epoch: 2 | Iteration: 73 | Classification loss: 0.21080 | Regression loss: 0.40541 | Running loss: 0.73838\n",
            "Epoch: 2 | Iteration: 74 | Classification loss: 0.16944 | Regression loss: 0.42238 | Running loss: 0.73917\n",
            "Epoch: 2 | Iteration: 75 | Classification loss: 0.25115 | Regression loss: 0.46351 | Running loss: 0.73989\n",
            "Epoch: 2 | Iteration: 76 | Classification loss: 0.15999 | Regression loss: 0.40777 | Running loss: 0.73969\n",
            "Epoch: 2 | Iteration: 77 | Classification loss: 0.20175 | Regression loss: 0.46331 | Running loss: 0.73873\n",
            "Epoch: 2 | Iteration: 78 | Classification loss: 0.05366 | Regression loss: 0.31860 | Running loss: 0.73694\n",
            "Epoch: 2 | Iteration: 79 | Classification loss: 0.35416 | Regression loss: 0.44550 | Running loss: 0.73661\n",
            "Epoch: 2 | Iteration: 80 | Classification loss: 0.05359 | Regression loss: 0.28033 | Running loss: 0.73638\n",
            "Epoch: 2 | Iteration: 81 | Classification loss: 0.13787 | Regression loss: 0.33098 | Running loss: 0.73623\n",
            "Epoch: 2 | Iteration: 82 | Classification loss: 0.08004 | Regression loss: 0.23446 | Running loss: 0.73480\n",
            "Epoch: 2 | Iteration: 83 | Classification loss: 0.10820 | Regression loss: 0.19891 | Running loss: 0.73419\n",
            "Epoch: 2 | Iteration: 84 | Classification loss: 0.14600 | Regression loss: 0.38392 | Running loss: 0.73508\n",
            "Epoch: 2 | Iteration: 85 | Classification loss: 0.41095 | Regression loss: 0.63239 | Running loss: 0.73647\n",
            "Epoch: 2 | Iteration: 86 | Classification loss: 0.00129 | Regression loss: 0.08032 | Running loss: 0.73554\n",
            "Epoch: 2 | Iteration: 87 | Classification loss: 0.10209 | Regression loss: 0.30909 | Running loss: 0.73608\n",
            "Epoch: 2 | Iteration: 88 | Classification loss: 0.00149 | Regression loss: 0.02779 | Running loss: 0.73342\n",
            "Epoch: 2 | Iteration: 89 | Classification loss: 0.51985 | Regression loss: 0.65853 | Running loss: 0.73275\n",
            "Epoch: 2 | Iteration: 90 | Classification loss: 0.00130 | Regression loss: 0.05458 | Running loss: 0.73079\n",
            "Epoch: 2 | Iteration: 91 | Classification loss: 0.14097 | Regression loss: 0.47248 | Running loss: 0.72912\n",
            "Epoch: 2 | Iteration: 92 | Classification loss: 0.07469 | Regression loss: 0.28285 | Running loss: 0.72981\n",
            "Epoch: 2 | Iteration: 93 | Classification loss: 0.23335 | Regression loss: 0.64010 | Running loss: 0.73030\n",
            "Epoch: 2 | Iteration: 94 | Classification loss: 0.00078 | Regression loss: 0.02289 | Running loss: 0.72827\n",
            "Epoch: 2 | Iteration: 95 | Classification loss: 0.13363 | Regression loss: 0.30874 | Running loss: 0.72897\n",
            "Epoch: 2 | Iteration: 96 | Classification loss: 0.19041 | Regression loss: 0.43620 | Running loss: 0.72774\n",
            "Epoch: 2 | Iteration: 97 | Classification loss: 0.06734 | Regression loss: 0.16837 | Running loss: 0.72568\n",
            "Epoch: 2 | Iteration: 98 | Classification loss: 0.35270 | Regression loss: 0.62952 | Running loss: 0.72664\n",
            "Epoch: 2 | Iteration: 99 | Classification loss: 0.05887 | Regression loss: 0.13636 | Running loss: 0.72680\n",
            "Epoch: 2 | Iteration: 100 | Classification loss: 0.21288 | Regression loss: 0.39736 | Running loss: 0.72728\n",
            "Epoch: 2 | Iteration: 101 | Classification loss: 0.35748 | Regression loss: 0.63921 | Running loss: 0.72682\n",
            "Epoch: 2 | Iteration: 102 | Classification loss: 0.11173 | Regression loss: 0.23000 | Running loss: 0.72750\n",
            "Epoch: 2 | Iteration: 103 | Classification loss: 0.17084 | Regression loss: 0.50979 | Running loss: 0.72741\n",
            "Epoch: 2 | Iteration: 104 | Classification loss: 0.21509 | Regression loss: 0.37999 | Running loss: 0.72719\n",
            "Epoch: 2 | Iteration: 105 | Classification loss: 0.07372 | Regression loss: 0.24531 | Running loss: 0.72627\n",
            "Epoch: 2 | Iteration: 106 | Classification loss: 0.37333 | Regression loss: 0.70820 | Running loss: 0.72660\n",
            "Epoch: 2 | Iteration: 107 | Classification loss: 0.20624 | Regression loss: 0.47659 | Running loss: 0.72637\n",
            "Epoch: 2 | Iteration: 108 | Classification loss: 0.12915 | Regression loss: 0.45255 | Running loss: 0.72638\n",
            "Epoch: 2 | Iteration: 109 | Classification loss: 0.13290 | Regression loss: 0.29115 | Running loss: 0.72504\n",
            "Epoch: 2 | Iteration: 110 | Classification loss: 0.20491 | Regression loss: 0.54883 | Running loss: 0.72655\n",
            "Epoch: 2 | Iteration: 111 | Classification loss: 0.17448 | Regression loss: 0.55165 | Running loss: 0.72692\n",
            "Epoch: 2 | Iteration: 112 | Classification loss: 0.31786 | Regression loss: 0.64944 | Running loss: 0.72788\n",
            "Epoch: 2 | Iteration: 113 | Classification loss: 0.34172 | Regression loss: 0.72945 | Running loss: 0.72745\n",
            "Epoch: 2 | Iteration: 114 | Classification loss: 0.06694 | Regression loss: 0.20604 | Running loss: 0.72682\n",
            "Epoch: 2 | Iteration: 115 | Classification loss: 0.21060 | Regression loss: 0.53303 | Running loss: 0.72684\n",
            "Epoch: 2 | Iteration: 116 | Classification loss: 0.41575 | Regression loss: 0.63962 | Running loss: 0.72713\n",
            "Epoch: 2 | Iteration: 117 | Classification loss: 0.00096 | Regression loss: 0.00000 | Running loss: 0.72553\n",
            "Epoch: 2 | Iteration: 118 | Classification loss: 0.14032 | Regression loss: 0.02740 | Running loss: 0.72520\n",
            "Epoch: 2 | Iteration: 119 | Classification loss: 0.15068 | Regression loss: 0.45569 | Running loss: 0.72293\n",
            "Epoch: 2 | Iteration: 120 | Classification loss: 0.04729 | Regression loss: 0.22672 | Running loss: 0.72199\n",
            "Epoch: 2 | Iteration: 121 | Classification loss: 0.25674 | Regression loss: 0.35891 | Running loss: 0.72067\n",
            "Epoch: 2 | Iteration: 122 | Classification loss: 0.18135 | Regression loss: 0.45346 | Running loss: 0.72072\n",
            "Epoch: 2 | Iteration: 123 | Classification loss: 0.40365 | Regression loss: 0.37291 | Running loss: 0.72142\n",
            "Epoch: 2 | Iteration: 124 | Classification loss: 0.18900 | Regression loss: 0.43275 | Running loss: 0.72178\n",
            "Epoch: 2 | Iteration: 125 | Classification loss: 0.21516 | Regression loss: 0.39208 | Running loss: 0.72209\n",
            "Epoch: 2 | Iteration: 126 | Classification loss: 0.06531 | Regression loss: 0.18624 | Running loss: 0.72259\n",
            "Epoch: 2 | Iteration: 127 | Classification loss: 0.25128 | Regression loss: 0.43241 | Running loss: 0.72284\n",
            "Epoch: 2 | Iteration: 128 | Classification loss: 0.11145 | Regression loss: 0.28117 | Running loss: 0.72275\n",
            "Epoch: 2 | Iteration: 129 | Classification loss: 0.22585 | Regression loss: 0.34721 | Running loss: 0.72276\n",
            "Epoch: 2 | Iteration: 130 | Classification loss: 0.28956 | Regression loss: 0.33690 | Running loss: 0.72186\n",
            "Epoch: 2 | Iteration: 131 | Classification loss: 0.09845 | Regression loss: 0.24184 | Running loss: 0.72182\n",
            "Epoch: 2 | Iteration: 132 | Classification loss: 0.00089 | Regression loss: 0.00615 | Running loss: 0.72170\n",
            "Epoch: 2 | Iteration: 133 | Classification loss: 0.53588 | Regression loss: 0.99575 | Running loss: 0.72382\n",
            "Epoch: 2 | Iteration: 134 | Classification loss: 0.00318 | Regression loss: 0.01079 | Running loss: 0.72119\n",
            "Epoch: 2 | Iteration: 135 | Classification loss: 1.47765 | Regression loss: 0.02308 | Running loss: 0.72141\n",
            "Epoch: 2 | Iteration: 136 | Classification loss: 0.27287 | Regression loss: 0.75646 | Running loss: 0.72168\n",
            "Epoch: 2 | Iteration: 137 | Classification loss: 0.00256 | Regression loss: 0.00000 | Running loss: 0.72149\n",
            "Epoch: 2 | Iteration: 138 | Classification loss: 0.24748 | Regression loss: 0.35656 | Running loss: 0.72137\n",
            "Epoch: 2 | Iteration: 139 | Classification loss: 0.15306 | Regression loss: 0.31371 | Running loss: 0.72090\n",
            "Epoch: 2 | Iteration: 140 | Classification loss: 0.29745 | Regression loss: 0.51955 | Running loss: 0.72106\n",
            "Epoch: 2 | Iteration: 141 | Classification loss: 0.29348 | Regression loss: 0.00000 | Running loss: 0.72037\n",
            "Epoch: 2 | Iteration: 142 | Classification loss: 0.06497 | Regression loss: 0.04560 | Running loss: 0.71856\n",
            "Epoch: 2 | Iteration: 143 | Classification loss: 0.18805 | Regression loss: 0.28314 | Running loss: 0.71638\n",
            "Epoch: 2 | Iteration: 144 | Classification loss: 0.43539 | Regression loss: 0.40501 | Running loss: 0.71639\n",
            "Epoch: 2 | Iteration: 145 | Classification loss: 0.55705 | Regression loss: 0.81477 | Running loss: 0.71823\n",
            "Epoch: 2 | Iteration: 146 | Classification loss: 0.21131 | Regression loss: 0.52440 | Running loss: 0.71891\n",
            "Epoch: 2 | Iteration: 147 | Classification loss: 0.34833 | Regression loss: 0.71473 | Running loss: 0.72012\n",
            "Epoch: 2 | Iteration: 148 | Classification loss: 0.12359 | Regression loss: 0.31476 | Running loss: 0.71931\n",
            "Epoch: 2 | Iteration: 149 | Classification loss: 0.40431 | Regression loss: 0.43364 | Running loss: 0.71822\n",
            "Epoch: 2 | Iteration: 150 | Classification loss: 0.44981 | Regression loss: 0.44469 | Running loss: 0.71996\n",
            "Epoch: 2 | Iteration: 151 | Classification loss: 0.08648 | Regression loss: 0.23571 | Running loss: 0.71969\n",
            "Epoch: 2 | Iteration: 152 | Classification loss: 0.22248 | Regression loss: 0.01917 | Running loss: 0.71909\n",
            "Epoch: 2 | Iteration: 153 | Classification loss: 0.29386 | Regression loss: 0.66765 | Running loss: 0.72085\n",
            "Epoch: 2 | Iteration: 154 | Classification loss: 0.11746 | Regression loss: 0.26050 | Running loss: 0.71992\n",
            "Epoch: 2 | Iteration: 155 | Classification loss: 0.00154 | Regression loss: 0.00000 | Running loss: 0.71720\n",
            "Epoch: 2 | Iteration: 156 | Classification loss: 0.02590 | Regression loss: 0.03156 | Running loss: 0.71633\n",
            "Epoch: 2 | Iteration: 157 | Classification loss: 0.20333 | Regression loss: 0.44151 | Running loss: 0.71747\n",
            "Epoch: 2 | Iteration: 158 | Classification loss: 0.21820 | Regression loss: 0.40793 | Running loss: 0.71872\n",
            "Epoch: 2 | Iteration: 159 | Classification loss: 0.23970 | Regression loss: 0.53753 | Running loss: 0.71900\n",
            "Epoch: 2 | Iteration: 160 | Classification loss: 0.10913 | Regression loss: 0.23642 | Running loss: 0.71952\n",
            "Epoch: 2 | Iteration: 161 | Classification loss: 0.12407 | Regression loss: 0.23533 | Running loss: 0.71833\n",
            "Epoch: 2 | Iteration: 162 | Classification loss: 0.11373 | Regression loss: 0.28642 | Running loss: 0.71829\n",
            "Epoch: 2 | Iteration: 163 | Classification loss: 0.38750 | Regression loss: 0.52619 | Running loss: 0.71717\n",
            "Epoch: 2 | Iteration: 164 | Classification loss: 0.29819 | Regression loss: 0.54633 | Running loss: 0.71745\n",
            "Epoch: 2 | Iteration: 165 | Classification loss: 0.20976 | Regression loss: 0.50635 | Running loss: 0.71879\n",
            "Epoch: 2 | Iteration: 166 | Classification loss: 0.20472 | Regression loss: 0.37664 | Running loss: 0.71530\n",
            "Epoch: 2 | Iteration: 167 | Classification loss: 0.62030 | Regression loss: 0.56161 | Running loss: 0.71509\n",
            "Epoch: 2 | Iteration: 168 | Classification loss: 0.19128 | Regression loss: 0.48363 | Running loss: 0.71460\n",
            "Epoch: 2 | Iteration: 169 | Classification loss: 0.18337 | Regression loss: 0.34312 | Running loss: 0.71525\n",
            "Epoch: 2 | Iteration: 170 | Classification loss: 0.46334 | Regression loss: 0.60920 | Running loss: 0.71423\n",
            "Epoch: 2 | Iteration: 171 | Classification loss: 0.83648 | Regression loss: 0.74325 | Running loss: 0.71500\n",
            "Epoch: 2 | Iteration: 172 | Classification loss: 0.28631 | Regression loss: 0.50274 | Running loss: 0.71442\n",
            "Epoch: 2 | Iteration: 173 | Classification loss: 0.45342 | Regression loss: 0.60451 | Running loss: 0.71454\n",
            "Epoch: 2 | Iteration: 174 | Classification loss: 0.14824 | Regression loss: 0.42407 | Running loss: 0.71337\n",
            "Epoch: 2 | Iteration: 175 | Classification loss: 0.30830 | Regression loss: 0.36552 | Running loss: 0.71451\n",
            "Epoch: 2 | Iteration: 176 | Classification loss: 0.51428 | Regression loss: 0.69882 | Running loss: 0.71598\n",
            "Epoch: 2 | Iteration: 177 | Classification loss: 0.25476 | Regression loss: 0.66414 | Running loss: 0.71593\n",
            "Epoch: 2 | Iteration: 178 | Classification loss: 0.49314 | Regression loss: 0.67872 | Running loss: 0.71638\n",
            "Epoch: 2 | Iteration: 179 | Classification loss: 0.16833 | Regression loss: 0.46654 | Running loss: 0.71574\n",
            "Epoch: 2 | Iteration: 180 | Classification loss: 0.39936 | Regression loss: 0.50853 | Running loss: 0.71603\n",
            "Epoch: 2 | Iteration: 181 | Classification loss: 0.11365 | Regression loss: 0.33383 | Running loss: 0.71470\n",
            "Epoch: 2 | Iteration: 182 | Classification loss: 0.20155 | Regression loss: 0.53913 | Running loss: 0.71477\n",
            "Epoch: 2 | Iteration: 183 | Classification loss: 0.21473 | Regression loss: 0.63215 | Running loss: 0.71436\n",
            "Epoch: 2 | Iteration: 184 | Classification loss: 0.45184 | Regression loss: 0.80846 | Running loss: 0.71609\n",
            "Epoch: 2 | Iteration: 185 | Classification loss: 0.40209 | Regression loss: 0.52715 | Running loss: 0.71795\n",
            "Epoch: 2 | Iteration: 186 | Classification loss: 0.55071 | Regression loss: 0.84534 | Running loss: 0.71987\n",
            "Epoch: 2 | Iteration: 187 | Classification loss: 0.21341 | Regression loss: 0.52988 | Running loss: 0.72110\n",
            "Epoch: 2 | Iteration: 188 | Classification loss: 0.15157 | Regression loss: 0.38956 | Running loss: 0.72069\n",
            "Epoch: 2 | Iteration: 189 | Classification loss: 0.60974 | Regression loss: 0.76092 | Running loss: 0.72275\n",
            "Epoch: 2 | Iteration: 190 | Classification loss: 0.15861 | Regression loss: 0.34861 | Running loss: 0.72148\n",
            "Epoch: 2 | Iteration: 191 | Classification loss: 0.06777 | Regression loss: 0.16764 | Running loss: 0.72185\n",
            "Epoch: 2 | Iteration: 192 | Classification loss: 0.03519 | Regression loss: 0.01627 | Running loss: 0.72163\n",
            "Epoch: 2 | Iteration: 193 | Classification loss: 0.11441 | Regression loss: 0.27323 | Running loss: 0.72021\n",
            "Epoch: 2 | Iteration: 194 | Classification loss: 0.00243 | Regression loss: 0.05076 | Running loss: 0.71778\n",
            "Epoch: 2 | Iteration: 195 | Classification loss: 0.00263 | Regression loss: 0.03496 | Running loss: 0.71785\n",
            "Epoch: 2 | Iteration: 196 | Classification loss: 0.41445 | Regression loss: 0.69333 | Running loss: 0.71937\n",
            "Epoch: 2 | Iteration: 197 | Classification loss: 0.08417 | Regression loss: 0.16373 | Running loss: 0.71968\n",
            "Epoch: 2 | Iteration: 198 | Classification loss: 0.47467 | Regression loss: 0.46725 | Running loss: 0.71726\n",
            "Epoch: 2 | Iteration: 199 | Classification loss: 0.31004 | Regression loss: 0.59154 | Running loss: 0.71816\n",
            "Epoch: 2 | Iteration: 200 | Classification loss: 0.26618 | Regression loss: 0.34217 | Running loss: 0.71854\n",
            "Epoch: 2 | Iteration: 201 | Classification loss: 0.18350 | Regression loss: 0.57859 | Running loss: 0.71741\n",
            "Epoch: 2 | Iteration: 202 | Classification loss: 0.38017 | Regression loss: 0.48710 | Running loss: 0.71805\n",
            "Epoch: 2 | Iteration: 203 | Classification loss: 0.23462 | Regression loss: 0.47924 | Running loss: 0.71934\n",
            "Epoch: 2 | Iteration: 204 | Classification loss: 0.11980 | Regression loss: 0.36361 | Running loss: 0.71799\n",
            "Epoch: 2 | Iteration: 205 | Classification loss: 0.09952 | Regression loss: 0.23769 | Running loss: 0.71826\n",
            "Epoch: 2 | Iteration: 206 | Classification loss: 0.06837 | Regression loss: 0.26041 | Running loss: 0.71754\n",
            "Epoch: 2 | Iteration: 207 | Classification loss: 0.23668 | Regression loss: 0.33685 | Running loss: 0.71647\n",
            "Epoch: 2 | Iteration: 208 | Classification loss: 0.00226 | Regression loss: 0.10047 | Running loss: 0.71471\n",
            "Epoch: 2 | Iteration: 209 | Classification loss: 0.07035 | Regression loss: 0.23059 | Running loss: 0.71366\n",
            "Epoch: 2 | Iteration: 210 | Classification loss: 0.21960 | Regression loss: 0.48425 | Running loss: 0.71353\n",
            "Epoch: 2 | Iteration: 211 | Classification loss: 0.25740 | Regression loss: 0.49618 | Running loss: 0.71341\n",
            "Epoch: 2 | Iteration: 212 | Classification loss: 0.18290 | Regression loss: 0.56736 | Running loss: 0.71279\n",
            "Epoch: 2 | Iteration: 213 | Classification loss: 0.16099 | Regression loss: 0.51082 | Running loss: 0.71353\n",
            "Epoch: 2 | Iteration: 214 | Classification loss: 0.00103 | Regression loss: 0.23566 | Running loss: 0.71318\n",
            "Epoch: 2 | Iteration: 215 | Classification loss: 0.26231 | Regression loss: 0.65389 | Running loss: 0.71429\n",
            "Epoch: 2 | Iteration: 216 | Classification loss: 0.15016 | Regression loss: 0.41547 | Running loss: 0.71368\n",
            "Epoch: 2 | Iteration: 217 | Classification loss: 0.01038 | Regression loss: 0.07526 | Running loss: 0.71309\n",
            "Epoch: 2 | Iteration: 218 | Classification loss: 0.14022 | Regression loss: 0.39218 | Running loss: 0.71285\n",
            "Epoch: 2 | Iteration: 219 | Classification loss: 0.16758 | Regression loss: 0.25960 | Running loss: 0.71204\n",
            "Epoch: 2 | Iteration: 220 | Classification loss: 0.23179 | Regression loss: 0.44706 | Running loss: 0.71327\n",
            "Epoch: 2 | Iteration: 221 | Classification loss: 0.11896 | Regression loss: 0.30975 | Running loss: 0.71398\n",
            "Epoch: 2 | Iteration: 222 | Classification loss: 0.46825 | Regression loss: 0.67886 | Running loss: 0.71447\n",
            "Epoch: 2 | Iteration: 223 | Classification loss: 0.00105 | Regression loss: 0.17105 | Running loss: 0.71332\n",
            "Epoch: 2 | Iteration: 224 | Classification loss: 0.11021 | Regression loss: 0.29930 | Running loss: 0.71273\n",
            "Epoch: 2 | Iteration: 225 | Classification loss: 0.00115 | Regression loss: 0.13224 | Running loss: 0.71269\n",
            "Epoch: 2 | Iteration: 226 | Classification loss: 0.11756 | Regression loss: 0.24003 | Running loss: 0.71212\n",
            "Epoch: 2 | Iteration: 227 | Classification loss: 0.27512 | Regression loss: 0.48337 | Running loss: 0.71159\n",
            "Epoch: 2 | Iteration: 228 | Classification loss: 0.54233 | Regression loss: 0.30526 | Running loss: 0.71263\n",
            "Epoch: 2 | Iteration: 229 | Classification loss: 0.05050 | Regression loss: 0.22000 | Running loss: 0.71106\n",
            "Epoch: 2 | Iteration: 230 | Classification loss: 0.40034 | Regression loss: 0.75123 | Running loss: 0.71115\n",
            "Epoch: 2 | Iteration: 231 | Classification loss: 0.00093 | Regression loss: 0.15833 | Running loss: 0.71130\n",
            "Epoch: 2 | Iteration: 232 | Classification loss: 0.21734 | Regression loss: 0.58925 | Running loss: 0.71044\n",
            "Epoch: 2 | Iteration: 233 | Classification loss: 0.10992 | Regression loss: 0.36066 | Running loss: 0.71122\n",
            "Epoch: 2 | Iteration: 234 | Classification loss: 0.15831 | Regression loss: 0.41604 | Running loss: 0.71101\n",
            "Epoch: 2 | Iteration: 235 | Classification loss: 0.07077 | Regression loss: 0.30973 | Running loss: 0.70858\n",
            "Epoch: 2 | Iteration: 236 | Classification loss: 0.15832 | Regression loss: 0.55346 | Running loss: 0.70893\n",
            "Epoch: 2 | Iteration: 237 | Classification loss: 0.01011 | Regression loss: 0.01836 | Running loss: 0.70888\n",
            "Epoch: 2 | Iteration: 238 | Classification loss: 0.00104 | Regression loss: 0.02742 | Running loss: 0.70890\n",
            "Epoch: 2 | Iteration: 239 | Classification loss: 0.12749 | Regression loss: 0.25476 | Running loss: 0.70799\n",
            "Epoch: 2 | Iteration: 240 | Classification loss: 0.06855 | Regression loss: 0.21829 | Running loss: 0.70843\n",
            "Epoch: 2 | Iteration: 241 | Classification loss: 0.06015 | Regression loss: 0.22311 | Running loss: 0.70786\n",
            "Epoch: 2 | Iteration: 242 | Classification loss: 0.14403 | Regression loss: 0.39453 | Running loss: 0.70748\n",
            "Epoch: 2 | Iteration: 243 | Classification loss: 0.00018 | Regression loss: 0.00000 | Running loss: 0.70631\n",
            "Epoch: 2 | Iteration: 244 | Classification loss: 0.11621 | Regression loss: 0.27749 | Running loss: 0.70538\n",
            "Epoch: 2 | Iteration: 245 | Classification loss: 0.00109 | Regression loss: 0.13460 | Running loss: 0.70460\n",
            "Epoch: 2 | Iteration: 246 | Classification loss: 0.21729 | Regression loss: 0.49604 | Running loss: 0.70387\n",
            "Epoch: 2 | Iteration: 247 | Classification loss: 0.00051 | Regression loss: 0.01477 | Running loss: 0.70194\n",
            "Epoch: 2 | Iteration: 248 | Classification loss: 0.46083 | Regression loss: 0.78762 | Running loss: 0.70407\n",
            "Epoch: 2 | Iteration: 249 | Classification loss: 0.12963 | Regression loss: 0.41094 | Running loss: 0.70307\n",
            "Epoch: 2 | Iteration: 250 | Classification loss: 0.06866 | Regression loss: 0.27072 | Running loss: 0.70370\n",
            "Epoch: 2 | Iteration: 251 | Classification loss: 0.20046 | Regression loss: 0.56474 | Running loss: 0.70497\n",
            "Epoch: 2 | Iteration: 252 | Classification loss: 0.06701 | Regression loss: 0.25510 | Running loss: 0.70323\n",
            "Epoch: 2 | Iteration: 253 | Classification loss: 0.10399 | Regression loss: 0.28954 | Running loss: 0.70304\n",
            "Epoch: 2 | Iteration: 254 | Classification loss: 0.00011 | Regression loss: 0.00000 | Running loss: 0.70153\n",
            "Epoch: 2 | Iteration: 255 | Classification loss: 0.36566 | Regression loss: 0.60494 | Running loss: 0.70177\n",
            "Epoch: 2 | Iteration: 256 | Classification loss: 0.00078 | Regression loss: 0.05403 | Running loss: 0.70058\n",
            "Epoch: 2 | Iteration: 257 | Classification loss: 0.39898 | Regression loss: 0.65103 | Running loss: 0.70096\n",
            "Epoch: 2 | Iteration: 258 | Classification loss: 0.17849 | Regression loss: 0.33303 | Running loss: 0.69894\n",
            "Epoch: 2 | Iteration: 259 | Classification loss: 0.75698 | Regression loss: 0.45771 | Running loss: 0.70039\n",
            "Epoch: 2 | Iteration: 260 | Classification loss: 0.41783 | Regression loss: 0.69258 | Running loss: 0.70114\n",
            "Epoch: 2 | Iteration: 261 | Classification loss: 0.30533 | Regression loss: 0.48570 | Running loss: 0.70230\n",
            "Epoch: 2 | Iteration: 262 | Classification loss: 0.15487 | Regression loss: 0.37174 | Running loss: 0.70161\n",
            "Epoch: 2 | Iteration: 263 | Classification loss: 0.54295 | Regression loss: 0.63422 | Running loss: 0.70208\n",
            "Epoch: 2 | Iteration: 264 | Classification loss: 0.19123 | Regression loss: 0.49280 | Running loss: 0.70279\n",
            "Epoch: 2 | Iteration: 265 | Classification loss: 0.09037 | Regression loss: 0.32375 | Running loss: 0.70227\n",
            "Epoch: 2 | Iteration: 266 | Classification loss: 0.34971 | Regression loss: 0.72260 | Running loss: 0.70231\n",
            "Epoch: 2 | Iteration: 267 | Classification loss: 0.00048 | Regression loss: 0.12273 | Running loss: 0.70141\n",
            "Epoch: 2 | Iteration: 268 | Classification loss: 0.36432 | Regression loss: 0.62516 | Running loss: 0.70317\n",
            "Epoch: 2 | Iteration: 269 | Classification loss: 0.28357 | Regression loss: 0.56067 | Running loss: 0.70344\n",
            "Epoch: 2 | Iteration: 270 | Classification loss: 0.21689 | Regression loss: 0.52858 | Running loss: 0.70249\n",
            "Epoch: 2 | Iteration: 271 | Classification loss: 0.20491 | Regression loss: 0.27744 | Running loss: 0.70256\n",
            "Epoch: 2 | Iteration: 272 | Classification loss: 0.09468 | Regression loss: 0.37114 | Running loss: 0.70331\n",
            "Epoch: 2 | Iteration: 273 | Classification loss: 0.36542 | Regression loss: 0.53808 | Running loss: 0.70212\n",
            "Epoch: 2 | Iteration: 274 | Classification loss: 0.12707 | Regression loss: 0.30092 | Running loss: 0.70145\n",
            "Epoch: 2 | Iteration: 275 | Classification loss: 0.09087 | Regression loss: 0.38782 | Running loss: 0.70240\n",
            "Epoch: 2 | Iteration: 276 | Classification loss: 0.15523 | Regression loss: 0.37840 | Running loss: 0.70055\n",
            "Epoch: 2 | Iteration: 277 | Classification loss: 0.18448 | Regression loss: 0.38320 | Running loss: 0.69858\n",
            "Epoch: 2 | Iteration: 278 | Classification loss: 0.23503 | Regression loss: 0.59676 | Running loss: 0.69793\n",
            "Epoch: 2 | Iteration: 279 | Classification loss: 0.13300 | Regression loss: 0.42545 | Running loss: 0.69881\n",
            "Epoch: 2 | Iteration: 280 | Classification loss: 0.37767 | Regression loss: 0.24211 | Running loss: 0.69827\n",
            "Epoch: 2 | Iteration: 281 | Classification loss: 0.25885 | Regression loss: 0.58816 | Running loss: 0.69868\n",
            "Epoch: 2 | Iteration: 282 | Classification loss: 0.01893 | Regression loss: 0.01347 | Running loss: 0.69669\n",
            "Epoch: 2 | Iteration: 283 | Classification loss: 0.34562 | Regression loss: 0.01015 | Running loss: 0.69447\n",
            "Epoch: 2 | Iteration: 284 | Classification loss: 0.13232 | Regression loss: 0.23204 | Running loss: 0.69407\n",
            "Epoch: 2 | Iteration: 285 | Classification loss: 0.06202 | Regression loss: 0.18922 | Running loss: 0.69193\n",
            "Epoch: 2 | Iteration: 286 | Classification loss: 0.00008 | Regression loss: 0.00000 | Running loss: 0.68967\n",
            "Epoch: 2 | Iteration: 287 | Classification loss: 0.00358 | Regression loss: 0.01851 | Running loss: 0.68734\n",
            "Epoch: 2 | Iteration: 288 | Classification loss: 0.45868 | Regression loss: 0.71120 | Running loss: 0.68794\n",
            "Epoch: 2 | Iteration: 289 | Classification loss: 0.23504 | Regression loss: 0.55268 | Running loss: 0.68934\n",
            "Epoch: 2 | Iteration: 290 | Classification loss: 0.50111 | Regression loss: 0.54311 | Running loss: 0.69131\n",
            "Epoch: 2 | Iteration: 291 | Classification loss: 0.26276 | Regression loss: 0.49762 | Running loss: 0.68997\n",
            "Epoch: 2 | Iteration: 292 | Classification loss: 0.46864 | Regression loss: 0.73052 | Running loss: 0.69105\n",
            "Epoch: 2 | Iteration: 293 | Classification loss: 0.45816 | Regression loss: 0.56796 | Running loss: 0.69118\n",
            "Epoch: 2 | Iteration: 294 | Classification loss: 0.36561 | Regression loss: 0.54473 | Running loss: 0.69179\n",
            "Epoch: 2 | Iteration: 295 | Classification loss: 0.14525 | Regression loss: 0.44931 | Running loss: 0.69085\n",
            "Epoch: 2 | Iteration: 296 | Classification loss: 0.34712 | Regression loss: 0.59511 | Running loss: 0.69141\n",
            "Epoch: 2 | Iteration: 297 | Classification loss: 0.10947 | Regression loss: 0.19459 | Running loss: 0.68926\n",
            "Epoch: 2 | Iteration: 298 | Classification loss: 0.28579 | Regression loss: 0.00000 | Running loss: 0.65832\n",
            "Epoch: 2 | Iteration: 299 | Classification loss: 0.00035 | Regression loss: 0.00000 | Running loss: 0.65564\n",
            "Epoch: 2 | Iteration: 300 | Classification loss: 0.29115 | Regression loss: 0.41020 | Running loss: 0.65512\n",
            "Epoch: 2 | Iteration: 301 | Classification loss: 0.14088 | Regression loss: 0.30168 | Running loss: 0.65338\n",
            "Epoch: 2 | Iteration: 302 | Classification loss: 0.33314 | Regression loss: 0.40292 | Running loss: 0.65232\n",
            "Epoch: 2 | Iteration: 303 | Classification loss: 0.05670 | Regression loss: 0.40365 | Running loss: 0.65163\n",
            "Epoch: 2 | Iteration: 304 | Classification loss: 0.07296 | Regression loss: 0.39982 | Running loss: 0.65113\n",
            "Epoch: 2 | Iteration: 305 | Classification loss: 0.13374 | Regression loss: 0.27838 | Running loss: 0.64994\n",
            "Epoch: 2 | Iteration: 306 | Classification loss: 0.16957 | Regression loss: 0.47733 | Running loss: 0.64930\n",
            "Epoch: 2 | Iteration: 307 | Classification loss: 0.37670 | Regression loss: 0.49522 | Running loss: 0.65060\n",
            "Epoch: 2 | Iteration: 308 | Classification loss: 0.53704 | Regression loss: 0.70051 | Running loss: 0.65177\n",
            "Epoch: 2 | Iteration: 309 | Classification loss: 0.25495 | Regression loss: 0.59477 | Running loss: 0.65217\n",
            "Epoch: 2 | Iteration: 310 | Classification loss: 0.18070 | Regression loss: 0.50206 | Running loss: 0.65132\n",
            "Epoch: 2 | Iteration: 311 | Classification loss: 0.42604 | Regression loss: 0.83738 | Running loss: 0.65365\n",
            "Epoch: 2 | Iteration: 312 | Classification loss: 0.11752 | Regression loss: 0.25577 | Running loss: 0.65262\n",
            "Epoch: 2 | Iteration: 313 | Classification loss: 0.17922 | Regression loss: 0.50396 | Running loss: 0.65257\n",
            "Epoch: 2 | Iteration: 314 | Classification loss: 0.08095 | Regression loss: 0.11959 | Running loss: 0.65022\n",
            "Epoch: 2 | Iteration: 315 | Classification loss: 0.02742 | Regression loss: 0.03308 | Running loss: 0.64915\n",
            "Epoch: 2 | Iteration: 316 | Classification loss: 0.17368 | Regression loss: 0.22735 | Running loss: 0.64809\n",
            "Epoch: 2 | Iteration: 317 | Classification loss: 0.17966 | Regression loss: 0.27815 | Running loss: 0.64696\n",
            "Epoch: 2 | Iteration: 318 | Classification loss: 0.27809 | Regression loss: 0.48131 | Running loss: 0.64700\n",
            "Epoch: 2 | Iteration: 319 | Classification loss: 0.11442 | Regression loss: 0.38078 | Running loss: 0.64713\n",
            "Epoch: 2 | Iteration: 320 | Classification loss: 0.17428 | Regression loss: 0.42430 | Running loss: 0.64818\n",
            "Epoch: 2 | Iteration: 321 | Classification loss: 0.06869 | Regression loss: 0.17887 | Running loss: 0.64768\n",
            "Epoch: 2 | Iteration: 322 | Classification loss: 0.30955 | Regression loss: 0.65259 | Running loss: 0.64793\n",
            "Epoch: 2 | Iteration: 323 | Classification loss: 0.17797 | Regression loss: 0.43035 | Running loss: 0.64900\n",
            "Epoch: 2 | Iteration: 324 | Classification loss: 0.09999 | Regression loss: 0.27720 | Running loss: 0.64907\n",
            "Epoch: 2 | Iteration: 325 | Classification loss: 0.48842 | Regression loss: 0.65758 | Running loss: 0.64897\n",
            "Epoch: 2 | Iteration: 326 | Classification loss: 0.24127 | Regression loss: 0.52314 | Running loss: 0.64891\n",
            "Epoch: 2 | Iteration: 327 | Classification loss: 0.35364 | Regression loss: 0.68129 | Running loss: 0.64847\n",
            "Epoch: 2 | Iteration: 328 | Classification loss: 0.33142 | Regression loss: 0.49643 | Running loss: 0.64932\n",
            "Epoch: 2 | Iteration: 329 | Classification loss: 0.23630 | Regression loss: 0.52461 | Running loss: 0.64958\n",
            "Epoch: 2 | Iteration: 330 | Classification loss: 0.26183 | Regression loss: 0.55503 | Running loss: 0.65011\n",
            "Epoch: 2 | Iteration: 331 | Classification loss: 0.09846 | Regression loss: 0.27855 | Running loss: 0.64955\n",
            "Epoch: 2 | Iteration: 332 | Classification loss: 0.29839 | Regression loss: 0.42777 | Running loss: 0.64949\n",
            "Epoch: 2 | Iteration: 333 | Classification loss: 0.09968 | Regression loss: 0.38993 | Running loss: 0.65038\n",
            "Epoch: 2 | Iteration: 334 | Classification loss: 0.27781 | Regression loss: 0.57411 | Running loss: 0.65207\n",
            "Epoch: 2 | Iteration: 335 | Classification loss: 0.13646 | Regression loss: 0.33923 | Running loss: 0.65118\n",
            "Epoch: 2 | Iteration: 336 | Classification loss: 0.21192 | Regression loss: 0.38776 | Running loss: 0.65087\n",
            "Epoch: 2 | Iteration: 337 | Classification loss: 0.26226 | Regression loss: 0.54459 | Running loss: 0.65148\n",
            "Epoch: 2 | Iteration: 338 | Classification loss: 0.31472 | Regression loss: 0.19316 | Running loss: 0.65085\n",
            "Epoch: 2 | Iteration: 339 | Classification loss: 0.16045 | Regression loss: 0.32006 | Running loss: 0.65035\n",
            "Epoch: 2 | Iteration: 340 | Classification loss: 0.20770 | Regression loss: 0.48608 | Running loss: 0.64915\n",
            "Epoch: 2 | Iteration: 341 | Classification loss: 0.00052 | Regression loss: 0.00000 | Running loss: 0.64628\n",
            "Epoch: 2 | Iteration: 342 | Classification loss: 0.34909 | Regression loss: 0.58298 | Running loss: 0.64719\n",
            "Epoch: 2 | Iteration: 343 | Classification loss: 0.41433 | Regression loss: 0.66350 | Running loss: 0.64813\n",
            "Epoch: 2 | Iteration: 344 | Classification loss: 0.01038 | Regression loss: 0.12669 | Running loss: 0.64689\n",
            "Epoch: 2 | Iteration: 345 | Classification loss: 0.26087 | Regression loss: 0.36661 | Running loss: 0.64592\n",
            "Epoch: 2 | Iteration: 346 | Classification loss: 0.03249 | Regression loss: 0.11738 | Running loss: 0.64473\n",
            "Epoch: 2 | Iteration: 347 | Classification loss: 0.12356 | Regression loss: 0.29684 | Running loss: 0.64415\n",
            "Epoch: 2 | Iteration: 348 | Classification loss: 0.76247 | Regression loss: 0.73205 | Running loss: 0.64631\n",
            "Epoch: 2 | Iteration: 349 | Classification loss: 0.00316 | Regression loss: 0.08998 | Running loss: 0.64412\n",
            "Epoch: 2 | Iteration: 350 | Classification loss: 0.65088 | Regression loss: 0.47644 | Running loss: 0.64525\n",
            "Epoch: 2 | Iteration: 351 | Classification loss: 0.38878 | Regression loss: 0.40466 | Running loss: 0.64549\n",
            "Epoch: 2 | Iteration: 352 | Classification loss: 0.12937 | Regression loss: 0.14887 | Running loss: 0.64382\n",
            "Epoch: 2 | Iteration: 353 | Classification loss: 0.43840 | Regression loss: 0.64025 | Running loss: 0.64510\n",
            "Epoch: 2 | Iteration: 354 | Classification loss: 0.00047 | Regression loss: 0.31129 | Running loss: 0.64349\n",
            "Epoch: 2 | Iteration: 355 | Classification loss: 0.00090 | Regression loss: 0.00000 | Running loss: 0.64308\n",
            "Epoch: 2 | Iteration: 356 | Classification loss: 0.12012 | Regression loss: 0.44036 | Running loss: 0.64264\n",
            "Epoch: 2 | Iteration: 357 | Classification loss: 0.23011 | Regression loss: 0.39710 | Running loss: 0.64275\n",
            "Epoch: 2 | Iteration: 358 | Classification loss: 0.00053 | Regression loss: 0.27028 | Running loss: 0.64180\n",
            "Epoch: 2 | Iteration: 359 | Classification loss: 0.24862 | Regression loss: 0.48982 | Running loss: 0.64059\n",
            "Epoch: 2 | Iteration: 360 | Classification loss: 0.42517 | Regression loss: 0.77258 | Running loss: 0.64038\n",
            "Epoch: 2 | Iteration: 361 | Classification loss: 0.00045 | Regression loss: 0.08780 | Running loss: 0.63975\n",
            "Epoch: 2 | Iteration: 362 | Classification loss: 0.22580 | Regression loss: 0.50734 | Running loss: 0.63873\n",
            "Epoch: 2 | Iteration: 363 | Classification loss: 0.12146 | Regression loss: 0.45495 | Running loss: 0.63857\n",
            "Epoch: 2 | Iteration: 364 | Classification loss: 0.24148 | Regression loss: 0.55834 | Running loss: 0.63836\n",
            "Epoch: 2 | Iteration: 365 | Classification loss: 0.16576 | Regression loss: 0.34800 | Running loss: 0.63726\n",
            "Epoch: 2 | Iteration: 366 | Classification loss: 0.07284 | Regression loss: 0.41214 | Running loss: 0.63669\n",
            "Epoch: 2 | Iteration: 367 | Classification loss: 0.01227 | Regression loss: 0.14493 | Running loss: 0.63488\n",
            "Epoch: 2 | Iteration: 368 | Classification loss: 0.22018 | Regression loss: 0.62111 | Running loss: 0.63425\n",
            "Epoch: 2 | Iteration: 369 | Classification loss: 0.06465 | Regression loss: 0.40303 | Running loss: 0.63329\n",
            "Epoch: 2 | Iteration: 370 | Classification loss: 0.07852 | Regression loss: 0.19505 | Running loss: 0.63255\n",
            "Epoch: 2 | Iteration: 371 | Classification loss: 0.24744 | Regression loss: 0.62393 | Running loss: 0.63363\n",
            "Epoch: 2 | Iteration: 372 | Classification loss: 0.08196 | Regression loss: 0.16368 | Running loss: 0.63405\n",
            "Epoch: 2 | Iteration: 373 | Classification loss: 0.14913 | Regression loss: 0.32472 | Running loss: 0.63441\n",
            "Epoch: 2 | Iteration: 374 | Classification loss: 0.07567 | Regression loss: 0.20857 | Running loss: 0.63361\n",
            "Epoch: 2 | Iteration: 375 | Classification loss: 0.08511 | Regression loss: 0.17766 | Running loss: 0.63169\n",
            "Epoch: 2 | Iteration: 376 | Classification loss: 0.27255 | Regression loss: 0.47892 | Running loss: 0.63142\n",
            "Epoch: 2 | Iteration: 377 | Classification loss: 0.13832 | Regression loss: 0.29887 | Running loss: 0.62999\n",
            "Epoch: 2 | Iteration: 378 | Classification loss: 0.34895 | Regression loss: 0.35252 | Running loss: 0.63104\n",
            "Epoch: 2 | Iteration: 379 | Classification loss: 0.04208 | Regression loss: 0.27917 | Running loss: 0.62984\n",
            "Epoch: 2 | Iteration: 380 | Classification loss: 0.11215 | Regression loss: 0.38100 | Running loss: 0.62972\n",
            "Epoch: 2 | Iteration: 381 | Classification loss: 0.21719 | Regression loss: 0.49106 | Running loss: 0.63024\n",
            "Epoch: 2 | Iteration: 382 | Classification loss: 0.61915 | Regression loss: 0.54555 | Running loss: 0.63073\n",
            "Epoch: 2 | Iteration: 383 | Classification loss: 0.15184 | Regression loss: 0.37882 | Running loss: 0.63153\n",
            "Epoch: 2 | Iteration: 384 | Classification loss: 0.12798 | Regression loss: 0.32647 | Running loss: 0.63142\n",
            "Epoch: 2 | Iteration: 385 | Classification loss: 0.05895 | Regression loss: 0.22959 | Running loss: 0.63014\n",
            "Epoch: 2 | Iteration: 386 | Classification loss: 0.00314 | Regression loss: 0.04570 | Running loss: 0.63014\n",
            "Epoch: 2 | Iteration: 387 | Classification loss: 0.19586 | Regression loss: 0.36979 | Running loss: 0.62938\n",
            "Epoch: 2 | Iteration: 388 | Classification loss: 0.34886 | Regression loss: 0.67882 | Running loss: 0.63042\n",
            "Epoch: 2 | Iteration: 389 | Classification loss: 0.13973 | Regression loss: 0.24372 | Running loss: 0.62996\n",
            "Epoch: 2 | Iteration: 390 | Classification loss: 0.31721 | Regression loss: 0.64087 | Running loss: 0.62937\n",
            "Epoch: 2 | Iteration: 391 | Classification loss: 0.00190 | Regression loss: 0.11192 | Running loss: 0.62774\n",
            "Epoch: 2 | Iteration: 392 | Classification loss: 0.08078 | Regression loss: 0.27680 | Running loss: 0.62831\n",
            "Epoch: 2 | Iteration: 393 | Classification loss: 0.16125 | Regression loss: 0.21091 | Running loss: 0.62765\n",
            "Epoch: 2 | Iteration: 394 | Classification loss: 0.32554 | Regression loss: 0.57719 | Running loss: 0.62864\n",
            "Epoch: 2 | Iteration: 395 | Classification loss: 0.33157 | Regression loss: 0.48404 | Running loss: 0.62861\n",
            "Epoch: 2 | Iteration: 396 | Classification loss: 0.20537 | Regression loss: 0.45882 | Running loss: 0.62768\n",
            "Epoch: 2 | Iteration: 397 | Classification loss: 0.34085 | Regression loss: 0.45789 | Running loss: 0.62784\n",
            "Epoch: 2 | Iteration: 398 | Classification loss: 0.13318 | Regression loss: 0.30588 | Running loss: 0.62861\n",
            "Epoch: 2 | Iteration: 399 | Classification loss: 0.00052 | Regression loss: 0.04291 | Running loss: 0.62730\n",
            "Epoch: 2 | Iteration: 400 | Classification loss: 0.12323 | Regression loss: 0.35935 | Running loss: 0.62717\n",
            "Epoch: 2 | Iteration: 401 | Classification loss: 0.09402 | Regression loss: 0.35287 | Running loss: 0.62671\n",
            "Epoch: 2 | Iteration: 402 | Classification loss: 0.35107 | Regression loss: 0.48682 | Running loss: 0.62622\n",
            "Epoch: 2 | Iteration: 403 | Classification loss: 0.09448 | Regression loss: 0.30089 | Running loss: 0.62545\n",
            "Epoch: 2 | Iteration: 404 | Classification loss: 0.23503 | Regression loss: 0.34503 | Running loss: 0.62441\n",
            "Epoch: 2 | Iteration: 405 | Classification loss: 0.01980 | Regression loss: 0.01899 | Running loss: 0.62273\n",
            "Epoch: 2 | Iteration: 406 | Classification loss: 0.32072 | Regression loss: 0.58797 | Running loss: 0.62447\n",
            "Epoch: 2 | Iteration: 407 | Classification loss: 0.24279 | Regression loss: 0.72932 | Running loss: 0.62450\n",
            "Epoch: 2 | Iteration: 408 | Classification loss: 0.00037 | Regression loss: 0.05924 | Running loss: 0.62367\n",
            "Epoch: 2 | Iteration: 409 | Classification loss: 0.50586 | Regression loss: 0.51200 | Running loss: 0.62352\n",
            "Epoch: 2 | Iteration: 410 | Classification loss: 0.23761 | Regression loss: 0.56240 | Running loss: 0.62358\n",
            "Epoch: 2 | Iteration: 411 | Classification loss: 0.16256 | Regression loss: 0.39617 | Running loss: 0.62340\n",
            "Epoch: 2 | Iteration: 412 | Classification loss: 0.22114 | Regression loss: 0.56616 | Running loss: 0.62283\n",
            "Epoch: 2 | Iteration: 413 | Classification loss: 0.34836 | Regression loss: 0.64755 | Running loss: 0.62404\n",
            "Epoch: 2 | Iteration: 414 | Classification loss: 0.42859 | Regression loss: 0.69222 | Running loss: 0.62526\n",
            "Epoch: 2 | Iteration: 415 | Classification loss: 0.00078 | Regression loss: 0.04264 | Running loss: 0.62446\n",
            "Epoch: 2 | Iteration: 416 | Classification loss: 0.00169 | Regression loss: 0.04097 | Running loss: 0.62366\n",
            "Epoch: 2 | Iteration: 417 | Classification loss: 0.07135 | Regression loss: 0.14540 | Running loss: 0.62333\n",
            "Epoch: 2 | Iteration: 418 | Classification loss: 0.78724 | Regression loss: 0.79623 | Running loss: 0.62420\n",
            "Epoch: 2 | Iteration: 419 | Classification loss: 0.00105 | Regression loss: 0.01683 | Running loss: 0.62213\n",
            "Epoch: 2 | Iteration: 420 | Classification loss: 0.16425 | Regression loss: 0.45030 | Running loss: 0.62145\n",
            "Epoch: 2 | Iteration: 421 | Classification loss: 0.47463 | Regression loss: 0.67045 | Running loss: 0.62228\n",
            "Epoch: 2 | Iteration: 422 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.62037\n",
            "Epoch: 2 | Iteration: 423 | Classification loss: 0.06148 | Regression loss: 0.01678 | Running loss: 0.61872\n",
            "Epoch: 2 | Iteration: 424 | Classification loss: 0.16019 | Regression loss: 0.44520 | Running loss: 0.61573\n",
            "Epoch: 2 | Iteration: 425 | Classification loss: 0.00039 | Regression loss: 0.02368 | Running loss: 0.61451\n",
            "Epoch: 2 | Iteration: 426 | Classification loss: 0.03197 | Regression loss: 0.12515 | Running loss: 0.61303\n",
            "Epoch: 2 | Iteration: 427 | Classification loss: 0.37562 | Regression loss: 0.48689 | Running loss: 0.61206\n",
            "Epoch: 2 | Iteration: 428 | Classification loss: 0.31917 | Regression loss: 0.72376 | Running loss: 0.61209\n",
            "Epoch: 2 | Iteration: 429 | Classification loss: 0.11250 | Regression loss: 0.15690 | Running loss: 0.61220\n",
            "Epoch: 2 | Iteration: 430 | Classification loss: 0.36071 | Regression loss: 0.63670 | Running loss: 0.61384\n",
            "Epoch: 2 | Iteration: 431 | Classification loss: 0.25473 | Regression loss: 0.28594 | Running loss: 0.61337\n",
            "Epoch: 2 | Iteration: 432 | Classification loss: 0.21543 | Regression loss: 0.46054 | Running loss: 0.61324\n",
            "Epoch: 2 | Iteration: 433 | Classification loss: 0.14360 | Regression loss: 0.38029 | Running loss: 0.61413\n",
            "Epoch: 2 | Iteration: 434 | Classification loss: 0.00169 | Regression loss: 0.00000 | Running loss: 0.61283\n",
            "Epoch: 2 | Iteration: 435 | Classification loss: 0.36577 | Regression loss: 0.52420 | Running loss: 0.61321\n",
            "Epoch: 2 | Iteration: 436 | Classification loss: 0.38942 | Regression loss: 0.51181 | Running loss: 0.61362\n",
            "Epoch: 2 | Iteration: 437 | Classification loss: 0.17203 | Regression loss: 0.43621 | Running loss: 0.61407\n",
            "Epoch: 2 | Iteration: 438 | Classification loss: 0.28051 | Regression loss: 0.49740 | Running loss: 0.61405\n",
            "Epoch: 2 | Iteration: 439 | Classification loss: 0.21646 | Regression loss: 0.36547 | Running loss: 0.61452\n",
            "Epoch: 2 | Iteration: 440 | Classification loss: 0.06470 | Regression loss: 0.28382 | Running loss: 0.61507\n",
            "Epoch: 2 | Iteration: 441 | Classification loss: 0.14960 | Regression loss: 0.50303 | Running loss: 0.61622\n",
            "Epoch: 2 | Iteration: 442 | Classification loss: 0.20349 | Regression loss: 0.36074 | Running loss: 0.61633\n",
            "Epoch: 2 | Iteration: 443 | Classification loss: 0.26375 | Regression loss: 0.49172 | Running loss: 0.61768\n",
            "Epoch: 2 | Iteration: 444 | Classification loss: 0.12525 | Regression loss: 0.38680 | Running loss: 0.61682\n",
            "Epoch: 2 | Iteration: 445 | Classification loss: 0.00195 | Regression loss: 0.04267 | Running loss: 0.61567\n",
            "Epoch: 2 | Iteration: 446 | Classification loss: 0.12600 | Regression loss: 0.37826 | Running loss: 0.61547\n",
            "Epoch: 2 | Iteration: 447 | Classification loss: 0.07828 | Regression loss: 0.31236 | Running loss: 0.61544\n",
            "Epoch: 2 | Iteration: 448 | Classification loss: 0.32787 | Regression loss: 0.21587 | Running loss: 0.61635\n",
            "Epoch: 2 | Iteration: 449 | Classification loss: 0.00101 | Regression loss: 0.03594 | Running loss: 0.61423\n",
            "Epoch: 2 | Iteration: 450 | Classification loss: 0.21100 | Regression loss: 0.51226 | Running loss: 0.61480\n",
            "Epoch: 2 | Iteration: 451 | Classification loss: 0.26731 | Regression loss: 0.52116 | Running loss: 0.61515\n",
            "Epoch: 2 | Iteration: 452 | Classification loss: 0.08985 | Regression loss: 0.21127 | Running loss: 0.59530\n",
            "Epoch: 2 | Iteration: 453 | Classification loss: 0.08379 | Regression loss: 0.21351 | Running loss: 0.59578\n",
            "Epoch: 2 | Iteration: 454 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.59502\n",
            "Epoch: 2 | Iteration: 455 | Classification loss: 0.00007 | Regression loss: 0.00000 | Running loss: 0.59482\n",
            "Epoch: 2 | Iteration: 456 | Classification loss: 0.00024 | Regression loss: 0.02120 | Running loss: 0.59269\n",
            "Epoch: 2 | Iteration: 457 | Classification loss: 0.00092 | Regression loss: 0.01779 | Running loss: 0.59142\n",
            "Epoch: 2 | Iteration: 458 | Classification loss: 0.22272 | Regression loss: 0.61005 | Running loss: 0.59226\n",
            "Epoch: 2 | Iteration: 459 | Classification loss: 0.35190 | Regression loss: 0.62920 | Running loss: 0.59208\n",
            "Epoch: 2 | Iteration: 460 | Classification loss: 0.24376 | Regression loss: 0.55744 | Running loss: 0.59289\n",
            "Epoch: 2 | Iteration: 461 | Classification loss: 0.37847 | Regression loss: 0.56637 | Running loss: 0.59259\n",
            "Epoch: 2 | Iteration: 462 | Classification loss: 0.15815 | Regression loss: 0.43754 | Running loss: 0.59190\n",
            "Epoch: 2 | Iteration: 463 | Classification loss: 0.03974 | Regression loss: 0.04111 | Running loss: 0.58959\n",
            "Epoch: 2 | Iteration: 464 | Classification loss: 0.00054 | Regression loss: 0.07932 | Running loss: 0.58838\n",
            "Epoch: 2 | Iteration: 465 | Classification loss: 0.16970 | Regression loss: 0.29346 | Running loss: 0.58798\n",
            "Epoch: 2 | Iteration: 466 | Classification loss: 0.64403 | Regression loss: 0.83791 | Running loss: 0.58866\n",
            "Epoch: 2 | Iteration: 467 | Classification loss: 0.26645 | Regression loss: 0.57235 | Running loss: 0.58878\n",
            "Epoch: 2 | Iteration: 468 | Classification loss: 0.08873 | Regression loss: 0.25655 | Running loss: 0.58781\n",
            "Epoch: 2 | Iteration: 469 | Classification loss: 0.00178 | Regression loss: 0.14095 | Running loss: 0.58795\n",
            "Epoch: 2 | Iteration: 470 | Classification loss: 0.07258 | Regression loss: 0.27090 | Running loss: 0.58835\n",
            "Epoch: 2 | Iteration: 471 | Classification loss: 0.26863 | Regression loss: 0.24344 | Running loss: 0.58718\n",
            "Epoch: 2 | Iteration: 472 | Classification loss: 0.06941 | Regression loss: 0.21788 | Running loss: 0.58646\n",
            "Epoch: 2 | Iteration: 473 | Classification loss: 0.00047 | Regression loss: 0.02814 | Running loss: 0.58513\n",
            "Epoch: 2 | Iteration: 474 | Classification loss: 0.22251 | Regression loss: 0.29109 | Running loss: 0.58552\n",
            "Epoch: 2 | Iteration: 475 | Classification loss: 0.00133 | Regression loss: 0.00000 | Running loss: 0.58315\n",
            "Epoch: 2 | Iteration: 476 | Classification loss: 0.10058 | Regression loss: 0.24334 | Running loss: 0.58355\n",
            "Epoch: 2 | Iteration: 477 | Classification loss: 0.54149 | Regression loss: 0.74746 | Running loss: 0.58520\n",
            "Epoch: 2 | Iteration: 478 | Classification loss: 0.48052 | Regression loss: 0.79380 | Running loss: 0.58541\n",
            "Epoch: 2 | Iteration: 479 | Classification loss: 0.37231 | Regression loss: 0.53523 | Running loss: 0.58515\n",
            "Epoch: 2 | Iteration: 480 | Classification loss: 0.55891 | Regression loss: 0.59842 | Running loss: 0.58703\n",
            "Epoch: 2 | Iteration: 481 | Classification loss: 0.41423 | Regression loss: 0.45410 | Running loss: 0.58730\n",
            "Epoch: 2 | Iteration: 482 | Classification loss: 0.44731 | Regression loss: 0.57898 | Running loss: 0.58744\n",
            "Epoch: 2 | Iteration: 483 | Classification loss: 0.00463 | Regression loss: 0.04612 | Running loss: 0.58496\n",
            "Epoch: 2 | Iteration: 484 | Classification loss: 0.15052 | Regression loss: 0.36869 | Running loss: 0.58522\n",
            "Epoch: 2 | Iteration: 485 | Classification loss: 0.00069 | Regression loss: 0.04111 | Running loss: 0.58529\n",
            "Epoch: 2 | Iteration: 486 | Classification loss: 0.24266 | Regression loss: 0.55797 | Running loss: 0.58683\n",
            "Epoch: 2 | Iteration: 487 | Classification loss: 0.25506 | Regression loss: 0.44314 | Running loss: 0.58595\n",
            "Epoch: 2 | Iteration: 488 | Classification loss: 0.32034 | Regression loss: 0.73737 | Running loss: 0.58795\n",
            "Epoch: 2 | Iteration: 489 | Classification loss: 0.36608 | Regression loss: 0.50991 | Running loss: 0.58756\n",
            "Epoch: 2 | Iteration: 490 | Classification loss: 0.04388 | Regression loss: 0.06800 | Running loss: 0.58557\n",
            "Epoch: 2 | Iteration: 491 | Classification loss: 0.14355 | Regression loss: 0.38313 | Running loss: 0.58503\n",
            "Epoch: 2 | Iteration: 492 | Classification loss: 0.54648 | Regression loss: 0.58531 | Running loss: 0.58602\n",
            "Epoch: 2 | Iteration: 493 | Classification loss: 0.24568 | Regression loss: 0.54579 | Running loss: 0.58605\n",
            "Epoch: 2 | Iteration: 494 | Classification loss: 0.69793 | Regression loss: 0.05954 | Running loss: 0.58597\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.22s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.89s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.433\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.813\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.351\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.533\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.551\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 3 | Iteration: 0 | Classification loss: 0.06837 | Regression loss: 0.19294 | Running loss: 0.58538\n",
            "Epoch: 3 | Iteration: 1 | Classification loss: 0.06872 | Regression loss: 0.16487 | Running loss: 0.58481\n",
            "Epoch: 3 | Iteration: 2 | Classification loss: 0.32295 | Regression loss: 0.54067 | Running loss: 0.58640\n",
            "Epoch: 3 | Iteration: 3 | Classification loss: 0.00361 | Regression loss: 0.00000 | Running loss: 0.58444\n",
            "Epoch: 3 | Iteration: 4 | Classification loss: 0.29151 | Regression loss: 0.55676 | Running loss: 0.58425\n",
            "Epoch: 3 | Iteration: 5 | Classification loss: 0.00115 | Regression loss: 0.01457 | Running loss: 0.58280\n",
            "Epoch: 3 | Iteration: 6 | Classification loss: 0.24677 | Regression loss: 0.47265 | Running loss: 0.58411\n",
            "Epoch: 3 | Iteration: 7 | Classification loss: 0.17275 | Regression loss: 0.34224 | Running loss: 0.58271\n",
            "Epoch: 3 | Iteration: 8 | Classification loss: 0.22223 | Regression loss: 0.46086 | Running loss: 0.58330\n",
            "Epoch: 3 | Iteration: 9 | Classification loss: 0.00035 | Regression loss: 0.04035 | Running loss: 0.58130\n",
            "Epoch: 3 | Iteration: 10 | Classification loss: 0.55345 | Regression loss: 0.65592 | Running loss: 0.58305\n",
            "Epoch: 3 | Iteration: 11 | Classification loss: 0.46482 | Regression loss: 0.55627 | Running loss: 0.58394\n",
            "Epoch: 3 | Iteration: 12 | Classification loss: 0.31202 | Regression loss: 0.49154 | Running loss: 0.58293\n",
            "Epoch: 3 | Iteration: 13 | Classification loss: 0.14742 | Regression loss: 0.31123 | Running loss: 0.58377\n",
            "Epoch: 3 | Iteration: 14 | Classification loss: 0.29901 | Regression loss: 0.47905 | Running loss: 0.58382\n",
            "Epoch: 3 | Iteration: 15 | Classification loss: 0.00043 | Regression loss: 0.02793 | Running loss: 0.58242\n",
            "Epoch: 3 | Iteration: 16 | Classification loss: 0.04710 | Regression loss: 0.00000 | Running loss: 0.58251\n",
            "Epoch: 3 | Iteration: 17 | Classification loss: 0.20156 | Regression loss: 0.42078 | Running loss: 0.58102\n",
            "Epoch: 3 | Iteration: 18 | Classification loss: 0.00056 | Regression loss: 0.04363 | Running loss: 0.57950\n",
            "Epoch: 3 | Iteration: 19 | Classification loss: 0.00450 | Regression loss: 0.00990 | Running loss: 0.57944\n",
            "Epoch: 3 | Iteration: 20 | Classification loss: 0.16106 | Regression loss: 0.41937 | Running loss: 0.58056\n",
            "Epoch: 3 | Iteration: 21 | Classification loss: 0.11605 | Regression loss: 0.36425 | Running loss: 0.58044\n",
            "Epoch: 3 | Iteration: 22 | Classification loss: 0.06307 | Regression loss: 0.18975 | Running loss: 0.57952\n",
            "Epoch: 3 | Iteration: 23 | Classification loss: 0.19186 | Regression loss: 0.40646 | Running loss: 0.58068\n",
            "Epoch: 3 | Iteration: 24 | Classification loss: 0.34328 | Regression loss: 0.69184 | Running loss: 0.58016\n",
            "Epoch: 3 | Iteration: 25 | Classification loss: 0.21605 | Regression loss: 0.29505 | Running loss: 0.58087\n",
            "Epoch: 3 | Iteration: 26 | Classification loss: 0.11684 | Regression loss: 0.42020 | Running loss: 0.57997\n",
            "Epoch: 3 | Iteration: 27 | Classification loss: 0.20861 | Regression loss: 0.30791 | Running loss: 0.57979\n",
            "Epoch: 3 | Iteration: 28 | Classification loss: 0.00143 | Regression loss: 0.04995 | Running loss: 0.57828\n",
            "Epoch: 3 | Iteration: 29 | Classification loss: 0.17194 | Regression loss: 0.39553 | Running loss: 0.57827\n",
            "Epoch: 3 | Iteration: 30 | Classification loss: 0.14265 | Regression loss: 0.38103 | Running loss: 0.57708\n",
            "Epoch: 3 | Iteration: 31 | Classification loss: 0.23677 | Regression loss: 0.58816 | Running loss: 0.57744\n",
            "Epoch: 3 | Iteration: 32 | Classification loss: 0.40470 | Regression loss: 0.68474 | Running loss: 0.57789\n",
            "Epoch: 3 | Iteration: 33 | Classification loss: 0.17816 | Regression loss: 0.42979 | Running loss: 0.57747\n",
            "Epoch: 3 | Iteration: 34 | Classification loss: 0.00056 | Regression loss: 0.09356 | Running loss: 0.57649\n",
            "Epoch: 3 | Iteration: 35 | Classification loss: 0.52688 | Regression loss: 0.83027 | Running loss: 0.57788\n",
            "Epoch: 3 | Iteration: 36 | Classification loss: 0.21116 | Regression loss: 0.57452 | Running loss: 0.57796\n",
            "Epoch: 3 | Iteration: 37 | Classification loss: 0.09506 | Regression loss: 0.26340 | Running loss: 0.57740\n",
            "Epoch: 3 | Iteration: 38 | Classification loss: 0.01677 | Regression loss: 0.01346 | Running loss: 0.57535\n",
            "Epoch: 3 | Iteration: 39 | Classification loss: 0.16331 | Regression loss: 0.19074 | Running loss: 0.57473\n",
            "Epoch: 3 | Iteration: 40 | Classification loss: 0.20223 | Regression loss: 0.43866 | Running loss: 0.57586\n",
            "Epoch: 3 | Iteration: 41 | Classification loss: 0.27776 | Regression loss: 0.64957 | Running loss: 0.57579\n",
            "Epoch: 3 | Iteration: 42 | Classification loss: 0.07013 | Regression loss: 0.19421 | Running loss: 0.57541\n",
            "Epoch: 3 | Iteration: 43 | Classification loss: 0.14449 | Regression loss: 0.44745 | Running loss: 0.57526\n",
            "Epoch: 3 | Iteration: 44 | Classification loss: 0.05466 | Regression loss: 0.19635 | Running loss: 0.57492\n",
            "Epoch: 3 | Iteration: 45 | Classification loss: 0.19275 | Regression loss: 0.35941 | Running loss: 0.57501\n",
            "Epoch: 3 | Iteration: 46 | Classification loss: 0.32616 | Regression loss: 0.58718 | Running loss: 0.57561\n",
            "Epoch: 3 | Iteration: 47 | Classification loss: 0.08613 | Regression loss: 0.22584 | Running loss: 0.57457\n",
            "Epoch: 3 | Iteration: 48 | Classification loss: 0.14360 | Regression loss: 0.41005 | Running loss: 0.57552\n",
            "Epoch: 3 | Iteration: 49 | Classification loss: 0.36606 | Regression loss: 0.65636 | Running loss: 0.57586\n",
            "Epoch: 3 | Iteration: 50 | Classification loss: 0.26678 | Regression loss: 0.45716 | Running loss: 0.57655\n",
            "Epoch: 3 | Iteration: 51 | Classification loss: 0.06106 | Regression loss: 0.18373 | Running loss: 0.57571\n",
            "Epoch: 3 | Iteration: 52 | Classification loss: 0.05396 | Regression loss: 0.18450 | Running loss: 0.57549\n",
            "Epoch: 3 | Iteration: 53 | Classification loss: 0.12652 | Regression loss: 0.34030 | Running loss: 0.57637\n",
            "Epoch: 3 | Iteration: 54 | Classification loss: 0.06216 | Regression loss: 0.19642 | Running loss: 0.57602\n",
            "Epoch: 3 | Iteration: 55 | Classification loss: 0.12466 | Regression loss: 0.35838 | Running loss: 0.57604\n",
            "Epoch: 3 | Iteration: 56 | Classification loss: 0.28413 | Regression loss: 0.43087 | Running loss: 0.57732\n",
            "Epoch: 3 | Iteration: 57 | Classification loss: 0.01546 | Regression loss: 0.02267 | Running loss: 0.57628\n",
            "Epoch: 3 | Iteration: 58 | Classification loss: 0.34890 | Regression loss: 0.40122 | Running loss: 0.57634\n",
            "Epoch: 3 | Iteration: 59 | Classification loss: 0.31052 | Regression loss: 0.62908 | Running loss: 0.57708\n",
            "Epoch: 3 | Iteration: 60 | Classification loss: 0.20382 | Regression loss: 0.47305 | Running loss: 0.57708\n",
            "Epoch: 3 | Iteration: 61 | Classification loss: 0.10960 | Regression loss: 0.47159 | Running loss: 0.57811\n",
            "Epoch: 3 | Iteration: 62 | Classification loss: 0.12880 | Regression loss: 0.42474 | Running loss: 0.57771\n",
            "Epoch: 3 | Iteration: 63 | Classification loss: 0.06316 | Regression loss: 0.27732 | Running loss: 0.57609\n",
            "Epoch: 3 | Iteration: 64 | Classification loss: 0.08719 | Regression loss: 0.38693 | Running loss: 0.57458\n",
            "Epoch: 3 | Iteration: 65 | Classification loss: 0.21885 | Regression loss: 0.38561 | Running loss: 0.57394\n",
            "Epoch: 3 | Iteration: 66 | Classification loss: 0.30509 | Regression loss: 0.45558 | Running loss: 0.57268\n",
            "Epoch: 3 | Iteration: 67 | Classification loss: 0.00052 | Regression loss: 0.06033 | Running loss: 0.57135\n",
            "Epoch: 3 | Iteration: 68 | Classification loss: 0.05221 | Regression loss: 0.30985 | Running loss: 0.57095\n",
            "Epoch: 3 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.57021\n",
            "Epoch: 3 | Iteration: 70 | Classification loss: 0.28380 | Regression loss: 0.45164 | Running loss: 0.57149\n",
            "Epoch: 3 | Iteration: 71 | Classification loss: 0.05647 | Regression loss: 0.24334 | Running loss: 0.57070\n",
            "Epoch: 3 | Iteration: 72 | Classification loss: 0.14816 | Regression loss: 0.36105 | Running loss: 0.57156\n",
            "Epoch: 3 | Iteration: 73 | Classification loss: 0.20922 | Regression loss: 0.61354 | Running loss: 0.57193\n",
            "Epoch: 3 | Iteration: 74 | Classification loss: 0.20139 | Regression loss: 0.50877 | Running loss: 0.57323\n",
            "Epoch: 3 | Iteration: 75 | Classification loss: 0.11361 | Regression loss: 0.25073 | Running loss: 0.57389\n",
            "Epoch: 3 | Iteration: 76 | Classification loss: 0.03842 | Regression loss: 0.28164 | Running loss: 0.57295\n",
            "Epoch: 3 | Iteration: 77 | Classification loss: 0.19651 | Regression loss: 0.38335 | Running loss: 0.57399\n",
            "Epoch: 3 | Iteration: 78 | Classification loss: 0.14017 | Regression loss: 0.32161 | Running loss: 0.57368\n",
            "Epoch: 3 | Iteration: 79 | Classification loss: 0.39289 | Regression loss: 0.50880 | Running loss: 0.57430\n",
            "Epoch: 3 | Iteration: 80 | Classification loss: 0.14588 | Regression loss: 0.37392 | Running loss: 0.57391\n",
            "Epoch: 3 | Iteration: 81 | Classification loss: 0.29068 | Regression loss: 0.61432 | Running loss: 0.57459\n",
            "Epoch: 3 | Iteration: 82 | Classification loss: 0.13200 | Regression loss: 0.33027 | Running loss: 0.57418\n",
            "Epoch: 3 | Iteration: 83 | Classification loss: 0.26200 | Regression loss: 0.50781 | Running loss: 0.57498\n",
            "Epoch: 3 | Iteration: 84 | Classification loss: 0.20901 | Regression loss: 0.44252 | Running loss: 0.57468\n",
            "Epoch: 3 | Iteration: 85 | Classification loss: 0.04883 | Regression loss: 0.22829 | Running loss: 0.57457\n",
            "Epoch: 3 | Iteration: 86 | Classification loss: 0.15788 | Regression loss: 0.49410 | Running loss: 0.57493\n",
            "Epoch: 3 | Iteration: 87 | Classification loss: 0.16900 | Regression loss: 0.40690 | Running loss: 0.57546\n",
            "Epoch: 3 | Iteration: 88 | Classification loss: 0.09856 | Regression loss: 0.30787 | Running loss: 0.57565\n",
            "Epoch: 3 | Iteration: 89 | Classification loss: 0.00069 | Regression loss: 0.07400 | Running loss: 0.57474\n",
            "Epoch: 3 | Iteration: 90 | Classification loss: 0.34405 | Regression loss: 0.30029 | Running loss: 0.57395\n",
            "Epoch: 3 | Iteration: 91 | Classification loss: 0.29851 | Regression loss: 0.41964 | Running loss: 0.57522\n",
            "Epoch: 3 | Iteration: 92 | Classification loss: 0.01919 | Regression loss: 0.03957 | Running loss: 0.57451\n",
            "Epoch: 3 | Iteration: 93 | Classification loss: 0.19012 | Regression loss: 0.44824 | Running loss: 0.57573\n",
            "Epoch: 3 | Iteration: 94 | Classification loss: 0.15848 | Regression loss: 0.42851 | Running loss: 0.57455\n",
            "Epoch: 3 | Iteration: 95 | Classification loss: 0.03915 | Regression loss: 0.16954 | Running loss: 0.57485\n",
            "Epoch: 3 | Iteration: 96 | Classification loss: 0.00205 | Regression loss: 0.01866 | Running loss: 0.57367\n",
            "Epoch: 3 | Iteration: 97 | Classification loss: 0.00326 | Regression loss: 0.02631 | Running loss: 0.57301\n",
            "Epoch: 3 | Iteration: 98 | Classification loss: 0.10861 | Regression loss: 0.26500 | Running loss: 0.57201\n",
            "Epoch: 3 | Iteration: 99 | Classification loss: 0.00525 | Regression loss: 0.05944 | Running loss: 0.57210\n",
            "Epoch: 3 | Iteration: 100 | Classification loss: 0.22589 | Regression loss: 0.46883 | Running loss: 0.57260\n",
            "Epoch: 3 | Iteration: 101 | Classification loss: 0.22270 | Regression loss: 0.43554 | Running loss: 0.57266\n",
            "Epoch: 3 | Iteration: 102 | Classification loss: 0.27443 | Regression loss: 0.50769 | Running loss: 0.57376\n",
            "Epoch: 3 | Iteration: 103 | Classification loss: 0.23072 | Regression loss: 0.62033 | Running loss: 0.57349\n",
            "Epoch: 3 | Iteration: 104 | Classification loss: 0.15815 | Regression loss: 0.39276 | Running loss: 0.57421\n",
            "Epoch: 3 | Iteration: 105 | Classification loss: 0.10771 | Regression loss: 0.33128 | Running loss: 0.57386\n",
            "Epoch: 3 | Iteration: 106 | Classification loss: 0.11219 | Regression loss: 0.29787 | Running loss: 0.57269\n",
            "Epoch: 3 | Iteration: 107 | Classification loss: 0.04737 | Regression loss: 0.23527 | Running loss: 0.57257\n",
            "Epoch: 3 | Iteration: 108 | Classification loss: 0.25424 | Regression loss: 0.50117 | Running loss: 0.57272\n",
            "Epoch: 3 | Iteration: 109 | Classification loss: 0.09144 | Regression loss: 0.19820 | Running loss: 0.57211\n",
            "Epoch: 3 | Iteration: 110 | Classification loss: 0.00093 | Regression loss: 0.03506 | Running loss: 0.57154\n",
            "Epoch: 3 | Iteration: 111 | Classification loss: 0.24730 | Regression loss: 0.38174 | Running loss: 0.57064\n",
            "Epoch: 3 | Iteration: 112 | Classification loss: 0.10460 | Regression loss: 0.23735 | Running loss: 0.56996\n",
            "Epoch: 3 | Iteration: 113 | Classification loss: 0.10337 | Regression loss: 0.00790 | Running loss: 0.56902\n",
            "Epoch: 3 | Iteration: 114 | Classification loss: 0.07659 | Regression loss: 0.17364 | Running loss: 0.56867\n",
            "Epoch: 3 | Iteration: 115 | Classification loss: 0.00111 | Regression loss: 0.00000 | Running loss: 0.56716\n",
            "Epoch: 3 | Iteration: 116 | Classification loss: 0.20438 | Regression loss: 0.41481 | Running loss: 0.56695\n",
            "Epoch: 3 | Iteration: 117 | Classification loss: 0.08831 | Regression loss: 0.36964 | Running loss: 0.56593\n",
            "Epoch: 3 | Iteration: 118 | Classification loss: 0.30336 | Regression loss: 0.52943 | Running loss: 0.56545\n",
            "Epoch: 3 | Iteration: 119 | Classification loss: 0.17782 | Regression loss: 0.53501 | Running loss: 0.56633\n",
            "Epoch: 3 | Iteration: 120 | Classification loss: 0.13286 | Regression loss: 0.46828 | Running loss: 0.56605\n",
            "Epoch: 3 | Iteration: 121 | Classification loss: 0.11675 | Regression loss: 0.20618 | Running loss: 0.56458\n",
            "Epoch: 3 | Iteration: 122 | Classification loss: 0.06988 | Regression loss: 0.29382 | Running loss: 0.56531\n",
            "Epoch: 3 | Iteration: 123 | Classification loss: 0.31051 | Regression loss: 0.53008 | Running loss: 0.56666\n",
            "Epoch: 3 | Iteration: 124 | Classification loss: 0.20721 | Regression loss: 0.41267 | Running loss: 0.56668\n",
            "Epoch: 3 | Iteration: 125 | Classification loss: 0.34164 | Regression loss: 0.61838 | Running loss: 0.56805\n",
            "Epoch: 3 | Iteration: 126 | Classification loss: 0.08456 | Regression loss: 0.26719 | Running loss: 0.56753\n",
            "Epoch: 3 | Iteration: 127 | Classification loss: 0.00077 | Regression loss: 0.05115 | Running loss: 0.56636\n",
            "Epoch: 3 | Iteration: 128 | Classification loss: 0.00869 | Regression loss: 0.03743 | Running loss: 0.56490\n",
            "Epoch: 3 | Iteration: 129 | Classification loss: 0.63312 | Regression loss: 0.22636 | Running loss: 0.56538\n",
            "Epoch: 3 | Iteration: 130 | Classification loss: 0.15188 | Regression loss: 0.26380 | Running loss: 0.56499\n",
            "Epoch: 3 | Iteration: 131 | Classification loss: 0.50156 | Regression loss: 0.69012 | Running loss: 0.56687\n",
            "Epoch: 3 | Iteration: 132 | Classification loss: 0.00084 | Regression loss: 0.03995 | Running loss: 0.56559\n",
            "Epoch: 3 | Iteration: 133 | Classification loss: 0.20432 | Regression loss: 0.24153 | Running loss: 0.56569\n",
            "Epoch: 3 | Iteration: 134 | Classification loss: 0.32257 | Regression loss: 0.65298 | Running loss: 0.56650\n",
            "Epoch: 3 | Iteration: 135 | Classification loss: 0.06992 | Regression loss: 0.18120 | Running loss: 0.56575\n",
            "Epoch: 3 | Iteration: 136 | Classification loss: 0.05882 | Regression loss: 0.20976 | Running loss: 0.56560\n",
            "Epoch: 3 | Iteration: 137 | Classification loss: 0.00435 | Regression loss: 0.00000 | Running loss: 0.56560\n",
            "Epoch: 3 | Iteration: 138 | Classification loss: 0.19296 | Regression loss: 0.50996 | Running loss: 0.56394\n",
            "Epoch: 3 | Iteration: 139 | Classification loss: 0.13477 | Regression loss: 0.50294 | Running loss: 0.56519\n",
            "Epoch: 3 | Iteration: 140 | Classification loss: 0.08683 | Regression loss: 0.17932 | Running loss: 0.56272\n",
            "Epoch: 3 | Iteration: 141 | Classification loss: 0.10954 | Regression loss: 0.33677 | Running loss: 0.56155\n",
            "Epoch: 3 | Iteration: 142 | Classification loss: 0.09824 | Regression loss: 0.29043 | Running loss: 0.56233\n",
            "Epoch: 3 | Iteration: 143 | Classification loss: 0.00020 | Regression loss: 0.01474 | Running loss: 0.56115\n",
            "Epoch: 3 | Iteration: 144 | Classification loss: 0.04815 | Regression loss: 0.16443 | Running loss: 0.56064\n",
            "Epoch: 3 | Iteration: 145 | Classification loss: 0.00032 | Regression loss: 0.03755 | Running loss: 0.55908\n",
            "Epoch: 3 | Iteration: 146 | Classification loss: 0.08929 | Regression loss: 0.30560 | Running loss: 0.55928\n",
            "Epoch: 3 | Iteration: 147 | Classification loss: 0.40444 | Regression loss: 0.33119 | Running loss: 0.56053\n",
            "Epoch: 3 | Iteration: 148 | Classification loss: 0.20825 | Regression loss: 0.34854 | Running loss: 0.56071\n",
            "Epoch: 3 | Iteration: 149 | Classification loss: 0.00095 | Regression loss: 0.05686 | Running loss: 0.55914\n",
            "Epoch: 3 | Iteration: 150 | Classification loss: 0.13838 | Regression loss: 0.24357 | Running loss: 0.55716\n",
            "Epoch: 3 | Iteration: 151 | Classification loss: 0.12478 | Regression loss: 0.42681 | Running loss: 0.55679\n",
            "Epoch: 3 | Iteration: 152 | Classification loss: 0.22488 | Regression loss: 0.37664 | Running loss: 0.55587\n",
            "Epoch: 3 | Iteration: 153 | Classification loss: 0.07694 | Regression loss: 0.22983 | Running loss: 0.55561\n",
            "Epoch: 3 | Iteration: 154 | Classification loss: 0.17640 | Regression loss: 0.36706 | Running loss: 0.55502\n",
            "Epoch: 3 | Iteration: 155 | Classification loss: 0.07369 | Regression loss: 0.19187 | Running loss: 0.55376\n",
            "Epoch: 3 | Iteration: 156 | Classification loss: 0.33737 | Regression loss: 0.37373 | Running loss: 0.55454\n",
            "Epoch: 3 | Iteration: 157 | Classification loss: 0.22628 | Regression loss: 0.47490 | Running loss: 0.55546\n",
            "Epoch: 3 | Iteration: 158 | Classification loss: 0.32616 | Regression loss: 0.62982 | Running loss: 0.55544\n",
            "Epoch: 3 | Iteration: 159 | Classification loss: 0.24410 | Regression loss: 0.45900 | Running loss: 0.55609\n",
            "Epoch: 3 | Iteration: 160 | Classification loss: 0.04259 | Regression loss: 0.13700 | Running loss: 0.55645\n",
            "Epoch: 3 | Iteration: 161 | Classification loss: 0.18136 | Regression loss: 0.41480 | Running loss: 0.55753\n",
            "Epoch: 3 | Iteration: 162 | Classification loss: 0.06057 | Regression loss: 0.20349 | Running loss: 0.55677\n",
            "Epoch: 3 | Iteration: 163 | Classification loss: 0.18394 | Regression loss: 0.40263 | Running loss: 0.55669\n",
            "Epoch: 3 | Iteration: 164 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.55513\n",
            "Epoch: 3 | Iteration: 165 | Classification loss: 0.32854 | Regression loss: 0.40655 | Running loss: 0.55591\n",
            "Epoch: 3 | Iteration: 166 | Classification loss: 0.11891 | Regression loss: 0.31841 | Running loss: 0.55607\n",
            "Epoch: 3 | Iteration: 167 | Classification loss: 0.49088 | Regression loss: 0.45562 | Running loss: 0.55716\n",
            "Epoch: 3 | Iteration: 168 | Classification loss: 0.23133 | Regression loss: 0.53615 | Running loss: 0.55687\n",
            "Epoch: 3 | Iteration: 169 | Classification loss: 0.11936 | Regression loss: 0.18703 | Running loss: 0.55579\n",
            "Epoch: 3 | Iteration: 170 | Classification loss: 0.27078 | Regression loss: 0.63141 | Running loss: 0.55616\n",
            "Epoch: 3 | Iteration: 171 | Classification loss: 0.00029 | Regression loss: 0.01984 | Running loss: 0.55504\n",
            "Epoch: 3 | Iteration: 172 | Classification loss: 0.03094 | Regression loss: 0.01610 | Running loss: 0.55277\n",
            "Epoch: 3 | Iteration: 173 | Classification loss: 0.07186 | Regression loss: 0.24427 | Running loss: 0.55205\n",
            "Epoch: 3 | Iteration: 174 | Classification loss: 0.10664 | Regression loss: 0.29428 | Running loss: 0.55180\n",
            "Epoch: 3 | Iteration: 175 | Classification loss: 0.24174 | Regression loss: 0.56273 | Running loss: 0.55127\n",
            "Epoch: 3 | Iteration: 176 | Classification loss: 0.10737 | Regression loss: 0.33416 | Running loss: 0.54899\n",
            "Epoch: 3 | Iteration: 177 | Classification loss: 0.29261 | Regression loss: 0.55546 | Running loss: 0.54911\n",
            "Epoch: 3 | Iteration: 178 | Classification loss: 0.13301 | Regression loss: 0.35446 | Running loss: 0.54797\n",
            "Epoch: 3 | Iteration: 179 | Classification loss: 0.18449 | Regression loss: 0.45095 | Running loss: 0.54809\n",
            "Epoch: 3 | Iteration: 180 | Classification loss: 0.06180 | Regression loss: 0.19416 | Running loss: 0.54726\n",
            "Epoch: 3 | Iteration: 181 | Classification loss: 0.34619 | Regression loss: 0.75212 | Running loss: 0.54703\n",
            "Epoch: 3 | Iteration: 182 | Classification loss: 0.12648 | Regression loss: 0.40891 | Running loss: 0.54626\n",
            "Epoch: 3 | Iteration: 183 | Classification loss: 0.09065 | Regression loss: 0.39656 | Running loss: 0.54489\n",
            "Epoch: 3 | Iteration: 184 | Classification loss: 0.21179 | Regression loss: 0.51448 | Running loss: 0.54508\n",
            "Epoch: 3 | Iteration: 185 | Classification loss: 0.23028 | Regression loss: 0.37244 | Running loss: 0.54447\n",
            "Epoch: 3 | Iteration: 186 | Classification loss: 0.13253 | Regression loss: 0.31825 | Running loss: 0.54447\n",
            "Epoch: 3 | Iteration: 187 | Classification loss: 0.32097 | Regression loss: 0.02533 | Running loss: 0.54368\n",
            "Epoch: 3 | Iteration: 188 | Classification loss: 0.12689 | Regression loss: 0.42156 | Running loss: 0.54309\n",
            "Epoch: 3 | Iteration: 189 | Classification loss: 0.13634 | Regression loss: 0.47271 | Running loss: 0.54178\n",
            "Epoch: 3 | Iteration: 190 | Classification loss: 0.35804 | Regression loss: 0.31625 | Running loss: 0.54127\n",
            "Epoch: 3 | Iteration: 191 | Classification loss: 0.33693 | Regression loss: 0.42974 | Running loss: 0.54002\n",
            "Epoch: 3 | Iteration: 192 | Classification loss: 0.09242 | Regression loss: 0.38368 | Running loss: 0.53948\n",
            "Epoch: 3 | Iteration: 193 | Classification loss: 0.07053 | Regression loss: 0.23241 | Running loss: 0.53900\n",
            "Epoch: 3 | Iteration: 194 | Classification loss: 0.18034 | Regression loss: 0.47029 | Running loss: 0.53756\n",
            "Epoch: 3 | Iteration: 195 | Classification loss: 0.04653 | Regression loss: 0.19527 | Running loss: 0.53703\n",
            "Epoch: 3 | Iteration: 196 | Classification loss: 0.52186 | Regression loss: 0.71760 | Running loss: 0.53904\n",
            "Epoch: 3 | Iteration: 197 | Classification loss: 0.08385 | Regression loss: 0.20214 | Running loss: 0.53951\n",
            "Epoch: 3 | Iteration: 198 | Classification loss: 0.00479 | Regression loss: 0.01537 | Running loss: 0.53878\n",
            "Epoch: 3 | Iteration: 199 | Classification loss: 0.00094 | Regression loss: 0.01820 | Running loss: 0.53871\n",
            "Epoch: 3 | Iteration: 200 | Classification loss: 0.12482 | Regression loss: 0.39023 | Running loss: 0.53966\n",
            "Epoch: 3 | Iteration: 201 | Classification loss: 0.22403 | Regression loss: 0.47613 | Running loss: 0.53885\n",
            "Epoch: 3 | Iteration: 202 | Classification loss: 0.14636 | Regression loss: 0.40033 | Running loss: 0.53944\n",
            "Epoch: 3 | Iteration: 203 | Classification loss: 0.22945 | Regression loss: 0.40812 | Running loss: 0.53884\n",
            "Epoch: 3 | Iteration: 204 | Classification loss: 0.24201 | Regression loss: 0.47460 | Running loss: 0.53847\n",
            "Epoch: 3 | Iteration: 205 | Classification loss: 0.24107 | Regression loss: 0.55793 | Running loss: 0.53885\n",
            "Epoch: 3 | Iteration: 206 | Classification loss: 0.09141 | Regression loss: 0.23127 | Running loss: 0.53797\n",
            "Epoch: 3 | Iteration: 207 | Classification loss: 0.14099 | Regression loss: 0.42187 | Running loss: 0.53736\n",
            "Epoch: 3 | Iteration: 208 | Classification loss: 0.30724 | Regression loss: 0.62276 | Running loss: 0.53779\n",
            "Epoch: 3 | Iteration: 209 | Classification loss: 0.08121 | Regression loss: 0.24907 | Running loss: 0.53749\n",
            "Epoch: 3 | Iteration: 210 | Classification loss: 0.13469 | Regression loss: 0.39198 | Running loss: 0.53786\n",
            "Epoch: 3 | Iteration: 211 | Classification loss: 0.08592 | Regression loss: 0.35755 | Running loss: 0.53809\n",
            "Epoch: 3 | Iteration: 212 | Classification loss: 0.05184 | Regression loss: 0.29412 | Running loss: 0.53764\n",
            "Epoch: 3 | Iteration: 213 | Classification loss: 0.13467 | Regression loss: 0.33835 | Running loss: 0.53838\n",
            "Epoch: 3 | Iteration: 214 | Classification loss: 0.20402 | Regression loss: 0.51002 | Running loss: 0.53921\n",
            "Epoch: 3 | Iteration: 215 | Classification loss: 0.07597 | Regression loss: 0.28254 | Running loss: 0.53852\n",
            "Epoch: 3 | Iteration: 216 | Classification loss: 0.00036 | Regression loss: 0.06059 | Running loss: 0.53713\n",
            "Epoch: 3 | Iteration: 217 | Classification loss: 0.10174 | Regression loss: 0.21694 | Running loss: 0.53627\n",
            "Epoch: 3 | Iteration: 218 | Classification loss: 0.07905 | Regression loss: 0.29664 | Running loss: 0.53567\n",
            "Epoch: 3 | Iteration: 219 | Classification loss: 0.30484 | Regression loss: 0.58106 | Running loss: 0.53697\n",
            "Epoch: 3 | Iteration: 220 | Classification loss: 0.32993 | Regression loss: 0.61820 | Running loss: 0.53704\n",
            "Epoch: 3 | Iteration: 221 | Classification loss: 0.39712 | Regression loss: 0.44096 | Running loss: 0.53758\n",
            "Epoch: 3 | Iteration: 222 | Classification loss: 0.08737 | Regression loss: 0.33653 | Running loss: 0.53826\n",
            "Epoch: 3 | Iteration: 223 | Classification loss: 0.00008 | Regression loss: 0.00000 | Running loss: 0.53719\n",
            "Epoch: 3 | Iteration: 224 | Classification loss: 0.32017 | Regression loss: 0.64980 | Running loss: 0.53828\n",
            "Epoch: 3 | Iteration: 225 | Classification loss: 0.30550 | Regression loss: 0.45796 | Running loss: 0.53845\n",
            "Epoch: 3 | Iteration: 226 | Classification loss: 0.10634 | Regression loss: 0.35007 | Running loss: 0.53850\n",
            "Epoch: 3 | Iteration: 227 | Classification loss: 0.17909 | Regression loss: 0.46954 | Running loss: 0.53751\n",
            "Epoch: 3 | Iteration: 228 | Classification loss: 0.01639 | Regression loss: 0.02400 | Running loss: 0.53724\n",
            "Epoch: 3 | Iteration: 229 | Classification loss: 0.05894 | Regression loss: 0.26844 | Running loss: 0.53708\n",
            "Epoch: 3 | Iteration: 230 | Classification loss: 0.19556 | Regression loss: 0.34766 | Running loss: 0.53790\n",
            "Epoch: 3 | Iteration: 231 | Classification loss: 0.31090 | Regression loss: 0.71263 | Running loss: 0.53923\n",
            "Epoch: 3 | Iteration: 232 | Classification loss: 0.04661 | Regression loss: 0.01953 | Running loss: 0.53785\n",
            "Epoch: 3 | Iteration: 233 | Classification loss: 0.22765 | Regression loss: 0.51436 | Running loss: 0.53763\n",
            "Epoch: 3 | Iteration: 234 | Classification loss: 0.00055 | Regression loss: 0.01319 | Running loss: 0.53712\n",
            "Epoch: 3 | Iteration: 235 | Classification loss: 0.23346 | Regression loss: 0.51295 | Running loss: 0.53631\n",
            "Epoch: 3 | Iteration: 236 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.53599\n",
            "Epoch: 3 | Iteration: 237 | Classification loss: 0.00032 | Regression loss: 0.00559 | Running loss: 0.53439\n",
            "Epoch: 3 | Iteration: 238 | Classification loss: 0.32530 | Regression loss: 0.49674 | Running loss: 0.53509\n",
            "Epoch: 3 | Iteration: 239 | Classification loss: 0.00019 | Regression loss: 0.01198 | Running loss: 0.53397\n",
            "Epoch: 3 | Iteration: 240 | Classification loss: 0.06717 | Regression loss: 0.19097 | Running loss: 0.53372\n",
            "Epoch: 3 | Iteration: 241 | Classification loss: 0.10740 | Regression loss: 0.26113 | Running loss: 0.53304\n",
            "Epoch: 3 | Iteration: 242 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.53298\n",
            "Epoch: 3 | Iteration: 243 | Classification loss: 0.25835 | Regression loss: 0.64250 | Running loss: 0.53473\n",
            "Epoch: 3 | Iteration: 244 | Classification loss: 0.22535 | Regression loss: 0.58771 | Running loss: 0.53559\n",
            "Epoch: 3 | Iteration: 245 | Classification loss: 0.20577 | Regression loss: 0.34117 | Running loss: 0.53611\n",
            "Epoch: 3 | Iteration: 246 | Classification loss: 0.41789 | Regression loss: 0.66553 | Running loss: 0.53771\n",
            "Epoch: 3 | Iteration: 247 | Classification loss: 0.05293 | Regression loss: 0.25223 | Running loss: 0.53724\n",
            "Epoch: 3 | Iteration: 248 | Classification loss: 0.20485 | Regression loss: 0.56770 | Running loss: 0.53879\n",
            "Epoch: 3 | Iteration: 249 | Classification loss: 0.24817 | Regression loss: 0.48648 | Running loss: 0.53947\n",
            "Epoch: 3 | Iteration: 250 | Classification loss: 0.00064 | Regression loss: 0.04836 | Running loss: 0.53929\n",
            "Epoch: 3 | Iteration: 251 | Classification loss: 0.00036 | Regression loss: 0.03310 | Running loss: 0.53794\n",
            "Epoch: 3 | Iteration: 252 | Classification loss: 0.11833 | Regression loss: 0.26420 | Running loss: 0.53867\n",
            "Epoch: 3 | Iteration: 253 | Classification loss: 0.32094 | Regression loss: 0.54473 | Running loss: 0.53790\n",
            "Epoch: 3 | Iteration: 254 | Classification loss: 0.00038 | Regression loss: 0.02952 | Running loss: 0.53688\n",
            "Epoch: 3 | Iteration: 255 | Classification loss: 0.05178 | Regression loss: 0.17685 | Running loss: 0.53666\n",
            "Epoch: 3 | Iteration: 256 | Classification loss: 0.00066 | Regression loss: 0.04636 | Running loss: 0.53522\n",
            "Epoch: 3 | Iteration: 257 | Classification loss: 0.00032 | Regression loss: 0.02038 | Running loss: 0.53462\n",
            "Epoch: 3 | Iteration: 258 | Classification loss: 0.26798 | Regression loss: 0.48501 | Running loss: 0.53534\n",
            "Epoch: 3 | Iteration: 259 | Classification loss: 0.13083 | Regression loss: 0.46827 | Running loss: 0.53654\n",
            "Epoch: 3 | Iteration: 260 | Classification loss: 0.00045 | Regression loss: 0.03632 | Running loss: 0.53467\n",
            "Epoch: 3 | Iteration: 261 | Classification loss: 0.28443 | Regression loss: 0.60818 | Running loss: 0.53635\n",
            "Epoch: 3 | Iteration: 262 | Classification loss: 0.00058 | Regression loss: 0.06531 | Running loss: 0.53438\n",
            "Epoch: 3 | Iteration: 263 | Classification loss: 0.13068 | Regression loss: 0.39759 | Running loss: 0.53441\n",
            "Epoch: 3 | Iteration: 264 | Classification loss: 0.11376 | Regression loss: 0.37752 | Running loss: 0.53297\n",
            "Epoch: 3 | Iteration: 265 | Classification loss: 0.13735 | Regression loss: 0.36629 | Running loss: 0.53175\n",
            "Epoch: 3 | Iteration: 266 | Classification loss: 0.29282 | Regression loss: 0.53873 | Running loss: 0.53183\n",
            "Epoch: 3 | Iteration: 267 | Classification loss: 0.70033 | Regression loss: 0.78770 | Running loss: 0.53376\n",
            "Epoch: 3 | Iteration: 268 | Classification loss: 0.00330 | Regression loss: 0.03807 | Running loss: 0.53148\n",
            "Epoch: 3 | Iteration: 269 | Classification loss: 0.06540 | Regression loss: 0.19863 | Running loss: 0.53064\n",
            "Epoch: 3 | Iteration: 270 | Classification loss: 0.00115 | Regression loss: 0.01387 | Running loss: 0.52985\n",
            "Epoch: 3 | Iteration: 271 | Classification loss: 0.20236 | Regression loss: 0.37177 | Running loss: 0.52885\n",
            "Epoch: 3 | Iteration: 272 | Classification loss: 0.24189 | Regression loss: 0.54572 | Running loss: 0.53018\n",
            "Epoch: 3 | Iteration: 273 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.52820\n",
            "Epoch: 3 | Iteration: 274 | Classification loss: 0.12020 | Regression loss: 0.26619 | Running loss: 0.52728\n",
            "Epoch: 3 | Iteration: 275 | Classification loss: 0.29713 | Regression loss: 0.58659 | Running loss: 0.52756\n",
            "Epoch: 3 | Iteration: 276 | Classification loss: 0.00018 | Regression loss: 0.00000 | Running loss: 0.52660\n",
            "Epoch: 3 | Iteration: 277 | Classification loss: 0.06909 | Regression loss: 0.23137 | Running loss: 0.52627\n",
            "Epoch: 3 | Iteration: 278 | Classification loss: 0.00447 | Regression loss: 0.05099 | Running loss: 0.52457\n",
            "Epoch: 3 | Iteration: 279 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.52371\n",
            "Epoch: 3 | Iteration: 280 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.52276\n",
            "Epoch: 3 | Iteration: 281 | Classification loss: 0.07018 | Regression loss: 0.29976 | Running loss: 0.52243\n",
            "Epoch: 3 | Iteration: 282 | Classification loss: 0.35002 | Regression loss: 0.51237 | Running loss: 0.52302\n",
            "Epoch: 3 | Iteration: 283 | Classification loss: 0.08015 | Regression loss: 0.19253 | Running loss: 0.52190\n",
            "Epoch: 3 | Iteration: 284 | Classification loss: 0.14834 | Regression loss: 0.38750 | Running loss: 0.52185\n",
            "Epoch: 3 | Iteration: 285 | Classification loss: 0.07403 | Regression loss: 0.17415 | Running loss: 0.52111\n",
            "Epoch: 3 | Iteration: 286 | Classification loss: 0.00044 | Regression loss: 0.13745 | Running loss: 0.51969\n",
            "Epoch: 3 | Iteration: 287 | Classification loss: 0.03549 | Regression loss: 0.24176 | Running loss: 0.52018\n",
            "Epoch: 3 | Iteration: 288 | Classification loss: 0.21102 | Regression loss: 0.50933 | Running loss: 0.52091\n",
            "Epoch: 3 | Iteration: 289 | Classification loss: 0.06129 | Regression loss: 0.15396 | Running loss: 0.52061\n",
            "Epoch: 3 | Iteration: 290 | Classification loss: 0.21427 | Regression loss: 0.44127 | Running loss: 0.52142\n",
            "Epoch: 3 | Iteration: 291 | Classification loss: 0.51466 | Regression loss: 0.66696 | Running loss: 0.52379\n",
            "Epoch: 3 | Iteration: 292 | Classification loss: 0.14274 | Regression loss: 0.42854 | Running loss: 0.52488\n",
            "Epoch: 3 | Iteration: 293 | Classification loss: 0.17282 | Regression loss: 0.45871 | Running loss: 0.52381\n",
            "Epoch: 3 | Iteration: 294 | Classification loss: 0.42157 | Regression loss: 0.67245 | Running loss: 0.52442\n",
            "Epoch: 3 | Iteration: 295 | Classification loss: 0.25483 | Regression loss: 0.37158 | Running loss: 0.52358\n",
            "Epoch: 3 | Iteration: 296 | Classification loss: 0.13375 | Regression loss: 0.39219 | Running loss: 0.52312\n",
            "Epoch: 3 | Iteration: 297 | Classification loss: 0.23086 | Regression loss: 0.55358 | Running loss: 0.52229\n",
            "Epoch: 3 | Iteration: 298 | Classification loss: 0.14044 | Regression loss: 0.46185 | Running loss: 0.52144\n",
            "Epoch: 3 | Iteration: 299 | Classification loss: 0.18822 | Regression loss: 0.54983 | Running loss: 0.52109\n",
            "Epoch: 3 | Iteration: 300 | Classification loss: 0.69562 | Regression loss: 0.47163 | Running loss: 0.52224\n",
            "Epoch: 3 | Iteration: 301 | Classification loss: 0.09711 | Regression loss: 0.19508 | Running loss: 0.52094\n",
            "Epoch: 3 | Iteration: 302 | Classification loss: 0.72879 | Regression loss: 0.70313 | Running loss: 0.52319\n",
            "Epoch: 3 | Iteration: 303 | Classification loss: 0.07841 | Regression loss: 0.21222 | Running loss: 0.52320\n",
            "Epoch: 3 | Iteration: 304 | Classification loss: 0.22495 | Regression loss: 0.31108 | Running loss: 0.52428\n",
            "Epoch: 3 | Iteration: 305 | Classification loss: 0.07135 | Regression loss: 0.19417 | Running loss: 0.52340\n",
            "Epoch: 3 | Iteration: 306 | Classification loss: 0.19223 | Regression loss: 0.23290 | Running loss: 0.52337\n",
            "Epoch: 3 | Iteration: 307 | Classification loss: 0.05751 | Regression loss: 0.14960 | Running loss: 0.52231\n",
            "Epoch: 3 | Iteration: 308 | Classification loss: 0.08725 | Regression loss: 0.19746 | Running loss: 0.52196\n",
            "Epoch: 3 | Iteration: 309 | Classification loss: 0.39210 | Regression loss: 0.69925 | Running loss: 0.52320\n",
            "Epoch: 3 | Iteration: 310 | Classification loss: 0.05565 | Regression loss: 0.12122 | Running loss: 0.52273\n",
            "Epoch: 3 | Iteration: 311 | Classification loss: 0.19617 | Regression loss: 0.52988 | Running loss: 0.52288\n",
            "Epoch: 3 | Iteration: 312 | Classification loss: 0.12803 | Regression loss: 0.37567 | Running loss: 0.52215\n",
            "Epoch: 3 | Iteration: 313 | Classification loss: 0.00266 | Regression loss: 0.15655 | Running loss: 0.51999\n",
            "Epoch: 3 | Iteration: 314 | Classification loss: 0.14162 | Regression loss: 0.35169 | Running loss: 0.51928\n",
            "Epoch: 3 | Iteration: 315 | Classification loss: 0.13066 | Regression loss: 0.34653 | Running loss: 0.51887\n",
            "Epoch: 3 | Iteration: 316 | Classification loss: 0.17316 | Regression loss: 0.47771 | Running loss: 0.51764\n",
            "Epoch: 3 | Iteration: 317 | Classification loss: 0.12957 | Regression loss: 0.37178 | Running loss: 0.51790\n",
            "Epoch: 3 | Iteration: 318 | Classification loss: 0.18807 | Regression loss: 0.53698 | Running loss: 0.51798\n",
            "Epoch: 3 | Iteration: 319 | Classification loss: 0.16010 | Regression loss: 0.46728 | Running loss: 0.51884\n",
            "Epoch: 3 | Iteration: 320 | Classification loss: 0.58826 | Regression loss: 0.24137 | Running loss: 0.52037\n",
            "Epoch: 3 | Iteration: 321 | Classification loss: 0.11867 | Regression loss: 0.34062 | Running loss: 0.52049\n",
            "Epoch: 3 | Iteration: 322 | Classification loss: 0.00103 | Regression loss: 0.06521 | Running loss: 0.51971\n",
            "Epoch: 3 | Iteration: 323 | Classification loss: 0.39356 | Regression loss: 0.03180 | Running loss: 0.51904\n",
            "Epoch: 3 | Iteration: 324 | Classification loss: 0.06776 | Regression loss: 0.27256 | Running loss: 0.51873\n",
            "Epoch: 3 | Iteration: 325 | Classification loss: 0.17730 | Regression loss: 0.35522 | Running loss: 0.51860\n",
            "Epoch: 3 | Iteration: 326 | Classification loss: 0.16693 | Regression loss: 0.38135 | Running loss: 0.51920\n",
            "Epoch: 3 | Iteration: 327 | Classification loss: 0.13024 | Regression loss: 0.39072 | Running loss: 0.51832\n",
            "Epoch: 3 | Iteration: 328 | Classification loss: 0.17131 | Regression loss: 0.43129 | Running loss: 0.51831\n",
            "Epoch: 3 | Iteration: 329 | Classification loss: 0.33532 | Regression loss: 0.23426 | Running loss: 0.51869\n",
            "Epoch: 3 | Iteration: 330 | Classification loss: 0.15655 | Regression loss: 0.28190 | Running loss: 0.51728\n",
            "Epoch: 3 | Iteration: 331 | Classification loss: 0.07280 | Regression loss: 0.21146 | Running loss: 0.51631\n",
            "Epoch: 3 | Iteration: 332 | Classification loss: 0.17140 | Regression loss: 0.47675 | Running loss: 0.51554\n",
            "Epoch: 3 | Iteration: 333 | Classification loss: 0.00018 | Regression loss: 0.00000 | Running loss: 0.51389\n",
            "Epoch: 3 | Iteration: 334 | Classification loss: 0.23017 | Regression loss: 0.46295 | Running loss: 0.51375\n",
            "Epoch: 3 | Iteration: 335 | Classification loss: 0.16507 | Regression loss: 0.40698 | Running loss: 0.51326\n",
            "Epoch: 3 | Iteration: 336 | Classification loss: 0.13971 | Regression loss: 0.37092 | Running loss: 0.51353\n",
            "Epoch: 3 | Iteration: 337 | Classification loss: 0.12654 | Regression loss: 0.32625 | Running loss: 0.51298\n",
            "Epoch: 3 | Iteration: 338 | Classification loss: 0.08289 | Regression loss: 0.34744 | Running loss: 0.51286\n",
            "Epoch: 3 | Iteration: 339 | Classification loss: 0.00743 | Regression loss: 0.04139 | Running loss: 0.51126\n",
            "Epoch: 3 | Iteration: 340 | Classification loss: 0.00084 | Regression loss: 0.05286 | Running loss: 0.51041\n",
            "Epoch: 3 | Iteration: 341 | Classification loss: 0.10783 | Regression loss: 0.33851 | Running loss: 0.51011\n",
            "Epoch: 3 | Iteration: 342 | Classification loss: 0.00077 | Regression loss: 0.03051 | Running loss: 0.50855\n",
            "Epoch: 3 | Iteration: 343 | Classification loss: 0.33522 | Regression loss: 0.37928 | Running loss: 0.50897\n",
            "Epoch: 3 | Iteration: 344 | Classification loss: 0.13771 | Regression loss: 0.30441 | Running loss: 0.50889\n",
            "Epoch: 3 | Iteration: 345 | Classification loss: 0.23920 | Regression loss: 0.62409 | Running loss: 0.50923\n",
            "Epoch: 3 | Iteration: 346 | Classification loss: 0.21960 | Regression loss: 0.39749 | Running loss: 0.51046\n",
            "Epoch: 3 | Iteration: 347 | Classification loss: 0.16724 | Regression loss: 0.40845 | Running loss: 0.50975\n",
            "Epoch: 3 | Iteration: 348 | Classification loss: 0.22546 | Regression loss: 0.51710 | Running loss: 0.50908\n",
            "Epoch: 3 | Iteration: 349 | Classification loss: 0.00034 | Regression loss: 0.10823 | Running loss: 0.50902\n",
            "Epoch: 3 | Iteration: 350 | Classification loss: 0.22936 | Regression loss: 0.49340 | Running loss: 0.50921\n",
            "Epoch: 3 | Iteration: 351 | Classification loss: 0.19153 | Regression loss: 0.48034 | Running loss: 0.51026\n",
            "Epoch: 3 | Iteration: 352 | Classification loss: 0.20141 | Regression loss: 0.48778 | Running loss: 0.51080\n",
            "Epoch: 3 | Iteration: 353 | Classification loss: 1.53888 | Regression loss: 0.20279 | Running loss: 0.51129\n",
            "Epoch: 3 | Iteration: 354 | Classification loss: 0.03424 | Regression loss: 0.12920 | Running loss: 0.51143\n",
            "Epoch: 3 | Iteration: 355 | Classification loss: 0.39756 | Regression loss: 0.52615 | Running loss: 0.51102\n",
            "Epoch: 3 | Iteration: 356 | Classification loss: 0.08919 | Regression loss: 0.20110 | Running loss: 0.51002\n",
            "Epoch: 3 | Iteration: 357 | Classification loss: 0.15691 | Regression loss: 0.39499 | Running loss: 0.51056\n",
            "Epoch: 3 | Iteration: 358 | Classification loss: 0.06010 | Regression loss: 0.20428 | Running loss: 0.50894\n",
            "Epoch: 3 | Iteration: 359 | Classification loss: 0.21178 | Regression loss: 0.46285 | Running loss: 0.50966\n",
            "Epoch: 3 | Iteration: 360 | Classification loss: 0.13828 | Regression loss: 0.34893 | Running loss: 0.51063\n",
            "Epoch: 3 | Iteration: 361 | Classification loss: 0.00054 | Regression loss: 0.05068 | Running loss: 0.50962\n",
            "Epoch: 3 | Iteration: 362 | Classification loss: 0.10906 | Regression loss: 0.39728 | Running loss: 0.50937\n",
            "Epoch: 3 | Iteration: 363 | Classification loss: 0.06746 | Regression loss: 0.23958 | Running loss: 0.50945\n",
            "Epoch: 3 | Iteration: 364 | Classification loss: 0.04646 | Regression loss: 0.17725 | Running loss: 0.50842\n",
            "Epoch: 3 | Iteration: 365 | Classification loss: 0.17159 | Regression loss: 0.35759 | Running loss: 0.50708\n",
            "Epoch: 3 | Iteration: 366 | Classification loss: 0.12611 | Regression loss: 0.23088 | Running loss: 0.50762\n",
            "Epoch: 3 | Iteration: 367 | Classification loss: 0.05828 | Regression loss: 0.21701 | Running loss: 0.50670\n",
            "Epoch: 3 | Iteration: 368 | Classification loss: 0.00057 | Regression loss: 0.01418 | Running loss: 0.50558\n",
            "Epoch: 3 | Iteration: 369 | Classification loss: 0.00039 | Regression loss: 0.00924 | Running loss: 0.50400\n",
            "Epoch: 3 | Iteration: 370 | Classification loss: 0.18890 | Regression loss: 0.45320 | Running loss: 0.50425\n",
            "Epoch: 3 | Iteration: 371 | Classification loss: 0.05249 | Regression loss: 0.17354 | Running loss: 0.50374\n",
            "Epoch: 3 | Iteration: 372 | Classification loss: 0.13632 | Regression loss: 0.28829 | Running loss: 0.50427\n",
            "Epoch: 3 | Iteration: 373 | Classification loss: 0.31587 | Regression loss: 0.77450 | Running loss: 0.50477\n",
            "Epoch: 3 | Iteration: 374 | Classification loss: 0.05500 | Regression loss: 0.23137 | Running loss: 0.50441\n",
            "Epoch: 3 | Iteration: 375 | Classification loss: 0.05548 | Regression loss: 0.23931 | Running loss: 0.50445\n",
            "Epoch: 3 | Iteration: 376 | Classification loss: 0.15623 | Regression loss: 0.45635 | Running loss: 0.50393\n",
            "Epoch: 3 | Iteration: 377 | Classification loss: 0.07090 | Regression loss: 0.13444 | Running loss: 0.50385\n",
            "Epoch: 3 | Iteration: 378 | Classification loss: 0.31071 | Regression loss: 0.58755 | Running loss: 0.50470\n",
            "Epoch: 3 | Iteration: 379 | Classification loss: 0.08438 | Regression loss: 0.30506 | Running loss: 0.50491\n",
            "Epoch: 3 | Iteration: 380 | Classification loss: 0.27985 | Regression loss: 0.53386 | Running loss: 0.50601\n",
            "Epoch: 3 | Iteration: 381 | Classification loss: 0.12655 | Regression loss: 0.40331 | Running loss: 0.50557\n",
            "Epoch: 3 | Iteration: 382 | Classification loss: 0.37551 | Regression loss: 0.58420 | Running loss: 0.50661\n",
            "Epoch: 3 | Iteration: 383 | Classification loss: 0.00028 | Regression loss: 0.07872 | Running loss: 0.50537\n",
            "Epoch: 3 | Iteration: 384 | Classification loss: 0.43634 | Regression loss: 0.38663 | Running loss: 0.50637\n",
            "Epoch: 3 | Iteration: 385 | Classification loss: 0.25694 | Regression loss: 0.63677 | Running loss: 0.50717\n",
            "Epoch: 3 | Iteration: 386 | Classification loss: 0.30548 | Regression loss: 0.59710 | Running loss: 0.50756\n",
            "Epoch: 3 | Iteration: 387 | Classification loss: 0.11259 | Regression loss: 0.30066 | Running loss: 0.50606\n",
            "Epoch: 3 | Iteration: 388 | Classification loss: 0.14655 | Regression loss: 0.41299 | Running loss: 0.50612\n",
            "Epoch: 3 | Iteration: 389 | Classification loss: 0.04907 | Regression loss: 0.16474 | Running loss: 0.50564\n",
            "Epoch: 3 | Iteration: 390 | Classification loss: 0.00034 | Regression loss: 0.05431 | Running loss: 0.50517\n",
            "Epoch: 3 | Iteration: 391 | Classification loss: 0.10095 | Regression loss: 0.30987 | Running loss: 0.50589\n",
            "Epoch: 3 | Iteration: 392 | Classification loss: 0.03199 | Regression loss: 0.01864 | Running loss: 0.50486\n",
            "Epoch: 3 | Iteration: 393 | Classification loss: 0.08906 | Regression loss: 0.20671 | Running loss: 0.50340\n",
            "Epoch: 3 | Iteration: 394 | Classification loss: 0.11519 | Regression loss: 0.28928 | Running loss: 0.50344\n",
            "Epoch: 3 | Iteration: 395 | Classification loss: 0.16755 | Regression loss: 0.38962 | Running loss: 0.50264\n",
            "Epoch: 3 | Iteration: 396 | Classification loss: 0.19214 | Regression loss: 0.57589 | Running loss: 0.50395\n",
            "Epoch: 3 | Iteration: 397 | Classification loss: 0.04843 | Regression loss: 0.21056 | Running loss: 0.50375\n",
            "Epoch: 3 | Iteration: 398 | Classification loss: 0.00020 | Regression loss: 0.04631 | Running loss: 0.50310\n",
            "Epoch: 3 | Iteration: 399 | Classification loss: 0.15644 | Regression loss: 0.39487 | Running loss: 0.50240\n",
            "Epoch: 3 | Iteration: 400 | Classification loss: 0.25201 | Regression loss: 0.44887 | Running loss: 0.50217\n",
            "Epoch: 3 | Iteration: 401 | Classification loss: 0.05163 | Regression loss: 0.19188 | Running loss: 0.50132\n",
            "Epoch: 3 | Iteration: 402 | Classification loss: 0.00359 | Regression loss: 0.02970 | Running loss: 0.49979\n",
            "Epoch: 3 | Iteration: 403 | Classification loss: 0.16603 | Regression loss: 0.24040 | Running loss: 0.49973\n",
            "Epoch: 3 | Iteration: 404 | Classification loss: 0.13307 | Regression loss: 0.34271 | Running loss: 0.50059\n",
            "Epoch: 3 | Iteration: 405 | Classification loss: 0.23374 | Regression loss: 0.66256 | Running loss: 0.50142\n",
            "Epoch: 3 | Iteration: 406 | Classification loss: 0.10637 | Regression loss: 0.25409 | Running loss: 0.50125\n",
            "Epoch: 3 | Iteration: 407 | Classification loss: 0.04975 | Regression loss: 0.20080 | Running loss: 0.50007\n",
            "Epoch: 3 | Iteration: 408 | Classification loss: 0.10271 | Regression loss: 0.25813 | Running loss: 0.50000\n",
            "Epoch: 3 | Iteration: 409 | Classification loss: 0.00032 | Regression loss: 0.02442 | Running loss: 0.49889\n",
            "Epoch: 3 | Iteration: 410 | Classification loss: 0.43066 | Regression loss: 0.66060 | Running loss: 0.50100\n",
            "Epoch: 3 | Iteration: 411 | Classification loss: 0.01108 | Regression loss: 0.02412 | Running loss: 0.49925\n",
            "Epoch: 3 | Iteration: 412 | Classification loss: 0.00031 | Regression loss: 0.01124 | Running loss: 0.49733\n",
            "Epoch: 3 | Iteration: 413 | Classification loss: 0.36290 | Regression loss: 0.74002 | Running loss: 0.49942\n",
            "Epoch: 3 | Iteration: 414 | Classification loss: 0.06675 | Regression loss: 0.17583 | Running loss: 0.49787\n",
            "Epoch: 3 | Iteration: 415 | Classification loss: 0.16041 | Regression loss: 0.40666 | Running loss: 0.49740\n",
            "Epoch: 3 | Iteration: 416 | Classification loss: 0.06203 | Regression loss: 0.23959 | Running loss: 0.49689\n",
            "Epoch: 3 | Iteration: 417 | Classification loss: 0.19685 | Regression loss: 0.34972 | Running loss: 0.49640\n",
            "Epoch: 3 | Iteration: 418 | Classification loss: 0.00157 | Regression loss: 0.06944 | Running loss: 0.49455\n",
            "Epoch: 3 | Iteration: 419 | Classification loss: 0.17271 | Regression loss: 0.30489 | Running loss: 0.49327\n",
            "Epoch: 3 | Iteration: 420 | Classification loss: 0.09642 | Regression loss: 0.16528 | Running loss: 0.49370\n",
            "Epoch: 3 | Iteration: 421 | Classification loss: 0.12953 | Regression loss: 0.24450 | Running loss: 0.49437\n",
            "Epoch: 3 | Iteration: 422 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.49393\n",
            "Epoch: 3 | Iteration: 423 | Classification loss: 0.39670 | Regression loss: 0.55931 | Running loss: 0.49268\n",
            "Epoch: 3 | Iteration: 424 | Classification loss: 0.14527 | Regression loss: 0.45698 | Running loss: 0.49385\n",
            "Epoch: 3 | Iteration: 425 | Classification loss: 0.00024 | Regression loss: 0.06820 | Running loss: 0.49276\n",
            "Epoch: 3 | Iteration: 426 | Classification loss: 0.15890 | Regression loss: 0.00000 | Running loss: 0.49078\n",
            "Epoch: 3 | Iteration: 427 | Classification loss: 0.00056 | Regression loss: 0.19223 | Running loss: 0.49117\n",
            "Epoch: 3 | Iteration: 428 | Classification loss: 0.05764 | Regression loss: 0.24764 | Running loss: 0.49162\n",
            "Epoch: 3 | Iteration: 429 | Classification loss: 0.10399 | Regression loss: 0.34830 | Running loss: 0.49132\n",
            "Epoch: 3 | Iteration: 430 | Classification loss: 0.00028 | Regression loss: 0.03461 | Running loss: 0.49134\n",
            "Epoch: 3 | Iteration: 431 | Classification loss: 0.00839 | Regression loss: 0.02113 | Running loss: 0.49108\n",
            "Epoch: 3 | Iteration: 432 | Classification loss: 0.10592 | Regression loss: 0.34561 | Running loss: 0.49026\n",
            "Epoch: 3 | Iteration: 433 | Classification loss: 0.18675 | Regression loss: 0.42676 | Running loss: 0.48940\n",
            "Epoch: 3 | Iteration: 434 | Classification loss: 0.00022 | Regression loss: 0.01269 | Running loss: 0.48889\n",
            "Epoch: 3 | Iteration: 435 | Classification loss: 0.15862 | Regression loss: 0.42624 | Running loss: 0.48806\n",
            "Epoch: 3 | Iteration: 436 | Classification loss: 0.00034 | Regression loss: 0.02425 | Running loss: 0.48703\n",
            "Epoch: 3 | Iteration: 437 | Classification loss: 0.28711 | Regression loss: 0.55998 | Running loss: 0.48737\n",
            "Epoch: 3 | Iteration: 438 | Classification loss: 0.22270 | Regression loss: 0.42746 | Running loss: 0.48763\n",
            "Epoch: 3 | Iteration: 439 | Classification loss: 0.31446 | Regression loss: 0.61978 | Running loss: 0.48949\n",
            "Epoch: 3 | Iteration: 440 | Classification loss: 0.18621 | Regression loss: 0.48017 | Running loss: 0.48904\n",
            "Epoch: 3 | Iteration: 441 | Classification loss: 0.09244 | Regression loss: 0.23146 | Running loss: 0.48789\n",
            "Epoch: 3 | Iteration: 442 | Classification loss: 0.13759 | Regression loss: 0.42475 | Running loss: 0.48780\n",
            "Epoch: 3 | Iteration: 443 | Classification loss: 0.07923 | Regression loss: 0.41426 | Running loss: 0.48723\n",
            "Epoch: 3 | Iteration: 444 | Classification loss: 0.11654 | Regression loss: 0.33264 | Running loss: 0.48696\n",
            "Epoch: 3 | Iteration: 445 | Classification loss: 0.09193 | Regression loss: 0.29978 | Running loss: 0.48705\n",
            "Epoch: 3 | Iteration: 446 | Classification loss: 0.19093 | Regression loss: 0.46473 | Running loss: 0.48706\n",
            "Epoch: 3 | Iteration: 447 | Classification loss: 0.04850 | Regression loss: 0.12345 | Running loss: 0.48627\n",
            "Epoch: 3 | Iteration: 448 | Classification loss: 0.20866 | Regression loss: 0.47456 | Running loss: 0.48613\n",
            "Epoch: 3 | Iteration: 449 | Classification loss: 0.00019 | Regression loss: 0.00955 | Running loss: 0.48512\n",
            "Epoch: 3 | Iteration: 450 | Classification loss: 0.23041 | Regression loss: 0.34795 | Running loss: 0.48619\n",
            "Epoch: 3 | Iteration: 451 | Classification loss: 0.00112 | Regression loss: 0.03830 | Running loss: 0.48526\n",
            "Epoch: 3 | Iteration: 452 | Classification loss: 0.23394 | Regression loss: 0.45366 | Running loss: 0.48585\n",
            "Epoch: 3 | Iteration: 453 | Classification loss: 0.11003 | Regression loss: 0.21995 | Running loss: 0.48543\n",
            "Epoch: 3 | Iteration: 454 | Classification loss: 0.19248 | Regression loss: 0.39942 | Running loss: 0.48654\n",
            "Epoch: 3 | Iteration: 455 | Classification loss: 0.06652 | Regression loss: 0.14961 | Running loss: 0.48552\n",
            "Epoch: 3 | Iteration: 456 | Classification loss: 0.40902 | Regression loss: 0.59861 | Running loss: 0.48596\n",
            "Epoch: 3 | Iteration: 457 | Classification loss: 0.38085 | Regression loss: 0.54371 | Running loss: 0.48721\n",
            "Epoch: 3 | Iteration: 458 | Classification loss: 0.15392 | Regression loss: 0.57742 | Running loss: 0.48808\n",
            "Epoch: 3 | Iteration: 459 | Classification loss: 0.12668 | Regression loss: 0.43029 | Running loss: 0.48919\n",
            "Epoch: 3 | Iteration: 460 | Classification loss: 0.09371 | Regression loss: 0.33392 | Running loss: 0.49005\n",
            "Epoch: 3 | Iteration: 461 | Classification loss: 0.14967 | Regression loss: 0.54638 | Running loss: 0.49139\n",
            "Epoch: 3 | Iteration: 462 | Classification loss: 0.30831 | Regression loss: 0.67153 | Running loss: 0.49332\n",
            "Epoch: 3 | Iteration: 463 | Classification loss: 0.15630 | Regression loss: 0.36720 | Running loss: 0.49270\n",
            "Epoch: 3 | Iteration: 464 | Classification loss: 0.11704 | Regression loss: 0.32434 | Running loss: 0.49162\n",
            "Epoch: 3 | Iteration: 465 | Classification loss: 0.12391 | Regression loss: 0.20051 | Running loss: 0.49067\n",
            "Epoch: 3 | Iteration: 466 | Classification loss: 0.22868 | Regression loss: 0.49211 | Running loss: 0.49022\n",
            "Epoch: 3 | Iteration: 467 | Classification loss: 0.17156 | Regression loss: 0.46431 | Running loss: 0.49030\n",
            "Epoch: 3 | Iteration: 468 | Classification loss: 0.00054 | Regression loss: 0.00000 | Running loss: 0.49014\n",
            "Epoch: 3 | Iteration: 469 | Classification loss: 0.21683 | Regression loss: 0.52931 | Running loss: 0.49147\n",
            "Epoch: 3 | Iteration: 470 | Classification loss: 0.11538 | Regression loss: 0.42998 | Running loss: 0.49163\n",
            "Epoch: 3 | Iteration: 471 | Classification loss: 0.07842 | Regression loss: 0.27993 | Running loss: 0.48939\n",
            "Epoch: 3 | Iteration: 472 | Classification loss: 0.23790 | Regression loss: 0.46635 | Running loss: 0.48912\n",
            "Epoch: 3 | Iteration: 473 | Classification loss: 0.12117 | Regression loss: 0.26361 | Running loss: 0.48920\n",
            "Epoch: 3 | Iteration: 474 | Classification loss: 0.00555 | Regression loss: 0.00000 | Running loss: 0.48892\n",
            "Epoch: 3 | Iteration: 475 | Classification loss: 0.22245 | Regression loss: 0.30863 | Running loss: 0.48930\n",
            "Epoch: 3 | Iteration: 476 | Classification loss: 0.06916 | Regression loss: 0.20266 | Running loss: 0.48882\n",
            "Epoch: 3 | Iteration: 477 | Classification loss: 0.07001 | Regression loss: 0.15123 | Running loss: 0.48868\n",
            "Epoch: 3 | Iteration: 478 | Classification loss: 0.14395 | Regression loss: 0.36864 | Running loss: 0.48965\n",
            "Epoch: 3 | Iteration: 479 | Classification loss: 0.00198 | Regression loss: 0.03768 | Running loss: 0.48870\n",
            "Epoch: 3 | Iteration: 480 | Classification loss: 0.00136 | Regression loss: 0.03315 | Running loss: 0.48877\n",
            "Epoch: 3 | Iteration: 481 | Classification loss: 0.24672 | Regression loss: 0.42311 | Running loss: 0.48942\n",
            "Epoch: 3 | Iteration: 482 | Classification loss: 0.38128 | Regression loss: 0.64250 | Running loss: 0.48889\n",
            "Epoch: 3 | Iteration: 483 | Classification loss: 0.14143 | Regression loss: 0.45202 | Running loss: 0.48753\n",
            "Epoch: 3 | Iteration: 484 | Classification loss: 0.27678 | Regression loss: 0.57390 | Running loss: 0.48742\n",
            "Epoch: 3 | Iteration: 485 | Classification loss: 0.00029 | Regression loss: 0.06668 | Running loss: 0.48524\n",
            "Epoch: 3 | Iteration: 486 | Classification loss: 0.00023 | Regression loss: 0.10793 | Running loss: 0.48372\n",
            "Epoch: 3 | Iteration: 487 | Classification loss: 0.09797 | Regression loss: 0.23575 | Running loss: 0.48233\n",
            "Epoch: 3 | Iteration: 488 | Classification loss: 0.20355 | Regression loss: 0.38687 | Running loss: 0.48341\n",
            "Epoch: 3 | Iteration: 489 | Classification loss: 0.20968 | Regression loss: 0.59323 | Running loss: 0.48398\n",
            "Epoch: 3 | Iteration: 490 | Classification loss: 0.19130 | Regression loss: 0.26110 | Running loss: 0.48480\n",
            "Epoch: 3 | Iteration: 491 | Classification loss: 0.12853 | Regression loss: 0.18366 | Running loss: 0.48382\n",
            "Epoch: 3 | Iteration: 492 | Classification loss: 0.09998 | Regression loss: 0.40184 | Running loss: 0.48343\n",
            "Epoch: 3 | Iteration: 493 | Classification loss: 0.07364 | Regression loss: 0.22894 | Running loss: 0.48192\n",
            "Epoch: 3 | Iteration: 494 | Classification loss: 0.31316 | Regression loss: 0.84679 | Running loss: 0.48249\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.18s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.50s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.442\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.816\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.433\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.405\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.531\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.542\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.505\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.470\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 4 | Iteration: 0 | Classification loss: 0.00037 | Regression loss: 0.15188 | Running loss: 0.48257\n",
            "Epoch: 4 | Iteration: 1 | Classification loss: 0.00037 | Regression loss: 0.06003 | Running loss: 0.48163\n",
            "Epoch: 4 | Iteration: 2 | Classification loss: 0.11608 | Regression loss: 0.27465 | Running loss: 0.48015\n",
            "Epoch: 4 | Iteration: 3 | Classification loss: 0.28165 | Regression loss: 0.57610 | Running loss: 0.48029\n",
            "Epoch: 4 | Iteration: 4 | Classification loss: 0.01342 | Regression loss: 0.00000 | Running loss: 0.47880\n",
            "Epoch: 4 | Iteration: 5 | Classification loss: 0.52545 | Regression loss: 0.70251 | Running loss: 0.48073\n",
            "Epoch: 4 | Iteration: 6 | Classification loss: 0.04674 | Regression loss: 0.27228 | Running loss: 0.48090\n",
            "Epoch: 4 | Iteration: 7 | Classification loss: 0.32280 | Regression loss: 0.69476 | Running loss: 0.48121\n",
            "Epoch: 4 | Iteration: 8 | Classification loss: 0.13322 | Regression loss: 0.34837 | Running loss: 0.48217\n",
            "Epoch: 4 | Iteration: 9 | Classification loss: 0.24853 | Regression loss: 0.42113 | Running loss: 0.48181\n",
            "Epoch: 4 | Iteration: 10 | Classification loss: 0.20491 | Regression loss: 0.39943 | Running loss: 0.48299\n",
            "Epoch: 4 | Iteration: 11 | Classification loss: 0.00074 | Regression loss: 0.13539 | Running loss: 0.48182\n",
            "Epoch: 4 | Iteration: 12 | Classification loss: 0.18270 | Regression loss: 0.34083 | Running loss: 0.48184\n",
            "Epoch: 4 | Iteration: 13 | Classification loss: 0.28089 | Regression loss: 0.43402 | Running loss: 0.48190\n",
            "Epoch: 4 | Iteration: 14 | Classification loss: 0.11953 | Regression loss: 0.26853 | Running loss: 0.48259\n",
            "Epoch: 4 | Iteration: 15 | Classification loss: 0.14361 | Regression loss: 0.49146 | Running loss: 0.48145\n",
            "Epoch: 4 | Iteration: 16 | Classification loss: 0.00386 | Regression loss: 0.02490 | Running loss: 0.47946\n",
            "Epoch: 4 | Iteration: 17 | Classification loss: 0.10185 | Regression loss: 0.25632 | Running loss: 0.47857\n",
            "Epoch: 4 | Iteration: 18 | Classification loss: 0.11663 | Regression loss: 0.30878 | Running loss: 0.47850\n",
            "Epoch: 4 | Iteration: 19 | Classification loss: 0.21591 | Regression loss: 0.54174 | Running loss: 0.47846\n",
            "Epoch: 4 | Iteration: 20 | Classification loss: 0.13745 | Regression loss: 0.32120 | Running loss: 0.47932\n",
            "Epoch: 4 | Iteration: 21 | Classification loss: 0.00054 | Regression loss: 0.09278 | Running loss: 0.47942\n",
            "Epoch: 4 | Iteration: 22 | Classification loss: 0.14377 | Regression loss: 0.39613 | Running loss: 0.47925\n",
            "Epoch: 4 | Iteration: 23 | Classification loss: 0.18501 | Regression loss: 0.35905 | Running loss: 0.48025\n",
            "Epoch: 4 | Iteration: 24 | Classification loss: 0.11484 | Regression loss: 0.42899 | Running loss: 0.48131\n",
            "Epoch: 4 | Iteration: 25 | Classification loss: 0.42507 | Regression loss: 0.70554 | Running loss: 0.48241\n",
            "Epoch: 4 | Iteration: 26 | Classification loss: 0.00062 | Regression loss: 0.04195 | Running loss: 0.48153\n",
            "Epoch: 4 | Iteration: 27 | Classification loss: 0.09910 | Regression loss: 0.25659 | Running loss: 0.48174\n",
            "Epoch: 4 | Iteration: 28 | Classification loss: 0.00455 | Regression loss: 0.01572 | Running loss: 0.48058\n",
            "Epoch: 4 | Iteration: 29 | Classification loss: 0.15354 | Regression loss: 0.31453 | Running loss: 0.47945\n",
            "Epoch: 4 | Iteration: 30 | Classification loss: 0.09609 | Regression loss: 0.34456 | Running loss: 0.47931\n",
            "Epoch: 4 | Iteration: 31 | Classification loss: 0.29863 | Regression loss: 0.29518 | Running loss: 0.47942\n",
            "Epoch: 4 | Iteration: 32 | Classification loss: 0.22616 | Regression loss: 0.45850 | Running loss: 0.47976\n",
            "Epoch: 4 | Iteration: 33 | Classification loss: 0.05522 | Regression loss: 0.17439 | Running loss: 0.48012\n",
            "Epoch: 4 | Iteration: 34 | Classification loss: 0.00012 | Regression loss: 0.00000 | Running loss: 0.47898\n",
            "Epoch: 4 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.47793\n",
            "Epoch: 4 | Iteration: 36 | Classification loss: 0.03770 | Regression loss: 0.20507 | Running loss: 0.47677\n",
            "Epoch: 4 | Iteration: 37 | Classification loss: 0.15123 | Regression loss: 0.37541 | Running loss: 0.47564\n",
            "Epoch: 4 | Iteration: 38 | Classification loss: 0.00061 | Regression loss: 0.01769 | Running loss: 0.47446\n",
            "Epoch: 4 | Iteration: 39 | Classification loss: 0.22137 | Regression loss: 0.39813 | Running loss: 0.47551\n",
            "Epoch: 4 | Iteration: 40 | Classification loss: 0.52646 | Regression loss: 0.63097 | Running loss: 0.47512\n",
            "Epoch: 4 | Iteration: 41 | Classification loss: 0.06439 | Regression loss: 0.22405 | Running loss: 0.47412\n",
            "Epoch: 4 | Iteration: 42 | Classification loss: 0.25329 | Regression loss: 0.50282 | Running loss: 0.47492\n",
            "Epoch: 4 | Iteration: 43 | Classification loss: 0.04108 | Regression loss: 0.14880 | Running loss: 0.47524\n",
            "Epoch: 4 | Iteration: 44 | Classification loss: 0.04014 | Regression loss: 0.17479 | Running loss: 0.47496\n",
            "Epoch: 4 | Iteration: 45 | Classification loss: 0.11879 | Regression loss: 0.40237 | Running loss: 0.47472\n",
            "Epoch: 4 | Iteration: 46 | Classification loss: 0.23319 | Regression loss: 0.56052 | Running loss: 0.47445\n",
            "Epoch: 4 | Iteration: 47 | Classification loss: 0.11953 | Regression loss: 0.20457 | Running loss: 0.47457\n",
            "Epoch: 4 | Iteration: 48 | Classification loss: 0.18251 | Regression loss: 0.28109 | Running loss: 0.47431\n",
            "Epoch: 4 | Iteration: 49 | Classification loss: 0.24840 | Regression loss: 0.58737 | Running loss: 0.47548\n",
            "Epoch: 4 | Iteration: 50 | Classification loss: 0.07978 | Regression loss: 0.25735 | Running loss: 0.47505\n",
            "Epoch: 4 | Iteration: 51 | Classification loss: 0.05344 | Regression loss: 0.19918 | Running loss: 0.47373\n",
            "Epoch: 4 | Iteration: 52 | Classification loss: 0.22797 | Regression loss: 0.47811 | Running loss: 0.47452\n",
            "Epoch: 4 | Iteration: 53 | Classification loss: 0.11522 | Regression loss: 0.27480 | Running loss: 0.47419\n",
            "Epoch: 4 | Iteration: 54 | Classification loss: 0.01420 | Regression loss: 0.03194 | Running loss: 0.47224\n",
            "Epoch: 4 | Iteration: 55 | Classification loss: 0.00271 | Regression loss: 0.00000 | Running loss: 0.47080\n",
            "Epoch: 4 | Iteration: 56 | Classification loss: 0.02711 | Regression loss: 0.02210 | Running loss: 0.47041\n",
            "Epoch: 4 | Iteration: 57 | Classification loss: 0.24768 | Regression loss: 0.44949 | Running loss: 0.47132\n",
            "Epoch: 4 | Iteration: 58 | Classification loss: 0.00130 | Regression loss: 0.04826 | Running loss: 0.47049\n",
            "Epoch: 4 | Iteration: 59 | Classification loss: 0.00375 | Regression loss: 0.03147 | Running loss: 0.47004\n",
            "Epoch: 4 | Iteration: 60 | Classification loss: 0.01185 | Regression loss: 0.15073 | Running loss: 0.46940\n",
            "Epoch: 4 | Iteration: 61 | Classification loss: 0.09956 | Regression loss: 0.37408 | Running loss: 0.46892\n",
            "Epoch: 4 | Iteration: 62 | Classification loss: 0.20633 | Regression loss: 0.47233 | Running loss: 0.47020\n",
            "Epoch: 4 | Iteration: 63 | Classification loss: 0.08308 | Regression loss: 0.29028 | Running loss: 0.46945\n",
            "Epoch: 4 | Iteration: 64 | Classification loss: 0.29808 | Regression loss: 0.50558 | Running loss: 0.46917\n",
            "Epoch: 4 | Iteration: 65 | Classification loss: 0.16530 | Regression loss: 0.50071 | Running loss: 0.46915\n",
            "Epoch: 4 | Iteration: 66 | Classification loss: 0.12448 | Regression loss: 0.23977 | Running loss: 0.46872\n",
            "Epoch: 4 | Iteration: 67 | Classification loss: 0.00048 | Regression loss: 0.06440 | Running loss: 0.46774\n",
            "Epoch: 4 | Iteration: 68 | Classification loss: 0.19503 | Regression loss: 0.47810 | Running loss: 0.46841\n",
            "Epoch: 4 | Iteration: 69 | Classification loss: 0.00330 | Regression loss: 0.09140 | Running loss: 0.46765\n",
            "Epoch: 4 | Iteration: 70 | Classification loss: 0.21676 | Regression loss: 0.32754 | Running loss: 0.46753\n",
            "Epoch: 4 | Iteration: 71 | Classification loss: 0.00243 | Regression loss: 0.05137 | Running loss: 0.46611\n",
            "Epoch: 4 | Iteration: 72 | Classification loss: 0.00526 | Regression loss: 0.03706 | Running loss: 0.46608\n",
            "Epoch: 4 | Iteration: 73 | Classification loss: 0.04043 | Regression loss: 0.11969 | Running loss: 0.46567\n",
            "Epoch: 4 | Iteration: 74 | Classification loss: 0.19557 | Regression loss: 0.45460 | Running loss: 0.46697\n",
            "Epoch: 4 | Iteration: 75 | Classification loss: 0.00599 | Regression loss: 0.09672 | Running loss: 0.46571\n",
            "Epoch: 4 | Iteration: 76 | Classification loss: 0.21486 | Regression loss: 0.23228 | Running loss: 0.46600\n",
            "Epoch: 4 | Iteration: 77 | Classification loss: 0.00261 | Regression loss: 0.08813 | Running loss: 0.46517\n",
            "Epoch: 4 | Iteration: 78 | Classification loss: 0.17424 | Regression loss: 0.35971 | Running loss: 0.46459\n",
            "Epoch: 4 | Iteration: 79 | Classification loss: 0.22010 | Regression loss: 0.44486 | Running loss: 0.46450\n",
            "Epoch: 4 | Iteration: 80 | Classification loss: 0.13498 | Regression loss: 0.31842 | Running loss: 0.46468\n",
            "Epoch: 4 | Iteration: 81 | Classification loss: 0.05631 | Regression loss: 0.21465 | Running loss: 0.46458\n",
            "Epoch: 4 | Iteration: 82 | Classification loss: 1.85909 | Regression loss: 0.07017 | Running loss: 0.46728\n",
            "Epoch: 4 | Iteration: 83 | Classification loss: 0.14054 | Regression loss: 0.39586 | Running loss: 0.46743\n",
            "Epoch: 4 | Iteration: 84 | Classification loss: 0.22573 | Regression loss: 0.34767 | Running loss: 0.46677\n",
            "Epoch: 4 | Iteration: 85 | Classification loss: 0.07689 | Regression loss: 0.20953 | Running loss: 0.46630\n",
            "Epoch: 4 | Iteration: 86 | Classification loss: 0.13465 | Regression loss: 0.32935 | Running loss: 0.46542\n",
            "Epoch: 4 | Iteration: 87 | Classification loss: 0.23046 | Regression loss: 0.50544 | Running loss: 0.46597\n",
            "Epoch: 4 | Iteration: 88 | Classification loss: 0.01359 | Regression loss: 0.21011 | Running loss: 0.46487\n",
            "Epoch: 4 | Iteration: 89 | Classification loss: 0.31741 | Regression loss: 0.50502 | Running loss: 0.46522\n",
            "Epoch: 4 | Iteration: 90 | Classification loss: 0.16743 | Regression loss: 0.39465 | Running loss: 0.46579\n",
            "Epoch: 4 | Iteration: 91 | Classification loss: 0.13582 | Regression loss: 0.33102 | Running loss: 0.46542\n",
            "Epoch: 4 | Iteration: 92 | Classification loss: 0.02810 | Regression loss: 0.16361 | Running loss: 0.46465\n",
            "Epoch: 4 | Iteration: 93 | Classification loss: 0.11747 | Regression loss: 0.19919 | Running loss: 0.46447\n",
            "Epoch: 4 | Iteration: 94 | Classification loss: 0.24889 | Regression loss: 0.42008 | Running loss: 0.46566\n",
            "Epoch: 4 | Iteration: 95 | Classification loss: 0.22667 | Regression loss: 0.60747 | Running loss: 0.46604\n",
            "Epoch: 4 | Iteration: 96 | Classification loss: 0.22575 | Regression loss: 0.54535 | Running loss: 0.46614\n",
            "Epoch: 4 | Iteration: 97 | Classification loss: 0.05355 | Regression loss: 0.19742 | Running loss: 0.46653\n",
            "Epoch: 4 | Iteration: 98 | Classification loss: 0.07205 | Regression loss: 0.18328 | Running loss: 0.46576\n",
            "Epoch: 4 | Iteration: 99 | Classification loss: 0.04944 | Regression loss: 0.05492 | Running loss: 0.46480\n",
            "Epoch: 4 | Iteration: 100 | Classification loss: 0.11530 | Regression loss: 0.37246 | Running loss: 0.46535\n",
            "Epoch: 4 | Iteration: 101 | Classification loss: 0.13097 | Regression loss: 0.50566 | Running loss: 0.46659\n",
            "Epoch: 4 | Iteration: 102 | Classification loss: 0.15972 | Regression loss: 0.29964 | Running loss: 0.46745\n",
            "Epoch: 4 | Iteration: 103 | Classification loss: 0.16796 | Regression loss: 0.36644 | Running loss: 0.46777\n",
            "Epoch: 4 | Iteration: 104 | Classification loss: 0.04735 | Regression loss: 0.28260 | Running loss: 0.46830\n",
            "Epoch: 4 | Iteration: 105 | Classification loss: 0.06795 | Regression loss: 0.31958 | Running loss: 0.46768\n",
            "Epoch: 4 | Iteration: 106 | Classification loss: 0.18010 | Regression loss: 0.51996 | Running loss: 0.46777\n",
            "Epoch: 4 | Iteration: 107 | Classification loss: 0.00052 | Regression loss: 0.04145 | Running loss: 0.46629\n",
            "Epoch: 4 | Iteration: 108 | Classification loss: 0.09565 | Regression loss: 0.34785 | Running loss: 0.46547\n",
            "Epoch: 4 | Iteration: 109 | Classification loss: 0.11253 | Regression loss: 0.02933 | Running loss: 0.46465\n",
            "Epoch: 4 | Iteration: 110 | Classification loss: 0.05270 | Regression loss: 0.25364 | Running loss: 0.46439\n",
            "Epoch: 4 | Iteration: 111 | Classification loss: 0.08291 | Regression loss: 0.31126 | Running loss: 0.46436\n",
            "Epoch: 4 | Iteration: 112 | Classification loss: 0.33586 | Regression loss: 0.52321 | Running loss: 0.46551\n",
            "Epoch: 4 | Iteration: 113 | Classification loss: 0.14421 | Regression loss: 0.39783 | Running loss: 0.46508\n",
            "Epoch: 4 | Iteration: 114 | Classification loss: 0.03130 | Regression loss: 0.16971 | Running loss: 0.46490\n",
            "Epoch: 4 | Iteration: 115 | Classification loss: 0.19504 | Regression loss: 0.52343 | Running loss: 0.46627\n",
            "Epoch: 4 | Iteration: 116 | Classification loss: 0.00054 | Regression loss: 0.20396 | Running loss: 0.46542\n",
            "Epoch: 4 | Iteration: 117 | Classification loss: 0.10798 | Regression loss: 0.43580 | Running loss: 0.46582\n",
            "Epoch: 4 | Iteration: 118 | Classification loss: 0.05949 | Regression loss: 0.32093 | Running loss: 0.46636\n",
            "Epoch: 4 | Iteration: 119 | Classification loss: 0.09442 | Regression loss: 0.42531 | Running loss: 0.46690\n",
            "Epoch: 4 | Iteration: 120 | Classification loss: 0.00066 | Regression loss: 0.06821 | Running loss: 0.46704\n",
            "Epoch: 4 | Iteration: 121 | Classification loss: 0.11914 | Regression loss: 0.29861 | Running loss: 0.46663\n",
            "Epoch: 4 | Iteration: 122 | Classification loss: 0.05001 | Regression loss: 0.22559 | Running loss: 0.46627\n",
            "Epoch: 4 | Iteration: 123 | Classification loss: 0.05505 | Regression loss: 0.16414 | Running loss: 0.46504\n",
            "Epoch: 4 | Iteration: 124 | Classification loss: 0.00112 | Regression loss: 0.01303 | Running loss: 0.46365\n",
            "Epoch: 4 | Iteration: 125 | Classification loss: 0.23277 | Regression loss: 0.41256 | Running loss: 0.46373\n",
            "Epoch: 4 | Iteration: 126 | Classification loss: 0.11542 | Regression loss: 0.36815 | Running loss: 0.46405\n",
            "Epoch: 4 | Iteration: 127 | Classification loss: 0.17089 | Regression loss: 0.45637 | Running loss: 0.46458\n",
            "Epoch: 4 | Iteration: 128 | Classification loss: 0.04190 | Regression loss: 0.22830 | Running loss: 0.46344\n",
            "Epoch: 4 | Iteration: 129 | Classification loss: 0.13528 | Regression loss: 0.43443 | Running loss: 0.46334\n",
            "Epoch: 4 | Iteration: 130 | Classification loss: 0.16594 | Regression loss: 0.29206 | Running loss: 0.46234\n",
            "Epoch: 4 | Iteration: 131 | Classification loss: 0.12858 | Regression loss: 0.40848 | Running loss: 0.46271\n",
            "Epoch: 4 | Iteration: 132 | Classification loss: 0.14257 | Regression loss: 0.42089 | Running loss: 0.46373\n",
            "Epoch: 4 | Iteration: 133 | Classification loss: 0.05399 | Regression loss: 0.11163 | Running loss: 0.46397\n",
            "Epoch: 4 | Iteration: 134 | Classification loss: 0.13133 | Regression loss: 0.40318 | Running loss: 0.46332\n",
            "Epoch: 4 | Iteration: 135 | Classification loss: 0.16003 | Regression loss: 0.37601 | Running loss: 0.46356\n",
            "Epoch: 4 | Iteration: 136 | Classification loss: 0.14157 | Regression loss: 0.44012 | Running loss: 0.46234\n",
            "Epoch: 4 | Iteration: 137 | Classification loss: 0.00068 | Regression loss: 0.06406 | Running loss: 0.46239\n",
            "Epoch: 4 | Iteration: 138 | Classification loss: 0.19849 | Regression loss: 0.59809 | Running loss: 0.46309\n",
            "Epoch: 4 | Iteration: 139 | Classification loss: 0.35994 | Regression loss: 0.23350 | Running loss: 0.46233\n",
            "Epoch: 4 | Iteration: 140 | Classification loss: 0.19751 | Regression loss: 0.53450 | Running loss: 0.46329\n",
            "Epoch: 4 | Iteration: 141 | Classification loss: 0.25278 | Regression loss: 0.34725 | Running loss: 0.46395\n",
            "Epoch: 4 | Iteration: 142 | Classification loss: 0.22365 | Regression loss: 0.43969 | Running loss: 0.46527\n",
            "Epoch: 4 | Iteration: 143 | Classification loss: 0.08839 | Regression loss: 0.38033 | Running loss: 0.46480\n",
            "Epoch: 4 | Iteration: 144 | Classification loss: 0.17142 | Regression loss: 0.41839 | Running loss: 0.46470\n",
            "Epoch: 4 | Iteration: 145 | Classification loss: 0.19150 | Regression loss: 0.45680 | Running loss: 0.46547\n",
            "Epoch: 4 | Iteration: 146 | Classification loss: 0.10521 | Regression loss: 0.42686 | Running loss: 0.46564\n",
            "Epoch: 4 | Iteration: 147 | Classification loss: 0.26458 | Regression loss: 0.60497 | Running loss: 0.46660\n",
            "Epoch: 4 | Iteration: 148 | Classification loss: 0.16623 | Regression loss: 0.48264 | Running loss: 0.46787\n",
            "Epoch: 4 | Iteration: 149 | Classification loss: 0.00072 | Regression loss: 0.10554 | Running loss: 0.46766\n",
            "Epoch: 4 | Iteration: 150 | Classification loss: 0.00008 | Regression loss: 0.00000 | Running loss: 0.46758\n",
            "Epoch: 4 | Iteration: 151 | Classification loss: 0.00082 | Regression loss: 0.02516 | Running loss: 0.46684\n",
            "Epoch: 4 | Iteration: 152 | Classification loss: 0.10105 | Regression loss: 0.30436 | Running loss: 0.46618\n",
            "Epoch: 4 | Iteration: 153 | Classification loss: 0.13528 | Regression loss: 0.36499 | Running loss: 0.46607\n",
            "Epoch: 4 | Iteration: 154 | Classification loss: 0.16661 | Regression loss: 0.42574 | Running loss: 0.46714\n",
            "Epoch: 4 | Iteration: 155 | Classification loss: 0.19026 | Regression loss: 0.50276 | Running loss: 0.46776\n",
            "Epoch: 4 | Iteration: 156 | Classification loss: 0.05818 | Regression loss: 0.25244 | Running loss: 0.46728\n",
            "Epoch: 4 | Iteration: 157 | Classification loss: 0.15538 | Regression loss: 0.40875 | Running loss: 0.46720\n",
            "Epoch: 4 | Iteration: 158 | Classification loss: 0.16198 | Regression loss: 0.34175 | Running loss: 0.46760\n",
            "Epoch: 4 | Iteration: 159 | Classification loss: 0.37176 | Regression loss: 0.58943 | Running loss: 0.46843\n",
            "Epoch: 4 | Iteration: 160 | Classification loss: 0.00041 | Regression loss: 0.12531 | Running loss: 0.46815\n",
            "Epoch: 4 | Iteration: 161 | Classification loss: 0.40470 | Regression loss: 0.03553 | Running loss: 0.46761\n",
            "Epoch: 4 | Iteration: 162 | Classification loss: 0.00015 | Regression loss: 0.02850 | Running loss: 0.46627\n",
            "Epoch: 4 | Iteration: 163 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.46436\n",
            "Epoch: 4 | Iteration: 164 | Classification loss: 0.30766 | Regression loss: 0.57445 | Running loss: 0.46471\n",
            "Epoch: 4 | Iteration: 165 | Classification loss: 0.11904 | Regression loss: 0.41663 | Running loss: 0.46543\n",
            "Epoch: 4 | Iteration: 166 | Classification loss: 0.13536 | Regression loss: 0.26136 | Running loss: 0.46503\n",
            "Epoch: 4 | Iteration: 167 | Classification loss: 0.13901 | Regression loss: 0.34675 | Running loss: 0.46547\n",
            "Epoch: 4 | Iteration: 168 | Classification loss: 0.85032 | Regression loss: 0.49229 | Running loss: 0.46698\n",
            "Epoch: 4 | Iteration: 169 | Classification loss: 0.37976 | Regression loss: 0.79839 | Running loss: 0.46934\n",
            "Epoch: 4 | Iteration: 170 | Classification loss: 0.04833 | Regression loss: 0.26072 | Running loss: 0.46849\n",
            "Epoch: 4 | Iteration: 171 | Classification loss: 0.42421 | Regression loss: 0.48335 | Running loss: 0.46943\n",
            "Epoch: 4 | Iteration: 172 | Classification loss: 0.05137 | Regression loss: 0.19574 | Running loss: 0.46803\n",
            "Epoch: 4 | Iteration: 173 | Classification loss: 0.00968 | Regression loss: 0.00000 | Running loss: 0.46651\n",
            "Epoch: 4 | Iteration: 174 | Classification loss: 0.11306 | Regression loss: 0.36559 | Running loss: 0.46686\n",
            "Epoch: 4 | Iteration: 175 | Classification loss: 0.06250 | Regression loss: 0.22181 | Running loss: 0.46562\n",
            "Epoch: 4 | Iteration: 176 | Classification loss: 0.16247 | Regression loss: 0.48903 | Running loss: 0.46688\n",
            "Epoch: 4 | Iteration: 177 | Classification loss: 0.00027 | Regression loss: 0.07459 | Running loss: 0.46694\n",
            "Epoch: 4 | Iteration: 178 | Classification loss: 0.06050 | Regression loss: 0.22192 | Running loss: 0.46687\n",
            "Epoch: 4 | Iteration: 179 | Classification loss: 0.13432 | Regression loss: 0.22558 | Running loss: 0.46679\n",
            "Epoch: 4 | Iteration: 180 | Classification loss: 0.03098 | Regression loss: 0.14236 | Running loss: 0.46553\n",
            "Epoch: 4 | Iteration: 181 | Classification loss: 0.03068 | Regression loss: 0.23263 | Running loss: 0.46517\n",
            "Epoch: 4 | Iteration: 182 | Classification loss: 0.03961 | Regression loss: 0.14205 | Running loss: 0.46384\n",
            "Epoch: 4 | Iteration: 183 | Classification loss: 0.13481 | Regression loss: 0.35694 | Running loss: 0.46385\n",
            "Epoch: 4 | Iteration: 184 | Classification loss: 0.24748 | Regression loss: 0.57656 | Running loss: 0.46422\n",
            "Epoch: 4 | Iteration: 185 | Classification loss: 0.06000 | Regression loss: 0.20083 | Running loss: 0.46423\n",
            "Epoch: 4 | Iteration: 186 | Classification loss: 0.23634 | Regression loss: 0.50470 | Running loss: 0.46352\n",
            "Epoch: 4 | Iteration: 187 | Classification loss: 0.12131 | Regression loss: 0.39919 | Running loss: 0.46349\n",
            "Epoch: 4 | Iteration: 188 | Classification loss: 0.00046 | Regression loss: 0.04833 | Running loss: 0.46261\n",
            "Epoch: 4 | Iteration: 189 | Classification loss: 0.00023 | Regression loss: 0.03071 | Running loss: 0.46122\n",
            "Epoch: 4 | Iteration: 190 | Classification loss: 0.07593 | Regression loss: 0.16501 | Running loss: 0.46050\n",
            "Epoch: 4 | Iteration: 191 | Classification loss: 0.12913 | Regression loss: 0.46523 | Running loss: 0.46079\n",
            "Epoch: 4 | Iteration: 192 | Classification loss: 0.04584 | Regression loss: 0.18208 | Running loss: 0.46055\n",
            "Epoch: 4 | Iteration: 193 | Classification loss: 0.11547 | Regression loss: 0.32475 | Running loss: 0.46033\n",
            "Epoch: 4 | Iteration: 194 | Classification loss: 0.11607 | Regression loss: 0.24253 | Running loss: 0.45983\n",
            "Epoch: 4 | Iteration: 195 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.45848\n",
            "Epoch: 4 | Iteration: 196 | Classification loss: 0.11746 | Regression loss: 0.35622 | Running loss: 0.45790\n",
            "Epoch: 4 | Iteration: 197 | Classification loss: 0.00099 | Regression loss: 0.03636 | Running loss: 0.45702\n",
            "Epoch: 4 | Iteration: 198 | Classification loss: 0.19832 | Regression loss: 0.54600 | Running loss: 0.45790\n",
            "Epoch: 4 | Iteration: 199 | Classification loss: 0.19049 | Regression loss: 0.49157 | Running loss: 0.45797\n",
            "Epoch: 4 | Iteration: 200 | Classification loss: 0.00026 | Regression loss: 0.03558 | Running loss: 0.45755\n",
            "Epoch: 4 | Iteration: 201 | Classification loss: 0.31093 | Regression loss: 0.62858 | Running loss: 0.45695\n",
            "Epoch: 4 | Iteration: 202 | Classification loss: 0.00078 | Regression loss: 0.04512 | Running loss: 0.45647\n",
            "Epoch: 4 | Iteration: 203 | Classification loss: 0.45281 | Regression loss: 0.68282 | Running loss: 0.45870\n",
            "Epoch: 4 | Iteration: 204 | Classification loss: 0.05125 | Regression loss: 0.17571 | Running loss: 0.45912\n",
            "Epoch: 4 | Iteration: 205 | Classification loss: 0.04971 | Regression loss: 0.14211 | Running loss: 0.45847\n",
            "Epoch: 4 | Iteration: 206 | Classification loss: 0.05663 | Regression loss: 0.22395 | Running loss: 0.45763\n",
            "Epoch: 4 | Iteration: 207 | Classification loss: 0.15781 | Regression loss: 0.42171 | Running loss: 0.45770\n",
            "Epoch: 4 | Iteration: 208 | Classification loss: 0.20849 | Regression loss: 0.45704 | Running loss: 0.45776\n",
            "Epoch: 4 | Iteration: 209 | Classification loss: 0.19394 | Regression loss: 0.46099 | Running loss: 0.45763\n",
            "Epoch: 4 | Iteration: 210 | Classification loss: 0.05586 | Regression loss: 0.19721 | Running loss: 0.45654\n",
            "Epoch: 4 | Iteration: 211 | Classification loss: 0.13981 | Regression loss: 0.44649 | Running loss: 0.45707\n",
            "Epoch: 4 | Iteration: 212 | Classification loss: 0.18333 | Regression loss: 0.37462 | Running loss: 0.45706\n",
            "Epoch: 4 | Iteration: 213 | Classification loss: 0.09884 | Regression loss: 0.35425 | Running loss: 0.45610\n",
            "Epoch: 4 | Iteration: 214 | Classification loss: 0.07117 | Regression loss: 0.28039 | Running loss: 0.45615\n",
            "Epoch: 4 | Iteration: 215 | Classification loss: 0.00047 | Regression loss: 0.05939 | Running loss: 0.45521\n",
            "Epoch: 4 | Iteration: 216 | Classification loss: 0.05771 | Regression loss: 0.18574 | Running loss: 0.45481\n",
            "Epoch: 4 | Iteration: 217 | Classification loss: 0.11122 | Regression loss: 0.30498 | Running loss: 0.45495\n",
            "Epoch: 4 | Iteration: 218 | Classification loss: 0.11140 | Regression loss: 0.40335 | Running loss: 0.45504\n",
            "Epoch: 4 | Iteration: 219 | Classification loss: 0.11400 | Regression loss: 0.24267 | Running loss: 0.45432\n",
            "Epoch: 4 | Iteration: 220 | Classification loss: 0.15320 | Regression loss: 0.33209 | Running loss: 0.45458\n",
            "Epoch: 4 | Iteration: 221 | Classification loss: 0.09757 | Regression loss: 0.30730 | Running loss: 0.45526\n",
            "Epoch: 4 | Iteration: 222 | Classification loss: 0.04505 | Regression loss: 0.15032 | Running loss: 0.45502\n",
            "Epoch: 4 | Iteration: 223 | Classification loss: 0.24770 | Regression loss: 0.55963 | Running loss: 0.45588\n",
            "Epoch: 4 | Iteration: 224 | Classification loss: 0.34449 | Regression loss: 0.62225 | Running loss: 0.45604\n",
            "Epoch: 4 | Iteration: 225 | Classification loss: 0.09636 | Regression loss: 0.39153 | Running loss: 0.45512\n",
            "Epoch: 4 | Iteration: 226 | Classification loss: 0.15390 | Regression loss: 0.37812 | Running loss: 0.45451\n",
            "Epoch: 4 | Iteration: 227 | Classification loss: 0.23987 | Regression loss: 0.53397 | Running loss: 0.45521\n",
            "Epoch: 4 | Iteration: 228 | Classification loss: 0.09203 | Regression loss: 0.20490 | Running loss: 0.45580\n",
            "Epoch: 4 | Iteration: 229 | Classification loss: 0.23096 | Regression loss: 0.45657 | Running loss: 0.45524\n",
            "Epoch: 4 | Iteration: 230 | Classification loss: 0.00064 | Regression loss: 0.02079 | Running loss: 0.45375\n",
            "Epoch: 4 | Iteration: 231 | Classification loss: 0.15869 | Regression loss: 0.47486 | Running loss: 0.45411\n",
            "Epoch: 4 | Iteration: 232 | Classification loss: 0.07892 | Regression loss: 0.18136 | Running loss: 0.45333\n",
            "Epoch: 4 | Iteration: 233 | Classification loss: 0.12737 | Regression loss: 0.32892 | Running loss: 0.45416\n",
            "Epoch: 4 | Iteration: 234 | Classification loss: 0.28281 | Regression loss: 0.49005 | Running loss: 0.45505\n",
            "Epoch: 4 | Iteration: 235 | Classification loss: 0.12345 | Regression loss: 0.46238 | Running loss: 0.45514\n",
            "Epoch: 4 | Iteration: 236 | Classification loss: 0.00252 | Regression loss: 0.00000 | Running loss: 0.45310\n",
            "Epoch: 4 | Iteration: 237 | Classification loss: 0.06827 | Regression loss: 0.21649 | Running loss: 0.45353\n",
            "Epoch: 4 | Iteration: 238 | Classification loss: 0.04590 | Regression loss: 0.19105 | Running loss: 0.45252\n",
            "Epoch: 4 | Iteration: 239 | Classification loss: 0.15165 | Regression loss: 0.42237 | Running loss: 0.45365\n",
            "Epoch: 4 | Iteration: 240 | Classification loss: 0.00026 | Regression loss: 0.01939 | Running loss: 0.45219\n",
            "Epoch: 4 | Iteration: 241 | Classification loss: 0.22936 | Regression loss: 0.34801 | Running loss: 0.45335\n",
            "Epoch: 4 | Iteration: 242 | Classification loss: 0.04792 | Regression loss: 0.31162 | Running loss: 0.45405\n",
            "Epoch: 4 | Iteration: 243 | Classification loss: 0.14207 | Regression loss: 0.37033 | Running loss: 0.45343\n",
            "Epoch: 4 | Iteration: 244 | Classification loss: 0.00026 | Regression loss: 0.01819 | Running loss: 0.45345\n",
            "Epoch: 4 | Iteration: 245 | Classification loss: 0.17842 | Regression loss: 0.42054 | Running loss: 0.45413\n",
            "Epoch: 4 | Iteration: 246 | Classification loss: 0.18057 | Regression loss: 0.50948 | Running loss: 0.45477\n",
            "Epoch: 4 | Iteration: 247 | Classification loss: 0.26701 | Regression loss: 0.40352 | Running loss: 0.45611\n",
            "Epoch: 4 | Iteration: 248 | Classification loss: 0.18180 | Regression loss: 0.38717 | Running loss: 0.45545\n",
            "Epoch: 4 | Iteration: 249 | Classification loss: 0.21009 | Regression loss: 0.49709 | Running loss: 0.45524\n",
            "Epoch: 4 | Iteration: 250 | Classification loss: 0.27459 | Regression loss: 0.66553 | Running loss: 0.45602\n",
            "Epoch: 4 | Iteration: 251 | Classification loss: 0.24248 | Regression loss: 0.44097 | Running loss: 0.45522\n",
            "Epoch: 4 | Iteration: 252 | Classification loss: 0.10416 | Regression loss: 0.40615 | Running loss: 0.45563\n",
            "Epoch: 4 | Iteration: 253 | Classification loss: 0.21426 | Regression loss: 0.48524 | Running loss: 0.45549\n",
            "Epoch: 4 | Iteration: 254 | Classification loss: 0.43884 | Regression loss: 0.62198 | Running loss: 0.45614\n",
            "Epoch: 4 | Iteration: 255 | Classification loss: 0.12829 | Regression loss: 0.31344 | Running loss: 0.45693\n",
            "Epoch: 4 | Iteration: 256 | Classification loss: 0.00022 | Regression loss: 0.02365 | Running loss: 0.45691\n",
            "Epoch: 4 | Iteration: 257 | Classification loss: 0.19251 | Regression loss: 0.41389 | Running loss: 0.45735\n",
            "Epoch: 4 | Iteration: 258 | Classification loss: 0.12795 | Regression loss: 0.39443 | Running loss: 0.45667\n",
            "Epoch: 4 | Iteration: 259 | Classification loss: 0.03990 | Regression loss: 0.11587 | Running loss: 0.45692\n",
            "Epoch: 4 | Iteration: 260 | Classification loss: 0.14672 | Regression loss: 0.30457 | Running loss: 0.45736\n",
            "Epoch: 4 | Iteration: 261 | Classification loss: 0.09435 | Regression loss: 0.21755 | Running loss: 0.45789\n",
            "Epoch: 4 | Iteration: 262 | Classification loss: 0.19199 | Regression loss: 0.48105 | Running loss: 0.45920\n",
            "Epoch: 4 | Iteration: 263 | Classification loss: 0.07198 | Regression loss: 0.36125 | Running loss: 0.45856\n",
            "Epoch: 4 | Iteration: 264 | Classification loss: 0.26666 | Regression loss: 0.41652 | Running loss: 0.45873\n",
            "Epoch: 4 | Iteration: 265 | Classification loss: 0.08583 | Regression loss: 0.27607 | Running loss: 0.45938\n",
            "Epoch: 4 | Iteration: 266 | Classification loss: 0.12116 | Regression loss: 0.34296 | Running loss: 0.45852\n",
            "Epoch: 4 | Iteration: 267 | Classification loss: 0.09648 | Regression loss: 0.31763 | Running loss: 0.45922\n",
            "Epoch: 4 | Iteration: 268 | Classification loss: 0.04060 | Regression loss: 0.23388 | Running loss: 0.45871\n",
            "Epoch: 4 | Iteration: 269 | Classification loss: 0.20475 | Regression loss: 0.56814 | Running loss: 0.45927\n",
            "Epoch: 4 | Iteration: 270 | Classification loss: 0.04114 | Regression loss: 0.13902 | Running loss: 0.45863\n",
            "Epoch: 4 | Iteration: 271 | Classification loss: 0.05990 | Regression loss: 0.25181 | Running loss: 0.45759\n",
            "Epoch: 4 | Iteration: 272 | Classification loss: 0.00029 | Regression loss: 0.04547 | Running loss: 0.45470\n",
            "Epoch: 4 | Iteration: 273 | Classification loss: 0.06961 | Regression loss: 0.15364 | Running loss: 0.45507\n",
            "Epoch: 4 | Iteration: 274 | Classification loss: 0.07210 | Regression loss: 0.12925 | Running loss: 0.45494\n",
            "Epoch: 4 | Iteration: 275 | Classification loss: 0.58265 | Regression loss: 0.64247 | Running loss: 0.45736\n",
            "Epoch: 4 | Iteration: 276 | Classification loss: 0.09415 | Regression loss: 0.33692 | Running loss: 0.45707\n",
            "Epoch: 4 | Iteration: 277 | Classification loss: 0.06324 | Regression loss: 0.31539 | Running loss: 0.45626\n",
            "Epoch: 4 | Iteration: 278 | Classification loss: 0.09522 | Regression loss: 0.38153 | Running loss: 0.45721\n",
            "Epoch: 4 | Iteration: 279 | Classification loss: 0.07799 | Regression loss: 0.30950 | Running loss: 0.45721\n",
            "Epoch: 4 | Iteration: 280 | Classification loss: 0.15881 | Regression loss: 0.40120 | Running loss: 0.45656\n",
            "Epoch: 4 | Iteration: 281 | Classification loss: 0.05289 | Regression loss: 0.24291 | Running loss: 0.45716\n",
            "Epoch: 4 | Iteration: 282 | Classification loss: 0.00021 | Regression loss: 0.02052 | Running loss: 0.45660\n",
            "Epoch: 4 | Iteration: 283 | Classification loss: 0.08008 | Regression loss: 0.35569 | Running loss: 0.45736\n",
            "Epoch: 4 | Iteration: 284 | Classification loss: 0.00043 | Regression loss: 0.00000 | Running loss: 0.45736\n",
            "Epoch: 4 | Iteration: 285 | Classification loss: 0.23218 | Regression loss: 0.49214 | Running loss: 0.45881\n",
            "Epoch: 4 | Iteration: 286 | Classification loss: 0.03936 | Regression loss: 0.21897 | Running loss: 0.45858\n",
            "Epoch: 4 | Iteration: 287 | Classification loss: 0.25972 | Regression loss: 0.62762 | Running loss: 0.45863\n",
            "Epoch: 4 | Iteration: 288 | Classification loss: 0.06470 | Regression loss: 0.17807 | Running loss: 0.45857\n",
            "Epoch: 4 | Iteration: 289 | Classification loss: 0.05690 | Regression loss: 0.17067 | Running loss: 0.45796\n",
            "Epoch: 4 | Iteration: 290 | Classification loss: 0.04514 | Regression loss: 0.16001 | Running loss: 0.45787\n",
            "Epoch: 4 | Iteration: 291 | Classification loss: 0.04482 | Regression loss: 0.18469 | Running loss: 0.45805\n",
            "Epoch: 4 | Iteration: 292 | Classification loss: 0.00042 | Regression loss: 0.02402 | Running loss: 0.45755\n",
            "Epoch: 4 | Iteration: 293 | Classification loss: 0.00515 | Regression loss: 0.01746 | Running loss: 0.45615\n",
            "Epoch: 4 | Iteration: 294 | Classification loss: 0.05625 | Regression loss: 0.20206 | Running loss: 0.45624\n",
            "Epoch: 4 | Iteration: 295 | Classification loss: 0.08741 | Regression loss: 0.29482 | Running loss: 0.45569\n",
            "Epoch: 4 | Iteration: 296 | Classification loss: 0.10555 | Regression loss: 0.34007 | Running loss: 0.45422\n",
            "Epoch: 4 | Iteration: 297 | Classification loss: 0.11798 | Regression loss: 0.22269 | Running loss: 0.45376\n",
            "Epoch: 4 | Iteration: 298 | Classification loss: 0.13786 | Regression loss: 0.28343 | Running loss: 0.45334\n",
            "Epoch: 4 | Iteration: 299 | Classification loss: 0.25385 | Regression loss: 0.64776 | Running loss: 0.45295\n",
            "Epoch: 4 | Iteration: 300 | Classification loss: 0.00020 | Regression loss: 0.11503 | Running loss: 0.45193\n",
            "Epoch: 4 | Iteration: 301 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.45088\n",
            "Epoch: 4 | Iteration: 302 | Classification loss: 0.18867 | Regression loss: 0.61218 | Running loss: 0.45091\n",
            "Epoch: 4 | Iteration: 303 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.44971\n",
            "Epoch: 4 | Iteration: 304 | Classification loss: 0.09271 | Regression loss: 0.13960 | Running loss: 0.44870\n",
            "Epoch: 4 | Iteration: 305 | Classification loss: 0.00035 | Regression loss: 0.21961 | Running loss: 0.44680\n",
            "Epoch: 4 | Iteration: 306 | Classification loss: 0.25024 | Regression loss: 0.50039 | Running loss: 0.44772\n",
            "Epoch: 4 | Iteration: 307 | Classification loss: 0.09638 | Regression loss: 0.28280 | Running loss: 0.44561\n",
            "Epoch: 4 | Iteration: 308 | Classification loss: 0.50586 | Regression loss: 0.43078 | Running loss: 0.44691\n",
            "Epoch: 4 | Iteration: 309 | Classification loss: 0.07670 | Regression loss: 0.33385 | Running loss: 0.44665\n",
            "Epoch: 4 | Iteration: 310 | Classification loss: 0.07942 | Regression loss: 0.35688 | Running loss: 0.44700\n",
            "Epoch: 4 | Iteration: 311 | Classification loss: 0.14266 | Regression loss: 0.49913 | Running loss: 0.44743\n",
            "Epoch: 4 | Iteration: 312 | Classification loss: 0.11545 | Regression loss: 0.46240 | Running loss: 0.44817\n",
            "Epoch: 4 | Iteration: 313 | Classification loss: 0.03779 | Regression loss: 0.15846 | Running loss: 0.44799\n",
            "Epoch: 4 | Iteration: 314 | Classification loss: 0.03642 | Regression loss: 0.20366 | Running loss: 0.44629\n",
            "Epoch: 4 | Iteration: 315 | Classification loss: 0.16514 | Regression loss: 0.42321 | Running loss: 0.44711\n",
            "Epoch: 4 | Iteration: 316 | Classification loss: 0.15645 | Regression loss: 0.41710 | Running loss: 0.44681\n",
            "Epoch: 4 | Iteration: 317 | Classification loss: 0.11491 | Regression loss: 0.36057 | Running loss: 0.44675\n",
            "Epoch: 4 | Iteration: 318 | Classification loss: 0.05525 | Regression loss: 0.17120 | Running loss: 0.44689\n",
            "Epoch: 4 | Iteration: 319 | Classification loss: 0.08551 | Regression loss: 0.17176 | Running loss: 0.44641\n",
            "Epoch: 4 | Iteration: 320 | Classification loss: 0.00738 | Regression loss: 0.03766 | Running loss: 0.44555\n",
            "Epoch: 4 | Iteration: 321 | Classification loss: 0.00318 | Regression loss: 0.04624 | Running loss: 0.44435\n",
            "Epoch: 4 | Iteration: 322 | Classification loss: 0.11404 | Regression loss: 0.38531 | Running loss: 0.44434\n",
            "Epoch: 4 | Iteration: 323 | Classification loss: 0.08342 | Regression loss: 0.17544 | Running loss: 0.44341\n",
            "Epoch: 4 | Iteration: 324 | Classification loss: 0.22567 | Regression loss: 0.46597 | Running loss: 0.44354\n",
            "Epoch: 4 | Iteration: 325 | Classification loss: 0.16784 | Regression loss: 0.42885 | Running loss: 0.44307\n",
            "Epoch: 4 | Iteration: 326 | Classification loss: 0.09428 | Regression loss: 0.34559 | Running loss: 0.44304\n",
            "Epoch: 4 | Iteration: 327 | Classification loss: 0.11931 | Regression loss: 0.36854 | Running loss: 0.44388\n",
            "Epoch: 4 | Iteration: 328 | Classification loss: 0.23044 | Regression loss: 0.38979 | Running loss: 0.44427\n",
            "Epoch: 4 | Iteration: 329 | Classification loss: 0.12666 | Regression loss: 0.47160 | Running loss: 0.44478\n",
            "Epoch: 4 | Iteration: 330 | Classification loss: 0.08303 | Regression loss: 0.26789 | Running loss: 0.44442\n",
            "Epoch: 4 | Iteration: 331 | Classification loss: 0.12048 | Regression loss: 0.37409 | Running loss: 0.44431\n",
            "Epoch: 4 | Iteration: 332 | Classification loss: 0.07282 | Regression loss: 0.38148 | Running loss: 0.44418\n",
            "Epoch: 4 | Iteration: 333 | Classification loss: 0.10130 | Regression loss: 0.23161 | Running loss: 0.44364\n",
            "Epoch: 4 | Iteration: 334 | Classification loss: 0.15112 | Regression loss: 0.24985 | Running loss: 0.44330\n",
            "Epoch: 4 | Iteration: 335 | Classification loss: 0.00029 | Regression loss: 0.03822 | Running loss: 0.44250\n",
            "Epoch: 4 | Iteration: 336 | Classification loss: 0.23501 | Regression loss: 0.41873 | Running loss: 0.44324\n",
            "Epoch: 4 | Iteration: 337 | Classification loss: 0.11124 | Regression loss: 0.43259 | Running loss: 0.44303\n",
            "Epoch: 4 | Iteration: 338 | Classification loss: 0.00033 | Regression loss: 0.03229 | Running loss: 0.44310\n",
            "Epoch: 4 | Iteration: 339 | Classification loss: 0.07590 | Regression loss: 0.32403 | Running loss: 0.44251\n",
            "Epoch: 4 | Iteration: 340 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.44137\n",
            "Epoch: 4 | Iteration: 341 | Classification loss: 0.13936 | Regression loss: 0.37678 | Running loss: 0.44138\n",
            "Epoch: 4 | Iteration: 342 | Classification loss: 0.10118 | Regression loss: 0.33750 | Running loss: 0.44135\n",
            "Epoch: 4 | Iteration: 343 | Classification loss: 0.27540 | Regression loss: 0.60864 | Running loss: 0.44226\n",
            "Epoch: 4 | Iteration: 344 | Classification loss: 0.22955 | Regression loss: 0.48975 | Running loss: 0.44360\n",
            "Epoch: 4 | Iteration: 345 | Classification loss: 0.04944 | Regression loss: 0.19961 | Running loss: 0.44399\n",
            "Epoch: 4 | Iteration: 346 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.44310\n",
            "Epoch: 4 | Iteration: 347 | Classification loss: 0.27583 | Regression loss: 0.55613 | Running loss: 0.44470\n",
            "Epoch: 4 | Iteration: 348 | Classification loss: 0.04630 | Regression loss: 0.14298 | Running loss: 0.44365\n",
            "Epoch: 4 | Iteration: 349 | Classification loss: 0.06406 | Regression loss: 0.22213 | Running loss: 0.44334\n",
            "Epoch: 4 | Iteration: 350 | Classification loss: 0.10567 | Regression loss: 0.37859 | Running loss: 0.44258\n",
            "Epoch: 4 | Iteration: 351 | Classification loss: 0.07643 | Regression loss: 0.18857 | Running loss: 0.44187\n",
            "Epoch: 4 | Iteration: 352 | Classification loss: 0.41191 | Regression loss: 0.71936 | Running loss: 0.44299\n",
            "Epoch: 4 | Iteration: 353 | Classification loss: 0.00032 | Regression loss: 0.05054 | Running loss: 0.44160\n",
            "Epoch: 4 | Iteration: 354 | Classification loss: 0.08099 | Regression loss: 0.39094 | Running loss: 0.44233\n",
            "Epoch: 4 | Iteration: 355 | Classification loss: 0.26616 | Regression loss: 0.62446 | Running loss: 0.44266\n",
            "Epoch: 4 | Iteration: 356 | Classification loss: 0.14796 | Regression loss: 0.47797 | Running loss: 0.44257\n",
            "Epoch: 4 | Iteration: 357 | Classification loss: 0.04555 | Regression loss: 0.10975 | Running loss: 0.44150\n",
            "Epoch: 4 | Iteration: 358 | Classification loss: 0.08176 | Regression loss: 0.31347 | Running loss: 0.43881\n",
            "Epoch: 4 | Iteration: 359 | Classification loss: 0.09743 | Regression loss: 0.23664 | Running loss: 0.43915\n",
            "Epoch: 4 | Iteration: 360 | Classification loss: 0.24458 | Regression loss: 0.24347 | Running loss: 0.43828\n",
            "Epoch: 4 | Iteration: 361 | Classification loss: 0.00092 | Regression loss: 0.01043 | Running loss: 0.43772\n",
            "Epoch: 4 | Iteration: 362 | Classification loss: 0.00043 | Regression loss: 0.03306 | Running loss: 0.43669\n",
            "Epoch: 4 | Iteration: 363 | Classification loss: 0.06326 | Regression loss: 0.16198 | Running loss: 0.43661\n",
            "Epoch: 4 | Iteration: 364 | Classification loss: 0.16101 | Regression loss: 0.33849 | Running loss: 0.43626\n",
            "Epoch: 4 | Iteration: 365 | Classification loss: 0.07649 | Regression loss: 0.14756 | Running loss: 0.43573\n",
            "Epoch: 4 | Iteration: 366 | Classification loss: 0.14119 | Regression loss: 0.28831 | Running loss: 0.43649\n",
            "Epoch: 4 | Iteration: 367 | Classification loss: 0.13165 | Regression loss: 0.35533 | Running loss: 0.43645\n",
            "Epoch: 4 | Iteration: 368 | Classification loss: 0.05949 | Regression loss: 0.18330 | Running loss: 0.43632\n",
            "Epoch: 4 | Iteration: 369 | Classification loss: 0.15594 | Regression loss: 0.36720 | Running loss: 0.43692\n",
            "Epoch: 4 | Iteration: 370 | Classification loss: 0.49212 | Regression loss: 0.60091 | Running loss: 0.43805\n",
            "Epoch: 4 | Iteration: 371 | Classification loss: 0.04929 | Regression loss: 0.19610 | Running loss: 0.43783\n",
            "Epoch: 4 | Iteration: 372 | Classification loss: 0.15021 | Regression loss: 0.23282 | Running loss: 0.43804\n",
            "Epoch: 4 | Iteration: 373 | Classification loss: 0.00013 | Regression loss: 0.03317 | Running loss: 0.43808\n",
            "Epoch: 4 | Iteration: 374 | Classification loss: 0.00409 | Regression loss: 0.02521 | Running loss: 0.43812\n",
            "Epoch: 4 | Iteration: 375 | Classification loss: 0.15902 | Regression loss: 0.38681 | Running loss: 0.43792\n",
            "Epoch: 4 | Iteration: 376 | Classification loss: 0.00258 | Regression loss: 0.04580 | Running loss: 0.43757\n",
            "Epoch: 4 | Iteration: 377 | Classification loss: 0.02784 | Regression loss: 0.13668 | Running loss: 0.43705\n",
            "Epoch: 4 | Iteration: 378 | Classification loss: 0.10960 | Regression loss: 0.17991 | Running loss: 0.43545\n",
            "Epoch: 4 | Iteration: 379 | Classification loss: 0.05240 | Regression loss: 0.19311 | Running loss: 0.43537\n",
            "Epoch: 4 | Iteration: 380 | Classification loss: 0.00012 | Regression loss: 0.01337 | Running loss: 0.43480\n",
            "Epoch: 4 | Iteration: 381 | Classification loss: 0.10046 | Regression loss: 0.24971 | Running loss: 0.43428\n",
            "Epoch: 4 | Iteration: 382 | Classification loss: 0.08269 | Regression loss: 0.29237 | Running loss: 0.43462\n",
            "Epoch: 4 | Iteration: 383 | Classification loss: 0.02607 | Regression loss: 0.09536 | Running loss: 0.43306\n",
            "Epoch: 4 | Iteration: 384 | Classification loss: 0.16170 | Regression loss: 0.38414 | Running loss: 0.43338\n",
            "Epoch: 4 | Iteration: 385 | Classification loss: 0.12835 | Regression loss: 0.32240 | Running loss: 0.43265\n",
            "Epoch: 4 | Iteration: 386 | Classification loss: 0.06126 | Regression loss: 0.21937 | Running loss: 0.43215\n",
            "Epoch: 4 | Iteration: 387 | Classification loss: 0.12026 | Regression loss: 0.43848 | Running loss: 0.43135\n",
            "Epoch: 4 | Iteration: 388 | Classification loss: 0.31608 | Regression loss: 0.28741 | Running loss: 0.43240\n",
            "Epoch: 4 | Iteration: 389 | Classification loss: 0.00212 | Regression loss: 0.03276 | Running loss: 0.43082\n",
            "Epoch: 4 | Iteration: 390 | Classification loss: 0.14716 | Regression loss: 0.30174 | Running loss: 0.42993\n",
            "Epoch: 4 | Iteration: 391 | Classification loss: 0.00141 | Regression loss: 0.02658 | Running loss: 0.42818\n",
            "Epoch: 4 | Iteration: 392 | Classification loss: 0.10187 | Regression loss: 0.16776 | Running loss: 0.42790\n",
            "Epoch: 4 | Iteration: 393 | Classification loss: 0.17670 | Regression loss: 0.46940 | Running loss: 0.42807\n",
            "Epoch: 4 | Iteration: 394 | Classification loss: 0.35642 | Regression loss: 0.59857 | Running loss: 0.42955\n",
            "Epoch: 4 | Iteration: 395 | Classification loss: 0.25810 | Regression loss: 0.24311 | Running loss: 0.43045\n",
            "Epoch: 4 | Iteration: 396 | Classification loss: 0.02973 | Regression loss: 0.02083 | Running loss: 0.42973\n",
            "Epoch: 4 | Iteration: 397 | Classification loss: 0.10074 | Regression loss: 0.36351 | Running loss: 0.43055\n",
            "Epoch: 4 | Iteration: 398 | Classification loss: 0.30062 | Regression loss: 0.47281 | Running loss: 0.43151\n",
            "Epoch: 4 | Iteration: 399 | Classification loss: 0.11988 | Regression loss: 0.40051 | Running loss: 0.43174\n",
            "Epoch: 4 | Iteration: 400 | Classification loss: 0.15245 | Regression loss: 0.38537 | Running loss: 0.43170\n",
            "Epoch: 4 | Iteration: 401 | Classification loss: 0.07770 | Regression loss: 0.17112 | Running loss: 0.43066\n",
            "Epoch: 4 | Iteration: 402 | Classification loss: 0.17150 | Regression loss: 0.35383 | Running loss: 0.43120\n",
            "Epoch: 4 | Iteration: 403 | Classification loss: 0.00028 | Regression loss: 0.07198 | Running loss: 0.43125\n",
            "Epoch: 4 | Iteration: 404 | Classification loss: 0.18365 | Regression loss: 0.02255 | Running loss: 0.43056\n",
            "Epoch: 4 | Iteration: 405 | Classification loss: 0.49504 | Regression loss: 0.43010 | Running loss: 0.43101\n",
            "Epoch: 4 | Iteration: 406 | Classification loss: 0.10062 | Regression loss: 0.30322 | Running loss: 0.43133\n",
            "Epoch: 4 | Iteration: 407 | Classification loss: 0.00029 | Regression loss: 0.04284 | Running loss: 0.43135\n",
            "Epoch: 4 | Iteration: 408 | Classification loss: 0.23903 | Regression loss: 0.55933 | Running loss: 0.43213\n",
            "Epoch: 4 | Iteration: 409 | Classification loss: 0.00822 | Regression loss: 0.00000 | Running loss: 0.43119\n",
            "Epoch: 4 | Iteration: 410 | Classification loss: 0.13327 | Regression loss: 0.35800 | Running loss: 0.43038\n",
            "Epoch: 4 | Iteration: 411 | Classification loss: 0.07637 | Regression loss: 0.36110 | Running loss: 0.43054\n",
            "Epoch: 4 | Iteration: 412 | Classification loss: 0.04599 | Regression loss: 0.21957 | Running loss: 0.43057\n",
            "Epoch: 4 | Iteration: 413 | Classification loss: 0.12857 | Regression loss: 0.23987 | Running loss: 0.43058\n",
            "Epoch: 4 | Iteration: 414 | Classification loss: 0.10390 | Regression loss: 0.31682 | Running loss: 0.43138\n",
            "Epoch: 4 | Iteration: 415 | Classification loss: 0.07702 | Regression loss: 0.29127 | Running loss: 0.42993\n",
            "Epoch: 4 | Iteration: 416 | Classification loss: 0.40922 | Regression loss: 0.57006 | Running loss: 0.43182\n",
            "Epoch: 4 | Iteration: 417 | Classification loss: 0.10565 | Regression loss: 0.13692 | Running loss: 0.43228\n",
            "Epoch: 4 | Iteration: 418 | Classification loss: 0.15611 | Regression loss: 0.46256 | Running loss: 0.43131\n",
            "Epoch: 4 | Iteration: 419 | Classification loss: 0.00039 | Regression loss: 0.01892 | Running loss: 0.43086\n",
            "Epoch: 4 | Iteration: 420 | Classification loss: 0.03778 | Regression loss: 0.13246 | Running loss: 0.43007\n",
            "Epoch: 4 | Iteration: 421 | Classification loss: 0.17325 | Regression loss: 0.35388 | Running loss: 0.43052\n",
            "Epoch: 4 | Iteration: 422 | Classification loss: 0.05338 | Regression loss: 0.20508 | Running loss: 0.42995\n",
            "Epoch: 4 | Iteration: 423 | Classification loss: 0.00067 | Regression loss: 0.01367 | Running loss: 0.42983\n",
            "Epoch: 4 | Iteration: 424 | Classification loss: 0.25592 | Regression loss: 0.51206 | Running loss: 0.43041\n",
            "Epoch: 4 | Iteration: 425 | Classification loss: 0.21242 | Regression loss: 0.36459 | Running loss: 0.43104\n",
            "Epoch: 4 | Iteration: 426 | Classification loss: 0.16305 | Regression loss: 0.34452 | Running loss: 0.43131\n",
            "Epoch: 4 | Iteration: 427 | Classification loss: 0.09698 | Regression loss: 0.20253 | Running loss: 0.43191\n",
            "Epoch: 4 | Iteration: 428 | Classification loss: 0.00054 | Regression loss: 0.01835 | Running loss: 0.43004\n",
            "Epoch: 4 | Iteration: 429 | Classification loss: 0.20284 | Regression loss: 0.57942 | Running loss: 0.43040\n",
            "Epoch: 4 | Iteration: 430 | Classification loss: 0.18145 | Regression loss: 0.49792 | Running loss: 0.43162\n",
            "Epoch: 4 | Iteration: 431 | Classification loss: 0.19267 | Regression loss: 0.48272 | Running loss: 0.43265\n",
            "Epoch: 4 | Iteration: 432 | Classification loss: 0.05742 | Regression loss: 0.16441 | Running loss: 0.43271\n",
            "Epoch: 4 | Iteration: 433 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.43210\n",
            "Epoch: 4 | Iteration: 434 | Classification loss: 0.27549 | Regression loss: 0.48420 | Running loss: 0.43271\n",
            "Epoch: 4 | Iteration: 435 | Classification loss: 0.08722 | Regression loss: 0.19656 | Running loss: 0.43321\n",
            "Epoch: 4 | Iteration: 436 | Classification loss: 0.00011 | Regression loss: 0.00000 | Running loss: 0.43315\n",
            "Epoch: 4 | Iteration: 437 | Classification loss: 0.07211 | Regression loss: 0.22088 | Running loss: 0.43283\n",
            "Epoch: 4 | Iteration: 438 | Classification loss: 0.00700 | Regression loss: 0.01373 | Running loss: 0.43165\n",
            "Epoch: 4 | Iteration: 439 | Classification loss: 0.05924 | Regression loss: 0.11169 | Running loss: 0.43197\n",
            "Epoch: 4 | Iteration: 440 | Classification loss: 0.07582 | Regression loss: 0.01041 | Running loss: 0.43097\n",
            "Epoch: 4 | Iteration: 441 | Classification loss: 0.05276 | Regression loss: 0.16334 | Running loss: 0.43135\n",
            "Epoch: 4 | Iteration: 442 | Classification loss: 0.16380 | Regression loss: 0.37498 | Running loss: 0.43073\n",
            "Epoch: 4 | Iteration: 443 | Classification loss: 0.04879 | Regression loss: 0.14772 | Running loss: 0.42983\n",
            "Epoch: 4 | Iteration: 444 | Classification loss: 0.31840 | Regression loss: 0.50745 | Running loss: 0.42961\n",
            "Epoch: 4 | Iteration: 445 | Classification loss: 0.11020 | Regression loss: 0.35286 | Running loss: 0.42920\n",
            "Epoch: 4 | Iteration: 446 | Classification loss: 0.13387 | Regression loss: 0.39515 | Running loss: 0.42961\n",
            "Epoch: 4 | Iteration: 447 | Classification loss: 0.21968 | Regression loss: 0.50686 | Running loss: 0.42994\n",
            "Epoch: 4 | Iteration: 448 | Classification loss: 0.12019 | Regression loss: 0.35727 | Running loss: 0.42991\n",
            "Epoch: 4 | Iteration: 449 | Classification loss: 0.00028 | Regression loss: 0.04790 | Running loss: 0.42911\n",
            "Epoch: 4 | Iteration: 450 | Classification loss: 0.00166 | Regression loss: 0.04355 | Running loss: 0.42842\n",
            "Epoch: 4 | Iteration: 451 | Classification loss: 0.16980 | Regression loss: 0.34173 | Running loss: 0.42813\n",
            "Epoch: 4 | Iteration: 452 | Classification loss: 0.23594 | Regression loss: 0.32139 | Running loss: 0.42890\n",
            "Epoch: 4 | Iteration: 453 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.42753\n",
            "Epoch: 4 | Iteration: 454 | Classification loss: 0.07500 | Regression loss: 0.19212 | Running loss: 0.42805\n",
            "Epoch: 4 | Iteration: 455 | Classification loss: 0.08655 | Regression loss: 0.33864 | Running loss: 0.42774\n",
            "Epoch: 4 | Iteration: 456 | Classification loss: 0.07165 | Regression loss: 0.33094 | Running loss: 0.42847\n",
            "Epoch: 4 | Iteration: 457 | Classification loss: 0.36653 | Regression loss: 0.58109 | Running loss: 0.42899\n",
            "Epoch: 4 | Iteration: 458 | Classification loss: 0.14221 | Regression loss: 0.22330 | Running loss: 0.42906\n",
            "Epoch: 4 | Iteration: 459 | Classification loss: 0.04670 | Regression loss: 0.30434 | Running loss: 0.42858\n",
            "Epoch: 4 | Iteration: 460 | Classification loss: 0.05209 | Regression loss: 0.28674 | Running loss: 0.42882\n",
            "Epoch: 4 | Iteration: 461 | Classification loss: 0.15790 | Regression loss: 0.26197 | Running loss: 0.42765\n",
            "Epoch: 4 | Iteration: 462 | Classification loss: 0.07032 | Regression loss: 0.20724 | Running loss: 0.42635\n",
            "Epoch: 4 | Iteration: 463 | Classification loss: 0.09171 | Regression loss: 0.26532 | Running loss: 0.42560\n",
            "Epoch: 4 | Iteration: 464 | Classification loss: 0.03484 | Regression loss: 0.17772 | Running loss: 0.42491\n",
            "Epoch: 4 | Iteration: 465 | Classification loss: 0.06838 | Regression loss: 0.26142 | Running loss: 0.42472\n",
            "Epoch: 4 | Iteration: 466 | Classification loss: 0.31881 | Regression loss: 0.46371 | Running loss: 0.42489\n",
            "Epoch: 4 | Iteration: 467 | Classification loss: 0.20068 | Regression loss: 0.37839 | Running loss: 0.42409\n",
            "Epoch: 4 | Iteration: 468 | Classification loss: 0.18175 | Regression loss: 0.43545 | Running loss: 0.42428\n",
            "Epoch: 4 | Iteration: 469 | Classification loss: 0.10061 | Regression loss: 0.34523 | Running loss: 0.42429\n",
            "Epoch: 4 | Iteration: 470 | Classification loss: 0.06203 | Regression loss: 0.20188 | Running loss: 0.42416\n",
            "Epoch: 4 | Iteration: 471 | Classification loss: 0.07477 | Regression loss: 0.24255 | Running loss: 0.42336\n",
            "Epoch: 4 | Iteration: 472 | Classification loss: 0.04521 | Regression loss: 0.16642 | Running loss: 0.42251\n",
            "Epoch: 4 | Iteration: 473 | Classification loss: 0.07398 | Regression loss: 0.28647 | Running loss: 0.42323\n",
            "Epoch: 4 | Iteration: 474 | Classification loss: 0.01058 | Regression loss: 0.00978 | Running loss: 0.42178\n",
            "Epoch: 4 | Iteration: 475 | Classification loss: 0.20469 | Regression loss: 0.48874 | Running loss: 0.42207\n",
            "Epoch: 4 | Iteration: 476 | Classification loss: 0.13277 | Regression loss: 0.25679 | Running loss: 0.42214\n",
            "Epoch: 4 | Iteration: 477 | Classification loss: 0.19483 | Regression loss: 0.20657 | Running loss: 0.42153\n",
            "Epoch: 4 | Iteration: 478 | Classification loss: 0.00335 | Regression loss: 0.02143 | Running loss: 0.42081\n",
            "Epoch: 4 | Iteration: 479 | Classification loss: 0.21602 | Regression loss: 0.49048 | Running loss: 0.42221\n",
            "Epoch: 4 | Iteration: 480 | Classification loss: 0.06551 | Regression loss: 0.19405 | Running loss: 0.42167\n",
            "Epoch: 4 | Iteration: 481 | Classification loss: 0.00016 | Regression loss: 0.03489 | Running loss: 0.42120\n",
            "Epoch: 4 | Iteration: 482 | Classification loss: 0.08671 | Regression loss: 0.25507 | Running loss: 0.42144\n",
            "Epoch: 4 | Iteration: 483 | Classification loss: 0.16465 | Regression loss: 0.35034 | Running loss: 0.42144\n",
            "Epoch: 4 | Iteration: 484 | Classification loss: 0.33315 | Regression loss: 0.47703 | Running loss: 0.42298\n",
            "Epoch: 4 | Iteration: 485 | Classification loss: 0.22373 | Regression loss: 0.50912 | Running loss: 0.42438\n",
            "Epoch: 4 | Iteration: 486 | Classification loss: 0.17773 | Regression loss: 0.23378 | Running loss: 0.42386\n",
            "Epoch: 4 | Iteration: 487 | Classification loss: 0.04522 | Regression loss: 0.18320 | Running loss: 0.42227\n",
            "Epoch: 4 | Iteration: 488 | Classification loss: 0.05602 | Regression loss: 0.16453 | Running loss: 0.42153\n",
            "Epoch: 4 | Iteration: 489 | Classification loss: 0.07314 | Regression loss: 0.31192 | Running loss: 0.42060\n",
            "Epoch: 4 | Iteration: 490 | Classification loss: 0.25413 | Regression loss: 0.58833 | Running loss: 0.42215\n",
            "Epoch: 4 | Iteration: 491 | Classification loss: 0.18012 | Regression loss: 0.48756 | Running loss: 0.42327\n",
            "Epoch: 4 | Iteration: 492 | Classification loss: 0.20193 | Regression loss: 0.35511 | Running loss: 0.42371\n",
            "Epoch: 4 | Iteration: 493 | Classification loss: 0.10055 | Regression loss: 0.18721 | Running loss: 0.42311\n",
            "Epoch: 4 | Iteration: 494 | Classification loss: 0.26583 | Regression loss: 0.53544 | Running loss: 0.42310\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.18s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.59s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.834\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.502\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.584\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.600\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.570\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 5 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.42220\n",
            "Epoch: 5 | Iteration: 1 | Classification loss: 0.10969 | Regression loss: 0.34296 | Running loss: 0.42248\n",
            "Epoch: 5 | Iteration: 2 | Classification loss: 0.12356 | Regression loss: 0.32184 | Running loss: 0.42237\n",
            "Epoch: 5 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.42176\n",
            "Epoch: 5 | Iteration: 4 | Classification loss: 0.08035 | Regression loss: 0.18964 | Running loss: 0.41998\n",
            "Epoch: 5 | Iteration: 5 | Classification loss: 0.11244 | Regression loss: 0.34448 | Running loss: 0.42059\n",
            "Epoch: 5 | Iteration: 6 | Classification loss: 0.15054 | Regression loss: 0.31564 | Running loss: 0.42140\n",
            "Epoch: 5 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.42062\n",
            "Epoch: 5 | Iteration: 8 | Classification loss: 0.06801 | Regression loss: 0.19160 | Running loss: 0.41942\n",
            "Epoch: 5 | Iteration: 9 | Classification loss: 0.32111 | Regression loss: 0.59213 | Running loss: 0.42122\n",
            "Epoch: 5 | Iteration: 10 | Classification loss: 0.16297 | Regression loss: 0.44655 | Running loss: 0.41999\n",
            "Epoch: 5 | Iteration: 11 | Classification loss: 0.06643 | Regression loss: 0.23628 | Running loss: 0.41995\n",
            "Epoch: 5 | Iteration: 12 | Classification loss: 0.07841 | Regression loss: 0.30770 | Running loss: 0.41869\n",
            "Epoch: 5 | Iteration: 13 | Classification loss: 0.21931 | Regression loss: 0.58815 | Running loss: 0.41934\n",
            "Epoch: 5 | Iteration: 14 | Classification loss: 0.00066 | Regression loss: 0.02870 | Running loss: 0.41806\n",
            "Epoch: 5 | Iteration: 15 | Classification loss: 0.09861 | Regression loss: 0.31021 | Running loss: 0.41767\n",
            "Epoch: 5 | Iteration: 16 | Classification loss: 0.11912 | Regression loss: 0.37845 | Running loss: 0.41839\n",
            "Epoch: 5 | Iteration: 17 | Classification loss: 0.00019 | Regression loss: 0.03209 | Running loss: 0.41741\n",
            "Epoch: 5 | Iteration: 18 | Classification loss: 0.11908 | Regression loss: 0.49839 | Running loss: 0.41722\n",
            "Epoch: 5 | Iteration: 19 | Classification loss: 0.52889 | Regression loss: 0.39174 | Running loss: 0.41828\n",
            "Epoch: 5 | Iteration: 20 | Classification loss: 0.00024 | Regression loss: 0.01535 | Running loss: 0.41704\n",
            "Epoch: 5 | Iteration: 21 | Classification loss: 0.08842 | Regression loss: 0.21347 | Running loss: 0.41759\n",
            "Epoch: 5 | Iteration: 22 | Classification loss: 0.32603 | Regression loss: 0.53192 | Running loss: 0.41859\n",
            "Epoch: 5 | Iteration: 23 | Classification loss: 0.14725 | Regression loss: 0.25169 | Running loss: 0.41854\n",
            "Epoch: 5 | Iteration: 24 | Classification loss: 0.01960 | Regression loss: 0.01629 | Running loss: 0.41709\n",
            "Epoch: 5 | Iteration: 25 | Classification loss: 0.10602 | Regression loss: 0.43004 | Running loss: 0.41725\n",
            "Epoch: 5 | Iteration: 26 | Classification loss: 0.11701 | Regression loss: 0.39654 | Running loss: 0.41809\n",
            "Epoch: 5 | Iteration: 27 | Classification loss: 0.03687 | Regression loss: 0.13970 | Running loss: 0.41736\n",
            "Epoch: 5 | Iteration: 28 | Classification loss: 0.05755 | Regression loss: 0.26417 | Running loss: 0.41692\n",
            "Epoch: 5 | Iteration: 29 | Classification loss: 0.00018 | Regression loss: 0.05850 | Running loss: 0.41595\n",
            "Epoch: 5 | Iteration: 30 | Classification loss: 0.04597 | Regression loss: 0.13869 | Running loss: 0.41405\n",
            "Epoch: 5 | Iteration: 31 | Classification loss: 0.03228 | Regression loss: 0.15618 | Running loss: 0.41435\n",
            "Epoch: 5 | Iteration: 32 | Classification loss: 0.00030 | Regression loss: 0.02901 | Running loss: 0.41369\n",
            "Epoch: 5 | Iteration: 33 | Classification loss: 0.02951 | Regression loss: 0.13985 | Running loss: 0.41399\n",
            "Epoch: 5 | Iteration: 34 | Classification loss: 0.00020 | Regression loss: 0.01374 | Running loss: 0.41308\n",
            "Epoch: 5 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.41220\n",
            "Epoch: 5 | Iteration: 36 | Classification loss: 0.07262 | Regression loss: 0.34589 | Running loss: 0.41185\n",
            "Epoch: 5 | Iteration: 37 | Classification loss: 0.05840 | Regression loss: 0.26693 | Running loss: 0.41113\n",
            "Epoch: 5 | Iteration: 38 | Classification loss: 0.02949 | Regression loss: 0.14902 | Running loss: 0.41103\n",
            "Epoch: 5 | Iteration: 39 | Classification loss: 0.11383 | Regression loss: 0.40488 | Running loss: 0.41207\n",
            "Epoch: 5 | Iteration: 40 | Classification loss: 0.11889 | Regression loss: 0.31254 | Running loss: 0.41293\n",
            "Epoch: 5 | Iteration: 41 | Classification loss: 0.02119 | Regression loss: 0.14277 | Running loss: 0.41277\n",
            "Epoch: 5 | Iteration: 42 | Classification loss: 0.13130 | Regression loss: 0.33243 | Running loss: 0.41265\n",
            "Epoch: 5 | Iteration: 43 | Classification loss: 0.14319 | Regression loss: 0.37034 | Running loss: 0.41364\n",
            "Epoch: 5 | Iteration: 44 | Classification loss: 0.07748 | Regression loss: 0.23049 | Running loss: 0.41301\n",
            "Epoch: 5 | Iteration: 45 | Classification loss: 0.09084 | Regression loss: 0.37198 | Running loss: 0.41163\n",
            "Epoch: 5 | Iteration: 46 | Classification loss: 0.00016 | Regression loss: 0.02764 | Running loss: 0.41110\n",
            "Epoch: 5 | Iteration: 47 | Classification loss: 0.00019 | Regression loss: 0.02866 | Running loss: 0.40965\n",
            "Epoch: 5 | Iteration: 48 | Classification loss: 0.06898 | Regression loss: 0.32749 | Running loss: 0.41006\n",
            "Epoch: 5 | Iteration: 49 | Classification loss: 0.00024 | Regression loss: 0.01434 | Running loss: 0.40966\n",
            "Epoch: 5 | Iteration: 50 | Classification loss: 0.30333 | Regression loss: 0.34799 | Running loss: 0.40992\n",
            "Epoch: 5 | Iteration: 51 | Classification loss: 0.31396 | Regression loss: 0.23942 | Running loss: 0.40944\n",
            "Epoch: 5 | Iteration: 52 | Classification loss: 0.08972 | Regression loss: 0.35597 | Running loss: 0.40969\n",
            "Epoch: 5 | Iteration: 53 | Classification loss: 0.27865 | Regression loss: 0.57445 | Running loss: 0.41046\n",
            "Epoch: 5 | Iteration: 54 | Classification loss: 0.04049 | Regression loss: 0.22841 | Running loss: 0.40933\n",
            "Epoch: 5 | Iteration: 55 | Classification loss: 0.12044 | Regression loss: 0.37254 | Running loss: 0.40964\n",
            "Epoch: 5 | Iteration: 56 | Classification loss: 0.08286 | Regression loss: 0.28944 | Running loss: 0.40988\n",
            "Epoch: 5 | Iteration: 57 | Classification loss: 0.15733 | Regression loss: 0.44168 | Running loss: 0.40967\n",
            "Epoch: 5 | Iteration: 58 | Classification loss: 0.06079 | Regression loss: 0.18705 | Running loss: 0.40938\n",
            "Epoch: 5 | Iteration: 59 | Classification loss: 0.03605 | Regression loss: 0.14898 | Running loss: 0.40966\n",
            "Epoch: 5 | Iteration: 60 | Classification loss: 0.04712 | Regression loss: 0.19517 | Running loss: 0.41014\n",
            "Epoch: 5 | Iteration: 61 | Classification loss: 0.06301 | Regression loss: 0.25624 | Running loss: 0.41068\n",
            "Epoch: 5 | Iteration: 62 | Classification loss: 0.12929 | Regression loss: 0.39993 | Running loss: 0.41034\n",
            "Epoch: 5 | Iteration: 63 | Classification loss: 0.22568 | Regression loss: 0.55714 | Running loss: 0.41181\n",
            "Epoch: 5 | Iteration: 64 | Classification loss: 0.06194 | Regression loss: 0.31281 | Running loss: 0.41249\n",
            "Epoch: 5 | Iteration: 65 | Classification loss: 0.09517 | Regression loss: 0.20676 | Running loss: 0.41277\n",
            "Epoch: 5 | Iteration: 66 | Classification loss: 0.08337 | Regression loss: 0.25564 | Running loss: 0.41250\n",
            "Epoch: 5 | Iteration: 67 | Classification loss: 0.12366 | Regression loss: 0.32409 | Running loss: 0.41204\n",
            "Epoch: 5 | Iteration: 68 | Classification loss: 0.10056 | Regression loss: 0.31250 | Running loss: 0.41212\n",
            "Epoch: 5 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.41051\n",
            "Epoch: 5 | Iteration: 70 | Classification loss: 0.00023 | Regression loss: 0.03065 | Running loss: 0.40924\n",
            "Epoch: 5 | Iteration: 71 | Classification loss: 0.00033 | Regression loss: 0.02181 | Running loss: 0.40856\n",
            "Epoch: 5 | Iteration: 72 | Classification loss: 0.00025 | Regression loss: 0.03743 | Running loss: 0.40850\n",
            "Epoch: 5 | Iteration: 73 | Classification loss: 0.34230 | Regression loss: 0.65161 | Running loss: 0.40914\n",
            "Epoch: 5 | Iteration: 74 | Classification loss: 0.11741 | Regression loss: 0.22775 | Running loss: 0.40964\n",
            "Epoch: 5 | Iteration: 75 | Classification loss: 0.08754 | Regression loss: 0.19921 | Running loss: 0.40913\n",
            "Epoch: 5 | Iteration: 76 | Classification loss: 0.22142 | Regression loss: 0.38171 | Running loss: 0.41023\n",
            "Epoch: 5 | Iteration: 77 | Classification loss: 0.00028 | Regression loss: 0.02236 | Running loss: 0.41019\n",
            "Epoch: 5 | Iteration: 78 | Classification loss: 0.11390 | Regression loss: 0.43494 | Running loss: 0.41096\n",
            "Epoch: 5 | Iteration: 79 | Classification loss: 0.00030 | Regression loss: 0.02208 | Running loss: 0.40971\n",
            "Epoch: 5 | Iteration: 80 | Classification loss: 0.01669 | Regression loss: 0.00875 | Running loss: 0.40955\n",
            "Epoch: 5 | Iteration: 81 | Classification loss: 0.02995 | Regression loss: 0.13925 | Running loss: 0.40900\n",
            "Epoch: 5 | Iteration: 82 | Classification loss: 0.00010 | Regression loss: 0.01593 | Running loss: 0.40885\n",
            "Epoch: 5 | Iteration: 83 | Classification loss: 0.03293 | Regression loss: 0.13881 | Running loss: 0.40812\n",
            "Epoch: 5 | Iteration: 84 | Classification loss: 0.19587 | Regression loss: 0.38229 | Running loss: 0.40795\n",
            "Epoch: 5 | Iteration: 85 | Classification loss: 0.00067 | Regression loss: 0.02434 | Running loss: 0.40709\n",
            "Epoch: 5 | Iteration: 86 | Classification loss: 0.02455 | Regression loss: 0.12054 | Running loss: 0.40684\n",
            "Epoch: 5 | Iteration: 87 | Classification loss: 0.06884 | Regression loss: 0.29411 | Running loss: 0.40371\n",
            "Epoch: 5 | Iteration: 88 | Classification loss: 0.16114 | Regression loss: 0.25222 | Running loss: 0.40346\n",
            "Epoch: 5 | Iteration: 89 | Classification loss: 0.16509 | Regression loss: 0.38463 | Running loss: 0.40342\n",
            "Epoch: 5 | Iteration: 90 | Classification loss: 0.00025 | Regression loss: 0.00942 | Running loss: 0.40286\n",
            "Epoch: 5 | Iteration: 91 | Classification loss: 0.02095 | Regression loss: 0.11375 | Running loss: 0.40220\n",
            "Epoch: 5 | Iteration: 92 | Classification loss: 0.27183 | Regression loss: 0.58142 | Running loss: 0.40244\n",
            "Epoch: 5 | Iteration: 93 | Classification loss: 0.10208 | Regression loss: 0.31870 | Running loss: 0.40283\n",
            "Epoch: 5 | Iteration: 94 | Classification loss: 0.04324 | Regression loss: 0.12814 | Running loss: 0.40153\n",
            "Epoch: 5 | Iteration: 95 | Classification loss: 0.01022 | Regression loss: 0.00956 | Running loss: 0.40045\n",
            "Epoch: 5 | Iteration: 96 | Classification loss: 0.24000 | Regression loss: 0.24206 | Running loss: 0.40048\n",
            "Epoch: 5 | Iteration: 97 | Classification loss: 0.11757 | Regression loss: 0.29375 | Running loss: 0.40092\n",
            "Epoch: 5 | Iteration: 98 | Classification loss: 0.05058 | Regression loss: 0.17170 | Running loss: 0.40073\n",
            "Epoch: 5 | Iteration: 99 | Classification loss: 0.03579 | Regression loss: 0.14799 | Running loss: 0.39976\n",
            "Epoch: 5 | Iteration: 100 | Classification loss: 0.23846 | Regression loss: 0.41882 | Running loss: 0.39940\n",
            "Epoch: 5 | Iteration: 101 | Classification loss: 0.07933 | Regression loss: 0.33017 | Running loss: 0.39868\n",
            "Epoch: 5 | Iteration: 102 | Classification loss: 0.04221 | Regression loss: 0.21616 | Running loss: 0.39870\n",
            "Epoch: 5 | Iteration: 103 | Classification loss: 0.00076 | Regression loss: 0.01213 | Running loss: 0.39821\n",
            "Epoch: 5 | Iteration: 104 | Classification loss: 0.10733 | Regression loss: 0.28036 | Running loss: 0.39878\n",
            "Epoch: 5 | Iteration: 105 | Classification loss: 0.09168 | Regression loss: 0.32052 | Running loss: 0.39863\n",
            "Epoch: 5 | Iteration: 106 | Classification loss: 0.07248 | Regression loss: 0.13013 | Running loss: 0.39776\n",
            "Epoch: 5 | Iteration: 107 | Classification loss: 0.07523 | Regression loss: 0.32197 | Running loss: 0.39763\n",
            "Epoch: 5 | Iteration: 108 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.39656\n",
            "Epoch: 5 | Iteration: 109 | Classification loss: 0.16806 | Regression loss: 0.24676 | Running loss: 0.39673\n",
            "Epoch: 5 | Iteration: 110 | Classification loss: 0.11702 | Regression loss: 0.41194 | Running loss: 0.39702\n",
            "Epoch: 5 | Iteration: 111 | Classification loss: 0.02104 | Regression loss: 0.14360 | Running loss: 0.39595\n",
            "Epoch: 5 | Iteration: 112 | Classification loss: 0.10912 | Regression loss: 0.31663 | Running loss: 0.39671\n",
            "Epoch: 5 | Iteration: 113 | Classification loss: 0.17488 | Regression loss: 0.48269 | Running loss: 0.39714\n",
            "Epoch: 5 | Iteration: 114 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.39686\n",
            "Epoch: 5 | Iteration: 115 | Classification loss: 0.09685 | Regression loss: 0.29021 | Running loss: 0.39702\n",
            "Epoch: 5 | Iteration: 116 | Classification loss: 0.20669 | Regression loss: 0.32766 | Running loss: 0.39730\n",
            "Epoch: 5 | Iteration: 117 | Classification loss: 0.02008 | Regression loss: 0.11106 | Running loss: 0.39584\n",
            "Epoch: 5 | Iteration: 118 | Classification loss: 0.04858 | Regression loss: 0.19092 | Running loss: 0.39524\n",
            "Epoch: 5 | Iteration: 119 | Classification loss: 0.00009 | Regression loss: 0.00801 | Running loss: 0.39485\n",
            "Epoch: 5 | Iteration: 120 | Classification loss: 0.07561 | Regression loss: 0.21584 | Running loss: 0.39400\n",
            "Epoch: 5 | Iteration: 121 | Classification loss: 0.21283 | Regression loss: 0.56959 | Running loss: 0.39516\n",
            "Epoch: 5 | Iteration: 122 | Classification loss: 0.10208 | Regression loss: 0.34836 | Running loss: 0.39497\n",
            "Epoch: 5 | Iteration: 123 | Classification loss: 0.00014 | Regression loss: 0.01964 | Running loss: 0.39425\n",
            "Epoch: 5 | Iteration: 124 | Classification loss: 0.10055 | Regression loss: 0.31872 | Running loss: 0.39405\n",
            "Epoch: 5 | Iteration: 125 | Classification loss: 0.09544 | Regression loss: 0.35192 | Running loss: 0.39480\n",
            "Epoch: 5 | Iteration: 126 | Classification loss: 0.00012 | Regression loss: 0.01098 | Running loss: 0.39399\n",
            "Epoch: 5 | Iteration: 127 | Classification loss: 0.08329 | Regression loss: 0.37040 | Running loss: 0.39435\n",
            "Epoch: 5 | Iteration: 128 | Classification loss: 0.04054 | Regression loss: 0.14178 | Running loss: 0.39427\n",
            "Epoch: 5 | Iteration: 129 | Classification loss: 0.16570 | Regression loss: 0.41651 | Running loss: 0.39541\n",
            "Epoch: 5 | Iteration: 130 | Classification loss: 0.09461 | Regression loss: 0.32876 | Running loss: 0.39496\n",
            "Epoch: 5 | Iteration: 131 | Classification loss: 0.00206 | Regression loss: 0.00763 | Running loss: 0.39402\n",
            "Epoch: 5 | Iteration: 132 | Classification loss: 0.12269 | Regression loss: 0.35595 | Running loss: 0.39372\n",
            "Epoch: 5 | Iteration: 133 | Classification loss: 0.07963 | Regression loss: 0.14404 | Running loss: 0.39363\n",
            "Epoch: 5 | Iteration: 134 | Classification loss: 0.04342 | Regression loss: 0.15699 | Running loss: 0.39289\n",
            "Epoch: 5 | Iteration: 135 | Classification loss: 0.07406 | Regression loss: 0.26801 | Running loss: 0.39266\n",
            "Epoch: 5 | Iteration: 136 | Classification loss: 0.20029 | Regression loss: 0.43717 | Running loss: 0.39286\n",
            "Epoch: 5 | Iteration: 137 | Classification loss: 0.03389 | Regression loss: 0.11770 | Running loss: 0.39203\n",
            "Epoch: 5 | Iteration: 138 | Classification loss: 0.04830 | Regression loss: 0.20607 | Running loss: 0.39221\n",
            "Epoch: 5 | Iteration: 139 | Classification loss: 0.11664 | Regression loss: 0.36999 | Running loss: 0.39212\n",
            "Epoch: 5 | Iteration: 140 | Classification loss: 0.04218 | Regression loss: 0.23248 | Running loss: 0.39159\n",
            "Epoch: 5 | Iteration: 141 | Classification loss: 0.18379 | Regression loss: 0.53923 | Running loss: 0.39188\n",
            "Epoch: 5 | Iteration: 142 | Classification loss: 0.08046 | Regression loss: 0.32951 | Running loss: 0.39257\n",
            "Epoch: 5 | Iteration: 143 | Classification loss: 0.00033 | Regression loss: 0.00000 | Running loss: 0.39097\n",
            "Epoch: 5 | Iteration: 144 | Classification loss: 0.04908 | Regression loss: 0.27702 | Running loss: 0.39044\n",
            "Epoch: 5 | Iteration: 145 | Classification loss: 0.02006 | Regression loss: 0.16390 | Running loss: 0.38934\n",
            "Epoch: 5 | Iteration: 146 | Classification loss: 0.21908 | Regression loss: 0.43848 | Running loss: 0.38946\n",
            "Epoch: 5 | Iteration: 147 | Classification loss: 0.17686 | Regression loss: 0.16485 | Running loss: 0.38881\n",
            "Epoch: 5 | Iteration: 148 | Classification loss: 0.19279 | Regression loss: 0.40568 | Running loss: 0.38907\n",
            "Epoch: 5 | Iteration: 149 | Classification loss: 0.27851 | Regression loss: 0.59034 | Running loss: 0.38963\n",
            "Epoch: 5 | Iteration: 150 | Classification loss: 0.06384 | Regression loss: 0.33081 | Running loss: 0.38912\n",
            "Epoch: 5 | Iteration: 151 | Classification loss: 0.15012 | Regression loss: 0.37839 | Running loss: 0.38912\n",
            "Epoch: 5 | Iteration: 152 | Classification loss: 0.09192 | Regression loss: 0.29483 | Running loss: 0.38815\n",
            "Epoch: 5 | Iteration: 153 | Classification loss: 0.00065 | Regression loss: 0.02680 | Running loss: 0.38691\n",
            "Epoch: 5 | Iteration: 154 | Classification loss: 0.26583 | Regression loss: 0.42235 | Running loss: 0.38807\n",
            "Epoch: 5 | Iteration: 155 | Classification loss: 0.21880 | Regression loss: 0.54703 | Running loss: 0.38960\n",
            "Epoch: 5 | Iteration: 156 | Classification loss: 0.03699 | Regression loss: 0.12005 | Running loss: 0.38987\n",
            "Epoch: 5 | Iteration: 157 | Classification loss: 0.00013 | Regression loss: 0.03762 | Running loss: 0.38913\n",
            "Epoch: 5 | Iteration: 158 | Classification loss: 0.11943 | Regression loss: 0.41311 | Running loss: 0.38920\n",
            "Epoch: 5 | Iteration: 159 | Classification loss: 0.13142 | Regression loss: 0.29539 | Running loss: 0.38886\n",
            "Epoch: 5 | Iteration: 160 | Classification loss: 0.24101 | Regression loss: 0.46272 | Running loss: 0.38889\n",
            "Epoch: 5 | Iteration: 161 | Classification loss: 0.00015 | Regression loss: 0.00767 | Running loss: 0.38828\n",
            "Epoch: 5 | Iteration: 162 | Classification loss: 0.21879 | Regression loss: 0.23853 | Running loss: 0.38807\n",
            "Epoch: 5 | Iteration: 163 | Classification loss: 0.15391 | Regression loss: 0.44689 | Running loss: 0.38826\n",
            "Epoch: 5 | Iteration: 164 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.38634\n",
            "Epoch: 5 | Iteration: 165 | Classification loss: 0.20962 | Regression loss: 0.41569 | Running loss: 0.38734\n",
            "Epoch: 5 | Iteration: 166 | Classification loss: 0.09230 | Regression loss: 0.39629 | Running loss: 0.38743\n",
            "Epoch: 5 | Iteration: 167 | Classification loss: 0.11032 | Regression loss: 0.42488 | Running loss: 0.38845\n",
            "Epoch: 5 | Iteration: 168 | Classification loss: 0.18216 | Regression loss: 0.47796 | Running loss: 0.38977\n",
            "Epoch: 5 | Iteration: 169 | Classification loss: 0.08589 | Regression loss: 0.32512 | Running loss: 0.38883\n",
            "Epoch: 5 | Iteration: 170 | Classification loss: 0.20617 | Regression loss: 0.52935 | Running loss: 0.38923\n",
            "Epoch: 5 | Iteration: 171 | Classification loss: 0.14980 | Regression loss: 0.28010 | Running loss: 0.38929\n",
            "Epoch: 5 | Iteration: 172 | Classification loss: 0.30368 | Regression loss: 0.61713 | Running loss: 0.39016\n",
            "Epoch: 5 | Iteration: 173 | Classification loss: 0.00160 | Regression loss: 0.05813 | Running loss: 0.38760\n",
            "Epoch: 5 | Iteration: 174 | Classification loss: 0.13030 | Regression loss: 0.36900 | Running loss: 0.38624\n",
            "Epoch: 5 | Iteration: 175 | Classification loss: 0.05593 | Regression loss: 0.27022 | Running loss: 0.38627\n",
            "Epoch: 5 | Iteration: 176 | Classification loss: 0.00013 | Regression loss: 0.07089 | Running loss: 0.38460\n",
            "Epoch: 5 | Iteration: 177 | Classification loss: 0.23274 | Regression loss: 0.41316 | Running loss: 0.38540\n",
            "Epoch: 5 | Iteration: 178 | Classification loss: 0.05370 | Regression loss: 0.18076 | Running loss: 0.38585\n",
            "Epoch: 5 | Iteration: 179 | Classification loss: 0.23315 | Regression loss: 0.85298 | Running loss: 0.38706\n",
            "Epoch: 5 | Iteration: 180 | Classification loss: 0.00028 | Regression loss: 0.04713 | Running loss: 0.38659\n",
            "Epoch: 5 | Iteration: 181 | Classification loss: 0.09755 | Regression loss: 0.19203 | Running loss: 0.38586\n",
            "Epoch: 5 | Iteration: 182 | Classification loss: 0.24356 | Regression loss: 0.51950 | Running loss: 0.38724\n",
            "Epoch: 5 | Iteration: 183 | Classification loss: 0.26410 | Regression loss: 0.48179 | Running loss: 0.38817\n",
            "Epoch: 5 | Iteration: 184 | Classification loss: 0.06528 | Regression loss: 0.27549 | Running loss: 0.38813\n",
            "Epoch: 5 | Iteration: 185 | Classification loss: 0.11181 | Regression loss: 0.35254 | Running loss: 0.38871\n",
            "Epoch: 5 | Iteration: 186 | Classification loss: 0.03219 | Regression loss: 0.20039 | Running loss: 0.38865\n",
            "Epoch: 5 | Iteration: 187 | Classification loss: 0.12450 | Regression loss: 0.35914 | Running loss: 0.38925\n",
            "Epoch: 5 | Iteration: 188 | Classification loss: 0.25427 | Regression loss: 0.48671 | Running loss: 0.38975\n",
            "Epoch: 5 | Iteration: 189 | Classification loss: 0.18852 | Regression loss: 0.33594 | Running loss: 0.38915\n",
            "Epoch: 5 | Iteration: 190 | Classification loss: 0.00306 | Regression loss: 0.13087 | Running loss: 0.38890\n",
            "Epoch: 5 | Iteration: 191 | Classification loss: 0.08323 | Regression loss: 0.13932 | Running loss: 0.38786\n",
            "Epoch: 5 | Iteration: 192 | Classification loss: 0.00069 | Regression loss: 0.06541 | Running loss: 0.38695\n",
            "Epoch: 5 | Iteration: 193 | Classification loss: 0.28345 | Regression loss: 0.58239 | Running loss: 0.38859\n",
            "Epoch: 5 | Iteration: 194 | Classification loss: 0.18242 | Regression loss: 0.12177 | Running loss: 0.38913\n",
            "Epoch: 5 | Iteration: 195 | Classification loss: 0.06887 | Regression loss: 0.30015 | Running loss: 0.38939\n",
            "Epoch: 5 | Iteration: 196 | Classification loss: 0.18449 | Regression loss: 0.50430 | Running loss: 0.38958\n",
            "Epoch: 5 | Iteration: 197 | Classification loss: 0.09862 | Regression loss: 0.41495 | Running loss: 0.39015\n",
            "Epoch: 5 | Iteration: 198 | Classification loss: 0.07374 | Regression loss: 0.31364 | Running loss: 0.39004\n",
            "Epoch: 5 | Iteration: 199 | Classification loss: 0.34934 | Regression loss: 0.42631 | Running loss: 0.39088\n",
            "Epoch: 5 | Iteration: 200 | Classification loss: 0.08198 | Regression loss: 0.24152 | Running loss: 0.39153\n",
            "Epoch: 5 | Iteration: 201 | Classification loss: 0.02463 | Regression loss: 0.20118 | Running loss: 0.39103\n",
            "Epoch: 5 | Iteration: 202 | Classification loss: 0.07405 | Regression loss: 0.33145 | Running loss: 0.39177\n",
            "Epoch: 5 | Iteration: 203 | Classification loss: 0.11193 | Regression loss: 0.37520 | Running loss: 0.39125\n",
            "Epoch: 5 | Iteration: 204 | Classification loss: 0.15992 | Regression loss: 0.40542 | Running loss: 0.39102\n",
            "Epoch: 5 | Iteration: 205 | Classification loss: 0.18051 | Regression loss: 0.36290 | Running loss: 0.39203\n",
            "Epoch: 5 | Iteration: 206 | Classification loss: 0.02575 | Regression loss: 0.13634 | Running loss: 0.39048\n",
            "Epoch: 5 | Iteration: 207 | Classification loss: 0.00176 | Regression loss: 0.01098 | Running loss: 0.39041\n",
            "Epoch: 5 | Iteration: 208 | Classification loss: 0.05359 | Regression loss: 0.18992 | Running loss: 0.38863\n",
            "Epoch: 5 | Iteration: 209 | Classification loss: 0.13358 | Regression loss: 0.38195 | Running loss: 0.38920\n",
            "Epoch: 5 | Iteration: 210 | Classification loss: 0.07647 | Regression loss: 0.20211 | Running loss: 0.38938\n",
            "Epoch: 5 | Iteration: 211 | Classification loss: 0.00819 | Regression loss: 0.02360 | Running loss: 0.38888\n",
            "Epoch: 5 | Iteration: 212 | Classification loss: 0.03668 | Regression loss: 0.17818 | Running loss: 0.38815\n",
            "Epoch: 5 | Iteration: 213 | Classification loss: 0.07553 | Regression loss: 0.29257 | Running loss: 0.38756\n",
            "Epoch: 5 | Iteration: 214 | Classification loss: 0.00138 | Regression loss: 0.00814 | Running loss: 0.38627\n",
            "Epoch: 5 | Iteration: 215 | Classification loss: 0.17162 | Regression loss: 0.40725 | Running loss: 0.38692\n",
            "Epoch: 5 | Iteration: 216 | Classification loss: 0.07819 | Regression loss: 0.32438 | Running loss: 0.38655\n",
            "Epoch: 5 | Iteration: 217 | Classification loss: 0.12845 | Regression loss: 0.33885 | Running loss: 0.38637\n",
            "Epoch: 5 | Iteration: 218 | Classification loss: 0.10132 | Regression loss: 0.22771 | Running loss: 0.38612\n",
            "Epoch: 5 | Iteration: 219 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.38542\n",
            "Epoch: 5 | Iteration: 220 | Classification loss: 0.16237 | Regression loss: 0.53019 | Running loss: 0.38668\n",
            "Epoch: 5 | Iteration: 221 | Classification loss: 0.00019 | Regression loss: 0.02688 | Running loss: 0.38625\n",
            "Epoch: 5 | Iteration: 222 | Classification loss: 0.00017 | Regression loss: 0.01988 | Running loss: 0.38546\n",
            "Epoch: 5 | Iteration: 223 | Classification loss: 0.04397 | Regression loss: 0.16140 | Running loss: 0.38484\n",
            "Epoch: 5 | Iteration: 224 | Classification loss: 0.07798 | Regression loss: 0.30398 | Running loss: 0.38489\n",
            "Epoch: 5 | Iteration: 225 | Classification loss: 0.06501 | Regression loss: 0.17426 | Running loss: 0.38440\n",
            "Epoch: 5 | Iteration: 226 | Classification loss: 0.10728 | Regression loss: 0.25968 | Running loss: 0.38432\n",
            "Epoch: 5 | Iteration: 227 | Classification loss: 0.16162 | Regression loss: 0.44513 | Running loss: 0.38514\n",
            "Epoch: 5 | Iteration: 228 | Classification loss: 0.07149 | Regression loss: 0.27187 | Running loss: 0.38422\n",
            "Epoch: 5 | Iteration: 229 | Classification loss: 0.02859 | Regression loss: 0.17264 | Running loss: 0.38269\n",
            "Epoch: 5 | Iteration: 230 | Classification loss: 0.02450 | Regression loss: 0.16569 | Running loss: 0.38209\n",
            "Epoch: 5 | Iteration: 231 | Classification loss: 0.07337 | Regression loss: 0.27195 | Running loss: 0.38172\n",
            "Epoch: 5 | Iteration: 232 | Classification loss: 0.01025 | Regression loss: 0.03165 | Running loss: 0.38025\n",
            "Epoch: 5 | Iteration: 233 | Classification loss: 0.12978 | Regression loss: 0.31405 | Running loss: 0.38055\n",
            "Epoch: 5 | Iteration: 234 | Classification loss: 0.08520 | Regression loss: 0.30725 | Running loss: 0.37996\n",
            "Epoch: 5 | Iteration: 235 | Classification loss: 0.00028 | Regression loss: 0.02396 | Running loss: 0.37996\n",
            "Epoch: 5 | Iteration: 236 | Classification loss: 0.07402 | Regression loss: 0.38343 | Running loss: 0.37961\n",
            "Epoch: 5 | Iteration: 237 | Classification loss: 0.00029 | Regression loss: 0.02185 | Running loss: 0.37913\n",
            "Epoch: 5 | Iteration: 238 | Classification loss: 0.02716 | Regression loss: 0.15205 | Running loss: 0.37858\n",
            "Epoch: 5 | Iteration: 239 | Classification loss: 0.08006 | Regression loss: 0.21954 | Running loss: 0.37763\n",
            "Epoch: 5 | Iteration: 240 | Classification loss: 0.10387 | Regression loss: 0.27532 | Running loss: 0.37722\n",
            "Epoch: 5 | Iteration: 241 | Classification loss: 0.00039 | Regression loss: 0.07742 | Running loss: 0.37737\n",
            "Epoch: 5 | Iteration: 242 | Classification loss: 0.17614 | Regression loss: 0.38760 | Running loss: 0.37793\n",
            "Epoch: 5 | Iteration: 243 | Classification loss: 0.03591 | Regression loss: 0.20732 | Running loss: 0.37794\n",
            "Epoch: 5 | Iteration: 244 | Classification loss: 0.03869 | Regression loss: 0.16856 | Running loss: 0.37721\n",
            "Epoch: 5 | Iteration: 245 | Classification loss: 0.21283 | Regression loss: 0.43379 | Running loss: 0.37846\n",
            "Epoch: 5 | Iteration: 246 | Classification loss: 0.06667 | Regression loss: 0.18906 | Running loss: 0.37782\n",
            "Epoch: 5 | Iteration: 247 | Classification loss: 0.19592 | Regression loss: 0.39821 | Running loss: 0.37829\n",
            "Epoch: 5 | Iteration: 248 | Classification loss: 0.18225 | Regression loss: 0.42524 | Running loss: 0.37848\n",
            "Epoch: 5 | Iteration: 249 | Classification loss: 0.07287 | Regression loss: 0.23806 | Running loss: 0.37906\n",
            "Epoch: 5 | Iteration: 250 | Classification loss: 0.08610 | Regression loss: 0.36954 | Running loss: 0.37878\n",
            "Epoch: 5 | Iteration: 251 | Classification loss: 0.14804 | Regression loss: 0.47979 | Running loss: 0.37865\n",
            "Epoch: 5 | Iteration: 252 | Classification loss: 0.07473 | Regression loss: 0.31695 | Running loss: 0.37809\n",
            "Epoch: 5 | Iteration: 253 | Classification loss: 0.00014 | Regression loss: 0.02906 | Running loss: 0.37701\n",
            "Epoch: 5 | Iteration: 254 | Classification loss: 0.12483 | Regression loss: 0.19790 | Running loss: 0.37625\n",
            "Epoch: 5 | Iteration: 255 | Classification loss: 0.08667 | Regression loss: 0.25428 | Running loss: 0.37505\n",
            "Epoch: 5 | Iteration: 256 | Classification loss: 0.02868 | Regression loss: 0.12266 | Running loss: 0.37398\n",
            "Epoch: 5 | Iteration: 257 | Classification loss: 0.09017 | Regression loss: 0.23140 | Running loss: 0.37361\n",
            "Epoch: 5 | Iteration: 258 | Classification loss: 0.25192 | Regression loss: 0.52405 | Running loss: 0.37376\n",
            "Epoch: 5 | Iteration: 259 | Classification loss: 0.12337 | Regression loss: 0.30914 | Running loss: 0.37250\n",
            "Epoch: 5 | Iteration: 260 | Classification loss: 0.03720 | Regression loss: 0.17233 | Running loss: 0.37204\n",
            "Epoch: 5 | Iteration: 261 | Classification loss: 0.15686 | Regression loss: 0.44031 | Running loss: 0.37318\n",
            "Epoch: 5 | Iteration: 262 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.37197\n",
            "Epoch: 5 | Iteration: 263 | Classification loss: 0.03586 | Regression loss: 0.14748 | Running loss: 0.37129\n",
            "Epoch: 5 | Iteration: 264 | Classification loss: 0.00563 | Regression loss: 0.00943 | Running loss: 0.37101\n",
            "Epoch: 5 | Iteration: 265 | Classification loss: 0.09066 | Regression loss: 0.15824 | Running loss: 0.37061\n",
            "Epoch: 5 | Iteration: 266 | Classification loss: 0.16098 | Regression loss: 0.29380 | Running loss: 0.37089\n",
            "Epoch: 5 | Iteration: 267 | Classification loss: 0.08807 | Regression loss: 0.28840 | Running loss: 0.37030\n",
            "Epoch: 5 | Iteration: 268 | Classification loss: 0.07148 | Regression loss: 0.27459 | Running loss: 0.37012\n",
            "Epoch: 5 | Iteration: 269 | Classification loss: 0.05859 | Regression loss: 0.13030 | Running loss: 0.36914\n",
            "Epoch: 5 | Iteration: 270 | Classification loss: 0.06720 | Regression loss: 0.28942 | Running loss: 0.36913\n",
            "Epoch: 5 | Iteration: 271 | Classification loss: 0.11722 | Regression loss: 0.23705 | Running loss: 0.36891\n",
            "Epoch: 5 | Iteration: 272 | Classification loss: 0.04658 | Regression loss: 0.16536 | Running loss: 0.36850\n",
            "Epoch: 5 | Iteration: 273 | Classification loss: 0.07838 | Regression loss: 0.33811 | Running loss: 0.36879\n",
            "Epoch: 5 | Iteration: 274 | Classification loss: 0.18239 | Regression loss: 0.56533 | Running loss: 0.36874\n",
            "Epoch: 5 | Iteration: 275 | Classification loss: 0.02468 | Regression loss: 0.16663 | Running loss: 0.36876\n",
            "Epoch: 5 | Iteration: 276 | Classification loss: 0.06600 | Regression loss: 0.18283 | Running loss: 0.36863\n",
            "Epoch: 5 | Iteration: 277 | Classification loss: 0.08175 | Regression loss: 0.29218 | Running loss: 0.36929\n",
            "Epoch: 5 | Iteration: 278 | Classification loss: 0.05694 | Regression loss: 0.18302 | Running loss: 0.36932\n",
            "Epoch: 5 | Iteration: 279 | Classification loss: 0.13318 | Regression loss: 0.35537 | Running loss: 0.36990\n",
            "Epoch: 5 | Iteration: 280 | Classification loss: 0.19191 | Regression loss: 0.45067 | Running loss: 0.36873\n",
            "Epoch: 5 | Iteration: 281 | Classification loss: 0.04030 | Regression loss: 0.16417 | Running loss: 0.36828\n",
            "Epoch: 5 | Iteration: 282 | Classification loss: 0.11825 | Regression loss: 0.36308 | Running loss: 0.36848\n",
            "Epoch: 5 | Iteration: 283 | Classification loss: 0.03324 | Regression loss: 0.16296 | Running loss: 0.36792\n",
            "Epoch: 5 | Iteration: 284 | Classification loss: 0.35596 | Regression loss: 0.39298 | Running loss: 0.36864\n",
            "Epoch: 5 | Iteration: 285 | Classification loss: 0.00124 | Regression loss: 0.04655 | Running loss: 0.36762\n",
            "Epoch: 5 | Iteration: 286 | Classification loss: 0.22294 | Regression loss: 0.33356 | Running loss: 0.36814\n",
            "Epoch: 5 | Iteration: 287 | Classification loss: 0.10180 | Regression loss: 0.35570 | Running loss: 0.36902\n",
            "Epoch: 5 | Iteration: 288 | Classification loss: 0.03132 | Regression loss: 0.09890 | Running loss: 0.36840\n",
            "Epoch: 5 | Iteration: 289 | Classification loss: 0.09947 | Regression loss: 0.22368 | Running loss: 0.36905\n",
            "Epoch: 5 | Iteration: 290 | Classification loss: 0.12801 | Regression loss: 0.23942 | Running loss: 0.36834\n",
            "Epoch: 5 | Iteration: 291 | Classification loss: 0.06878 | Regression loss: 0.17089 | Running loss: 0.36830\n",
            "Epoch: 5 | Iteration: 292 | Classification loss: 0.14468 | Regression loss: 0.12133 | Running loss: 0.36706\n",
            "Epoch: 5 | Iteration: 293 | Classification loss: 0.06194 | Regression loss: 0.17220 | Running loss: 0.36704\n",
            "Epoch: 5 | Iteration: 294 | Classification loss: 0.08070 | Regression loss: 0.22842 | Running loss: 0.36720\n",
            "Epoch: 5 | Iteration: 295 | Classification loss: 0.18065 | Regression loss: 0.43208 | Running loss: 0.36802\n",
            "Epoch: 5 | Iteration: 296 | Classification loss: 0.00025 | Regression loss: 0.00000 | Running loss: 0.36756\n",
            "Epoch: 5 | Iteration: 297 | Classification loss: 0.21364 | Regression loss: 0.59070 | Running loss: 0.36912\n",
            "Epoch: 5 | Iteration: 298 | Classification loss: 0.00027 | Regression loss: 0.03515 | Running loss: 0.36914\n",
            "Epoch: 5 | Iteration: 299 | Classification loss: 0.22548 | Regression loss: 0.45898 | Running loss: 0.37000\n",
            "Epoch: 5 | Iteration: 300 | Classification loss: 0.16149 | Regression loss: 0.34363 | Running loss: 0.37024\n",
            "Epoch: 5 | Iteration: 301 | Classification loss: 0.36872 | Regression loss: 0.66903 | Running loss: 0.37143\n",
            "Epoch: 5 | Iteration: 302 | Classification loss: 0.15533 | Regression loss: 0.40697 | Running loss: 0.37187\n",
            "Epoch: 5 | Iteration: 303 | Classification loss: 0.16214 | Regression loss: 0.41610 | Running loss: 0.37218\n",
            "Epoch: 5 | Iteration: 304 | Classification loss: 0.35602 | Regression loss: 0.46603 | Running loss: 0.37202\n",
            "Epoch: 5 | Iteration: 305 | Classification loss: 0.59348 | Regression loss: 0.65326 | Running loss: 0.37429\n",
            "Epoch: 5 | Iteration: 306 | Classification loss: 0.00026 | Regression loss: 0.06951 | Running loss: 0.37443\n",
            "Epoch: 5 | Iteration: 307 | Classification loss: 0.31033 | Regression loss: 0.61551 | Running loss: 0.37468\n",
            "Epoch: 5 | Iteration: 308 | Classification loss: 0.09666 | Regression loss: 0.24016 | Running loss: 0.37535\n",
            "Epoch: 5 | Iteration: 309 | Classification loss: 1.16854 | Regression loss: 0.52243 | Running loss: 0.37827\n",
            "Epoch: 5 | Iteration: 310 | Classification loss: 0.03141 | Regression loss: 0.12757 | Running loss: 0.37815\n",
            "Epoch: 5 | Iteration: 311 | Classification loss: 0.20180 | Regression loss: 0.21726 | Running loss: 0.37748\n",
            "Epoch: 5 | Iteration: 312 | Classification loss: 0.15494 | Regression loss: 0.36756 | Running loss: 0.37777\n",
            "Epoch: 5 | Iteration: 313 | Classification loss: 0.21918 | Regression loss: 0.43198 | Running loss: 0.37720\n",
            "Epoch: 5 | Iteration: 314 | Classification loss: 0.12467 | Regression loss: 0.30519 | Running loss: 0.37724\n",
            "Epoch: 5 | Iteration: 315 | Classification loss: 0.15726 | Regression loss: 0.20049 | Running loss: 0.37708\n",
            "Epoch: 5 | Iteration: 316 | Classification loss: 0.17315 | Regression loss: 0.25776 | Running loss: 0.37666\n",
            "Epoch: 5 | Iteration: 317 | Classification loss: 0.13079 | Regression loss: 0.12180 | Running loss: 0.37601\n",
            "Epoch: 5 | Iteration: 318 | Classification loss: 0.09833 | Regression loss: 0.16763 | Running loss: 0.37615\n",
            "Epoch: 5 | Iteration: 319 | Classification loss: 0.25841 | Regression loss: 0.61251 | Running loss: 0.37741\n",
            "Epoch: 5 | Iteration: 320 | Classification loss: 0.14554 | Regression loss: 0.28071 | Running loss: 0.37708\n",
            "Epoch: 5 | Iteration: 321 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.37594\n",
            "Epoch: 5 | Iteration: 322 | Classification loss: 0.12599 | Regression loss: 0.32514 | Running loss: 0.37589\n",
            "Epoch: 5 | Iteration: 323 | Classification loss: 0.26279 | Regression loss: 0.57033 | Running loss: 0.37710\n",
            "Epoch: 5 | Iteration: 324 | Classification loss: 0.00024 | Regression loss: 0.04623 | Running loss: 0.37668\n",
            "Epoch: 5 | Iteration: 325 | Classification loss: 0.54318 | Regression loss: 0.60314 | Running loss: 0.37888\n",
            "Epoch: 5 | Iteration: 326 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.37878\n",
            "Epoch: 5 | Iteration: 327 | Classification loss: 0.00297 | Regression loss: 0.00000 | Running loss: 0.37779\n",
            "Epoch: 5 | Iteration: 328 | Classification loss: 0.00376 | Regression loss: 0.03354 | Running loss: 0.37735\n",
            "Epoch: 5 | Iteration: 329 | Classification loss: 0.27406 | Regression loss: 0.43850 | Running loss: 0.37739\n",
            "Epoch: 5 | Iteration: 330 | Classification loss: 0.37213 | Regression loss: 0.51028 | Running loss: 0.37796\n",
            "Epoch: 5 | Iteration: 331 | Classification loss: 0.11971 | Regression loss: 0.39538 | Running loss: 0.37811\n",
            "Epoch: 5 | Iteration: 332 | Classification loss: 0.25344 | Regression loss: 0.40229 | Running loss: 0.37845\n",
            "Epoch: 5 | Iteration: 333 | Classification loss: 0.17403 | Regression loss: 0.39047 | Running loss: 0.37834\n",
            "Epoch: 5 | Iteration: 334 | Classification loss: 0.13698 | Regression loss: 0.33563 | Running loss: 0.37808\n",
            "Epoch: 5 | Iteration: 335 | Classification loss: 0.46527 | Regression loss: 0.29062 | Running loss: 0.37889\n",
            "Epoch: 5 | Iteration: 336 | Classification loss: 0.14288 | Regression loss: 0.39827 | Running loss: 0.37899\n",
            "Epoch: 5 | Iteration: 337 | Classification loss: 0.14067 | Regression loss: 0.37130 | Running loss: 0.37910\n",
            "Epoch: 5 | Iteration: 338 | Classification loss: 0.14844 | Regression loss: 0.34936 | Running loss: 0.37943\n",
            "Epoch: 5 | Iteration: 339 | Classification loss: 0.23841 | Regression loss: 0.54224 | Running loss: 0.38019\n",
            "Epoch: 5 | Iteration: 340 | Classification loss: 0.00055 | Regression loss: 0.29956 | Running loss: 0.38072\n",
            "Epoch: 5 | Iteration: 341 | Classification loss: 0.00020 | Regression loss: 0.33081 | Running loss: 0.38007\n",
            "Epoch: 5 | Iteration: 342 | Classification loss: 0.00126 | Regression loss: 0.12978 | Running loss: 0.37924\n",
            "Epoch: 5 | Iteration: 343 | Classification loss: 0.04713 | Regression loss: 0.12455 | Running loss: 0.37952\n",
            "Epoch: 5 | Iteration: 344 | Classification loss: 0.04733 | Regression loss: 0.19087 | Running loss: 0.37920\n",
            "Epoch: 5 | Iteration: 345 | Classification loss: 0.15341 | Regression loss: 0.47917 | Running loss: 0.38046\n",
            "Epoch: 5 | Iteration: 346 | Classification loss: 0.21762 | Regression loss: 0.56935 | Running loss: 0.38101\n",
            "Epoch: 5 | Iteration: 347 | Classification loss: 0.00361 | Regression loss: 0.03758 | Running loss: 0.38021\n",
            "Epoch: 5 | Iteration: 348 | Classification loss: 0.14169 | Regression loss: 0.37249 | Running loss: 0.37947\n",
            "Epoch: 5 | Iteration: 349 | Classification loss: 0.10439 | Regression loss: 0.39192 | Running loss: 0.37903\n",
            "Epoch: 5 | Iteration: 350 | Classification loss: 0.11733 | Regression loss: 0.33644 | Running loss: 0.37943\n",
            "Epoch: 5 | Iteration: 351 | Classification loss: 0.07787 | Regression loss: 0.32995 | Running loss: 0.38025\n",
            "Epoch: 5 | Iteration: 352 | Classification loss: 0.00036 | Regression loss: 0.11541 | Running loss: 0.37882\n",
            "Epoch: 5 | Iteration: 353 | Classification loss: 0.10744 | Regression loss: 0.10876 | Running loss: 0.37887\n",
            "Epoch: 5 | Iteration: 354 | Classification loss: 0.09677 | Regression loss: 0.32845 | Running loss: 0.37915\n",
            "Epoch: 5 | Iteration: 355 | Classification loss: 0.03108 | Regression loss: 0.11820 | Running loss: 0.37848\n",
            "Epoch: 5 | Iteration: 356 | Classification loss: 0.09981 | Regression loss: 0.30748 | Running loss: 0.37876\n",
            "Epoch: 5 | Iteration: 357 | Classification loss: 0.00173 | Regression loss: 0.01317 | Running loss: 0.37653\n",
            "Epoch: 5 | Iteration: 358 | Classification loss: 0.11035 | Regression loss: 0.29346 | Running loss: 0.37724\n",
            "Epoch: 5 | Iteration: 359 | Classification loss: 0.05957 | Regression loss: 0.27252 | Running loss: 0.37696\n",
            "Epoch: 5 | Iteration: 360 | Classification loss: 0.04318 | Regression loss: 0.11046 | Running loss: 0.37548\n",
            "Epoch: 5 | Iteration: 361 | Classification loss: 0.00035 | Regression loss: 0.03563 | Running loss: 0.37430\n",
            "Epoch: 5 | Iteration: 362 | Classification loss: 0.13671 | Regression loss: 0.34246 | Running loss: 0.37495\n",
            "Epoch: 5 | Iteration: 363 | Classification loss: 0.03868 | Regression loss: 0.16034 | Running loss: 0.37456\n",
            "Epoch: 5 | Iteration: 364 | Classification loss: 0.21238 | Regression loss: 0.44818 | Running loss: 0.37521\n",
            "Epoch: 5 | Iteration: 365 | Classification loss: 0.44356 | Regression loss: 0.29250 | Running loss: 0.37571\n",
            "Epoch: 5 | Iteration: 366 | Classification loss: 0.28290 | Regression loss: 0.49565 | Running loss: 0.37724\n",
            "Epoch: 5 | Iteration: 367 | Classification loss: 0.25636 | Regression loss: 0.39314 | Running loss: 0.37847\n",
            "Epoch: 5 | Iteration: 368 | Classification loss: 0.22266 | Regression loss: 0.53249 | Running loss: 0.37953\n",
            "Epoch: 5 | Iteration: 369 | Classification loss: 0.20536 | Regression loss: 0.50721 | Running loss: 0.37996\n",
            "Epoch: 5 | Iteration: 370 | Classification loss: 0.00040 | Regression loss: 0.03370 | Running loss: 0.37958\n",
            "Epoch: 5 | Iteration: 371 | Classification loss: 0.15079 | Regression loss: 0.32407 | Running loss: 0.37967\n",
            "Epoch: 5 | Iteration: 372 | Classification loss: 0.07842 | Regression loss: 0.34948 | Running loss: 0.37955\n",
            "Epoch: 5 | Iteration: 373 | Classification loss: 0.21692 | Regression loss: 0.49935 | Running loss: 0.38050\n",
            "Epoch: 5 | Iteration: 374 | Classification loss: 0.00857 | Regression loss: 0.02143 | Running loss: 0.37951\n",
            "Epoch: 5 | Iteration: 375 | Classification loss: 0.00031 | Regression loss: 0.01343 | Running loss: 0.37736\n",
            "Epoch: 5 | Iteration: 376 | Classification loss: 0.00025 | Regression loss: 0.03410 | Running loss: 0.37693\n",
            "Epoch: 5 | Iteration: 377 | Classification loss: 0.10628 | Regression loss: 0.37859 | Running loss: 0.37714\n",
            "Epoch: 5 | Iteration: 378 | Classification loss: 0.08371 | Regression loss: 0.32973 | Running loss: 0.37790\n",
            "Epoch: 5 | Iteration: 379 | Classification loss: 0.21965 | Regression loss: 0.49165 | Running loss: 0.37926\n",
            "Epoch: 5 | Iteration: 380 | Classification loss: 0.17134 | Regression loss: 0.29680 | Running loss: 0.37911\n",
            "Epoch: 5 | Iteration: 381 | Classification loss: 0.25611 | Regression loss: 0.49211 | Running loss: 0.38051\n",
            "Epoch: 5 | Iteration: 382 | Classification loss: 0.00043 | Regression loss: 0.03339 | Running loss: 0.38024\n",
            "Epoch: 5 | Iteration: 383 | Classification loss: 0.06782 | Regression loss: 0.25899 | Running loss: 0.38032\n",
            "Epoch: 5 | Iteration: 384 | Classification loss: 0.11425 | Regression loss: 0.39016 | Running loss: 0.38084\n",
            "Epoch: 5 | Iteration: 385 | Classification loss: 0.06729 | Regression loss: 0.26974 | Running loss: 0.38148\n",
            "Epoch: 5 | Iteration: 386 | Classification loss: 0.04816 | Regression loss: 0.13139 | Running loss: 0.38114\n",
            "Epoch: 5 | Iteration: 387 | Classification loss: 0.14120 | Regression loss: 0.39611 | Running loss: 0.38147\n",
            "Epoch: 5 | Iteration: 388 | Classification loss: 0.11122 | Regression loss: 0.34725 | Running loss: 0.38214\n",
            "Epoch: 5 | Iteration: 389 | Classification loss: 0.06571 | Regression loss: 0.30497 | Running loss: 0.38179\n",
            "Epoch: 5 | Iteration: 390 | Classification loss: 0.38784 | Regression loss: 0.64461 | Running loss: 0.38295\n",
            "Epoch: 5 | Iteration: 391 | Classification loss: 0.09853 | Regression loss: 0.31673 | Running loss: 0.38322\n",
            "Epoch: 5 | Iteration: 392 | Classification loss: 0.25590 | Regression loss: 0.41137 | Running loss: 0.38344\n",
            "Epoch: 5 | Iteration: 393 | Classification loss: 0.04253 | Regression loss: 0.14925 | Running loss: 0.38262\n",
            "Epoch: 5 | Iteration: 394 | Classification loss: 0.09676 | Regression loss: 0.24416 | Running loss: 0.38323\n",
            "Epoch: 5 | Iteration: 395 | Classification loss: 0.09845 | Regression loss: 0.31133 | Running loss: 0.38315\n",
            "Epoch: 5 | Iteration: 396 | Classification loss: 0.19078 | Regression loss: 0.50664 | Running loss: 0.38449\n",
            "Epoch: 5 | Iteration: 397 | Classification loss: 0.07053 | Regression loss: 0.27979 | Running loss: 0.38465\n",
            "Epoch: 5 | Iteration: 398 | Classification loss: 0.22094 | Regression loss: 0.48386 | Running loss: 0.38477\n",
            "Epoch: 5 | Iteration: 399 | Classification loss: 0.06462 | Regression loss: 0.16468 | Running loss: 0.38332\n",
            "Epoch: 5 | Iteration: 400 | Classification loss: 0.13398 | Regression loss: 0.36591 | Running loss: 0.38331\n",
            "Epoch: 5 | Iteration: 401 | Classification loss: 0.05307 | Regression loss: 0.20778 | Running loss: 0.38374\n",
            "Epoch: 5 | Iteration: 402 | Classification loss: 0.03370 | Regression loss: 0.14342 | Running loss: 0.38316\n",
            "Epoch: 5 | Iteration: 403 | Classification loss: 0.19190 | Regression loss: 0.36798 | Running loss: 0.38273\n",
            "Epoch: 5 | Iteration: 404 | Classification loss: 0.09921 | Regression loss: 0.34333 | Running loss: 0.38258\n",
            "Epoch: 5 | Iteration: 405 | Classification loss: 0.06282 | Regression loss: 0.28437 | Running loss: 0.38220\n",
            "Epoch: 5 | Iteration: 406 | Classification loss: 0.06978 | Regression loss: 0.15222 | Running loss: 0.38214\n",
            "Epoch: 5 | Iteration: 407 | Classification loss: 0.11885 | Regression loss: 0.36270 | Running loss: 0.38206\n",
            "Epoch: 5 | Iteration: 408 | Classification loss: 0.10220 | Regression loss: 0.25220 | Running loss: 0.38262\n",
            "Epoch: 5 | Iteration: 409 | Classification loss: 0.06148 | Regression loss: 0.29524 | Running loss: 0.38292\n",
            "Epoch: 5 | Iteration: 410 | Classification loss: 0.15890 | Regression loss: 0.41548 | Running loss: 0.38222\n",
            "Epoch: 5 | Iteration: 411 | Classification loss: 0.04969 | Regression loss: 0.25928 | Running loss: 0.38203\n",
            "Epoch: 5 | Iteration: 412 | Classification loss: 0.00072 | Regression loss: 0.00901 | Running loss: 0.38196\n",
            "Epoch: 5 | Iteration: 413 | Classification loss: 0.04813 | Regression loss: 0.25980 | Running loss: 0.38098\n",
            "Epoch: 5 | Iteration: 414 | Classification loss: 0.11556 | Regression loss: 0.45616 | Running loss: 0.38211\n",
            "Epoch: 5 | Iteration: 415 | Classification loss: 0.10545 | Regression loss: 0.25461 | Running loss: 0.38185\n",
            "Epoch: 5 | Iteration: 416 | Classification loss: 0.07640 | Regression loss: 0.18783 | Running loss: 0.38150\n",
            "Epoch: 5 | Iteration: 417 | Classification loss: 0.02859 | Regression loss: 0.13737 | Running loss: 0.38130\n",
            "Epoch: 5 | Iteration: 418 | Classification loss: 0.13619 | Regression loss: 0.37270 | Running loss: 0.38158\n",
            "Epoch: 5 | Iteration: 419 | Classification loss: 0.02623 | Regression loss: 0.13258 | Running loss: 0.38106\n",
            "Epoch: 5 | Iteration: 420 | Classification loss: 0.07403 | Regression loss: 0.17409 | Running loss: 0.38082\n",
            "Epoch: 5 | Iteration: 421 | Classification loss: 0.03278 | Regression loss: 0.13794 | Running loss: 0.37920\n",
            "Epoch: 5 | Iteration: 422 | Classification loss: 0.00029 | Regression loss: 0.03356 | Running loss: 0.37878\n",
            "Epoch: 5 | Iteration: 423 | Classification loss: 0.09710 | Regression loss: 0.28004 | Running loss: 0.37830\n",
            "Epoch: 5 | Iteration: 424 | Classification loss: 0.15389 | Regression loss: 0.47504 | Running loss: 0.37952\n",
            "Epoch: 5 | Iteration: 425 | Classification loss: 0.00051 | Regression loss: 0.03839 | Running loss: 0.37926\n",
            "Epoch: 5 | Iteration: 426 | Classification loss: 0.00029 | Regression loss: 0.03085 | Running loss: 0.37826\n",
            "Epoch: 5 | Iteration: 427 | Classification loss: 0.25313 | Regression loss: 0.55873 | Running loss: 0.37937\n",
            "Epoch: 5 | Iteration: 428 | Classification loss: 0.14297 | Regression loss: 0.36279 | Running loss: 0.38035\n",
            "Epoch: 5 | Iteration: 429 | Classification loss: 0.00351 | Regression loss: 0.04102 | Running loss: 0.37891\n",
            "Epoch: 5 | Iteration: 430 | Classification loss: 0.06881 | Regression loss: 0.33295 | Running loss: 0.37856\n",
            "Epoch: 5 | Iteration: 431 | Classification loss: 0.03656 | Regression loss: 0.12915 | Running loss: 0.37787\n",
            "Epoch: 5 | Iteration: 432 | Classification loss: 0.00013 | Regression loss: 0.04422 | Running loss: 0.37736\n",
            "Epoch: 5 | Iteration: 433 | Classification loss: 0.00019 | Regression loss: 0.02091 | Running loss: 0.37737\n",
            "Epoch: 5 | Iteration: 434 | Classification loss: 0.05950 | Regression loss: 0.28315 | Running loss: 0.37649\n",
            "Epoch: 5 | Iteration: 435 | Classification loss: 0.04089 | Regression loss: 0.22312 | Running loss: 0.37566\n",
            "Epoch: 5 | Iteration: 436 | Classification loss: 0.00019 | Regression loss: 0.05838 | Running loss: 0.37442\n",
            "Epoch: 5 | Iteration: 437 | Classification loss: 0.03548 | Regression loss: 0.15457 | Running loss: 0.37436\n",
            "Epoch: 5 | Iteration: 438 | Classification loss: 0.22328 | Regression loss: 0.49019 | Running loss: 0.37579\n",
            "Epoch: 5 | Iteration: 439 | Classification loss: 0.07947 | Regression loss: 0.29871 | Running loss: 0.37502\n",
            "Epoch: 5 | Iteration: 440 | Classification loss: 0.03434 | Regression loss: 0.15342 | Running loss: 0.37483\n",
            "Epoch: 5 | Iteration: 441 | Classification loss: 0.07966 | Regression loss: 0.22526 | Running loss: 0.37544\n",
            "Epoch: 5 | Iteration: 442 | Classification loss: 0.08031 | Regression loss: 0.19943 | Running loss: 0.37542\n",
            "Epoch: 5 | Iteration: 443 | Classification loss: 0.11769 | Regression loss: 0.43452 | Running loss: 0.37648\n",
            "Epoch: 5 | Iteration: 444 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.37614\n",
            "Epoch: 5 | Iteration: 445 | Classification loss: 0.08067 | Regression loss: 0.26118 | Running loss: 0.37665\n",
            "Epoch: 5 | Iteration: 446 | Classification loss: 0.20938 | Regression loss: 0.49057 | Running loss: 0.37762\n",
            "Epoch: 5 | Iteration: 447 | Classification loss: 0.18823 | Regression loss: 0.25632 | Running loss: 0.37743\n",
            "Epoch: 5 | Iteration: 448 | Classification loss: 0.09286 | Regression loss: 0.29294 | Running loss: 0.37781\n",
            "Epoch: 5 | Iteration: 449 | Classification loss: 0.14575 | Regression loss: 0.36693 | Running loss: 0.37718\n",
            "Epoch: 5 | Iteration: 450 | Classification loss: 0.12774 | Regression loss: 0.34546 | Running loss: 0.37720\n",
            "Epoch: 5 | Iteration: 451 | Classification loss: 0.05004 | Regression loss: 0.18787 | Running loss: 0.37662\n",
            "Epoch: 5 | Iteration: 452 | Classification loss: 0.16710 | Regression loss: 0.48757 | Running loss: 0.37647\n",
            "Epoch: 5 | Iteration: 453 | Classification loss: 0.12516 | Regression loss: 0.35642 | Running loss: 0.37648\n",
            "Epoch: 5 | Iteration: 454 | Classification loss: 0.07297 | Regression loss: 0.35967 | Running loss: 0.37725\n",
            "Epoch: 5 | Iteration: 455 | Classification loss: 0.03631 | Regression loss: 0.16702 | Running loss: 0.37757\n",
            "Epoch: 5 | Iteration: 456 | Classification loss: 0.07973 | Regression loss: 0.32556 | Running loss: 0.37735\n",
            "Epoch: 5 | Iteration: 457 | Classification loss: 0.00027 | Regression loss: 0.02041 | Running loss: 0.37628\n",
            "Epoch: 5 | Iteration: 458 | Classification loss: 0.35109 | Regression loss: 0.59579 | Running loss: 0.37817\n",
            "Epoch: 5 | Iteration: 459 | Classification loss: 0.03482 | Regression loss: 0.14290 | Running loss: 0.37800\n",
            "Epoch: 5 | Iteration: 460 | Classification loss: 0.15945 | Regression loss: 0.40293 | Running loss: 0.37827\n",
            "Epoch: 5 | Iteration: 461 | Classification loss: 0.09282 | Regression loss: 0.32305 | Running loss: 0.37830\n",
            "Epoch: 5 | Iteration: 462 | Classification loss: 0.12371 | Regression loss: 0.33845 | Running loss: 0.37733\n",
            "Epoch: 5 | Iteration: 463 | Classification loss: 0.09906 | Regression loss: 0.46958 | Running loss: 0.37773\n",
            "Epoch: 5 | Iteration: 464 | Classification loss: 0.07085 | Regression loss: 0.16495 | Running loss: 0.37750\n",
            "Epoch: 5 | Iteration: 465 | Classification loss: 0.07457 | Regression loss: 0.23793 | Running loss: 0.37745\n",
            "Epoch: 5 | Iteration: 466 | Classification loss: 0.10171 | Regression loss: 0.33501 | Running loss: 0.37748\n",
            "Epoch: 5 | Iteration: 467 | Classification loss: 0.04620 | Regression loss: 0.17520 | Running loss: 0.37737\n",
            "Epoch: 5 | Iteration: 468 | Classification loss: 0.01141 | Regression loss: 0.02190 | Running loss: 0.37672\n",
            "Epoch: 5 | Iteration: 469 | Classification loss: 0.06413 | Regression loss: 0.31217 | Running loss: 0.37705\n",
            "Epoch: 5 | Iteration: 470 | Classification loss: 0.00015 | Regression loss: 0.00948 | Running loss: 0.37641\n",
            "Epoch: 5 | Iteration: 471 | Classification loss: 0.14032 | Regression loss: 0.38205 | Running loss: 0.37589\n",
            "Epoch: 5 | Iteration: 472 | Classification loss: 0.08609 | Regression loss: 0.16318 | Running loss: 0.37523\n",
            "Epoch: 5 | Iteration: 473 | Classification loss: 0.17841 | Regression loss: 0.20839 | Running loss: 0.37477\n",
            "Epoch: 5 | Iteration: 474 | Classification loss: 0.00084 | Regression loss: 0.02935 | Running loss: 0.37394\n",
            "Epoch: 5 | Iteration: 475 | Classification loss: 0.00038 | Regression loss: 0.01000 | Running loss: 0.37343\n",
            "Epoch: 5 | Iteration: 476 | Classification loss: 0.07590 | Regression loss: 0.13101 | Running loss: 0.37321\n",
            "Epoch: 5 | Iteration: 477 | Classification loss: 0.00059 | Regression loss: 0.00456 | Running loss: 0.37280\n",
            "Epoch: 5 | Iteration: 478 | Classification loss: 0.23927 | Regression loss: 0.37380 | Running loss: 0.37330\n",
            "Epoch: 5 | Iteration: 479 | Classification loss: 0.00162 | Regression loss: 0.00000 | Running loss: 0.37326\n",
            "Epoch: 5 | Iteration: 480 | Classification loss: 0.16545 | Regression loss: 0.49124 | Running loss: 0.37319\n",
            "Epoch: 5 | Iteration: 481 | Classification loss: 0.17585 | Regression loss: 0.48925 | Running loss: 0.37374\n",
            "Epoch: 5 | Iteration: 482 | Classification loss: 0.06775 | Regression loss: 0.32260 | Running loss: 0.37372\n",
            "Epoch: 5 | Iteration: 483 | Classification loss: 0.04932 | Regression loss: 0.23748 | Running loss: 0.37424\n",
            "Epoch: 5 | Iteration: 484 | Classification loss: 0.00063 | Regression loss: 0.05643 | Running loss: 0.37295\n",
            "Epoch: 5 | Iteration: 485 | Classification loss: 0.10503 | Regression loss: 0.25233 | Running loss: 0.37314\n",
            "Epoch: 5 | Iteration: 486 | Classification loss: 0.03374 | Regression loss: 0.12920 | Running loss: 0.37340\n",
            "Epoch: 5 | Iteration: 487 | Classification loss: 0.16334 | Regression loss: 0.47062 | Running loss: 0.37398\n",
            "Epoch: 5 | Iteration: 488 | Classification loss: 0.04567 | Regression loss: 0.18017 | Running loss: 0.37340\n",
            "Epoch: 5 | Iteration: 489 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.37178\n",
            "Epoch: 5 | Iteration: 490 | Classification loss: 0.04901 | Regression loss: 0.15528 | Running loss: 0.37073\n",
            "Epoch: 5 | Iteration: 491 | Classification loss: 0.19427 | Regression loss: 0.44930 | Running loss: 0.37119\n",
            "Epoch: 5 | Iteration: 492 | Classification loss: 0.02768 | Regression loss: 0.02097 | Running loss: 0.37083\n",
            "Epoch: 5 | Iteration: 493 | Classification loss: 0.12016 | Regression loss: 0.30647 | Running loss: 0.37124\n",
            "Epoch: 5 | Iteration: 494 | Classification loss: 0.13721 | Regression loss: 0.42198 | Running loss: 0.37159\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.41s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.855\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.478\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.583\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.592\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 6 | Iteration: 0 | Classification loss: 0.10146 | Regression loss: 0.39262 | Running loss: 0.37089\n",
            "Epoch: 6 | Iteration: 1 | Classification loss: 0.06874 | Regression loss: 0.38666 | Running loss: 0.37047\n",
            "Epoch: 6 | Iteration: 2 | Classification loss: 0.20256 | Regression loss: 0.49380 | Running loss: 0.37075\n",
            "Epoch: 6 | Iteration: 3 | Classification loss: 0.07447 | Regression loss: 0.32030 | Running loss: 0.37096\n",
            "Epoch: 6 | Iteration: 4 | Classification loss: 0.13352 | Regression loss: 0.34344 | Running loss: 0.37031\n",
            "Epoch: 6 | Iteration: 5 | Classification loss: 0.02458 | Regression loss: 0.13930 | Running loss: 0.37064\n",
            "Epoch: 6 | Iteration: 6 | Classification loss: 0.02667 | Regression loss: 0.18239 | Running loss: 0.37015\n",
            "Epoch: 6 | Iteration: 7 | Classification loss: 0.32206 | Regression loss: 0.43811 | Running loss: 0.37078\n",
            "Epoch: 6 | Iteration: 8 | Classification loss: 0.00031 | Regression loss: 0.01374 | Running loss: 0.37081\n",
            "Epoch: 6 | Iteration: 9 | Classification loss: 0.19760 | Regression loss: 0.50554 | Running loss: 0.37168\n",
            "Epoch: 6 | Iteration: 10 | Classification loss: 0.00183 | Regression loss: 0.01619 | Running loss: 0.37080\n",
            "Epoch: 6 | Iteration: 11 | Classification loss: 0.20684 | Regression loss: 0.48059 | Running loss: 0.37124\n",
            "Epoch: 6 | Iteration: 12 | Classification loss: 0.14004 | Regression loss: 0.47073 | Running loss: 0.37246\n",
            "Epoch: 6 | Iteration: 13 | Classification loss: 0.04220 | Regression loss: 0.15388 | Running loss: 0.37234\n",
            "Epoch: 6 | Iteration: 14 | Classification loss: 0.12326 | Regression loss: 0.24416 | Running loss: 0.37125\n",
            "Epoch: 6 | Iteration: 15 | Classification loss: 0.00022 | Regression loss: 0.05395 | Running loss: 0.37013\n",
            "Epoch: 6 | Iteration: 16 | Classification loss: 0.00021 | Regression loss: 0.03543 | Running loss: 0.36960\n",
            "Epoch: 6 | Iteration: 17 | Classification loss: 0.12562 | Regression loss: 0.35117 | Running loss: 0.36978\n",
            "Epoch: 6 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.36817\n",
            "Epoch: 6 | Iteration: 19 | Classification loss: 0.11929 | Regression loss: 0.35007 | Running loss: 0.36905\n",
            "Epoch: 6 | Iteration: 20 | Classification loss: 0.03363 | Regression loss: 0.14364 | Running loss: 0.36858\n",
            "Epoch: 6 | Iteration: 21 | Classification loss: 0.00024 | Regression loss: 0.14964 | Running loss: 0.36789\n",
            "Epoch: 6 | Iteration: 22 | Classification loss: 0.04851 | Regression loss: 0.28470 | Running loss: 0.36849\n",
            "Epoch: 6 | Iteration: 23 | Classification loss: 0.19106 | Regression loss: 0.41335 | Running loss: 0.36846\n",
            "Epoch: 6 | Iteration: 24 | Classification loss: 0.10879 | Regression loss: 0.32027 | Running loss: 0.36748\n",
            "Epoch: 6 | Iteration: 25 | Classification loss: 0.05373 | Regression loss: 0.23450 | Running loss: 0.36803\n",
            "Epoch: 6 | Iteration: 26 | Classification loss: 0.11339 | Regression loss: 0.41329 | Running loss: 0.36848\n",
            "Epoch: 6 | Iteration: 27 | Classification loss: 0.03654 | Regression loss: 0.17723 | Running loss: 0.36719\n",
            "Epoch: 6 | Iteration: 28 | Classification loss: 0.14259 | Regression loss: 0.48898 | Running loss: 0.36765\n",
            "Epoch: 6 | Iteration: 29 | Classification loss: 0.27971 | Regression loss: 0.50449 | Running loss: 0.36915\n",
            "Epoch: 6 | Iteration: 30 | Classification loss: 0.06796 | Regression loss: 0.30768 | Running loss: 0.36883\n",
            "Epoch: 6 | Iteration: 31 | Classification loss: 0.02033 | Regression loss: 0.11974 | Running loss: 0.36808\n",
            "Epoch: 6 | Iteration: 32 | Classification loss: 0.05680 | Regression loss: 0.20576 | Running loss: 0.36825\n",
            "Epoch: 6 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.36761\n",
            "Epoch: 6 | Iteration: 34 | Classification loss: 0.02335 | Regression loss: 0.13334 | Running loss: 0.36781\n",
            "Epoch: 6 | Iteration: 35 | Classification loss: 0.14053 | Regression loss: 0.39571 | Running loss: 0.36851\n",
            "Epoch: 6 | Iteration: 36 | Classification loss: 0.20845 | Regression loss: 0.42067 | Running loss: 0.36939\n",
            "Epoch: 6 | Iteration: 37 | Classification loss: 0.04165 | Regression loss: 0.26375 | Running loss: 0.36994\n",
            "Epoch: 6 | Iteration: 38 | Classification loss: 0.00027 | Regression loss: 0.03966 | Running loss: 0.36968\n",
            "Epoch: 6 | Iteration: 39 | Classification loss: 0.03399 | Regression loss: 0.15724 | Running loss: 0.37004\n",
            "Epoch: 6 | Iteration: 40 | Classification loss: 0.00045 | Regression loss: 0.01731 | Running loss: 0.37007\n",
            "Epoch: 6 | Iteration: 41 | Classification loss: 0.15544 | Regression loss: 0.37154 | Running loss: 0.37029\n",
            "Epoch: 6 | Iteration: 42 | Classification loss: 0.07266 | Regression loss: 0.21411 | Running loss: 0.37021\n",
            "Epoch: 6 | Iteration: 43 | Classification loss: 0.00496 | Regression loss: 0.01033 | Running loss: 0.36989\n",
            "Epoch: 6 | Iteration: 44 | Classification loss: 0.00015 | Regression loss: 0.00824 | Running loss: 0.36887\n",
            "Epoch: 6 | Iteration: 45 | Classification loss: 0.09905 | Regression loss: 0.34673 | Running loss: 0.36890\n",
            "Epoch: 6 | Iteration: 46 | Classification loss: 0.03680 | Regression loss: 0.07433 | Running loss: 0.36879\n",
            "Epoch: 6 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.36786\n",
            "Epoch: 6 | Iteration: 48 | Classification loss: 0.00144 | Regression loss: 0.05699 | Running loss: 0.36695\n",
            "Epoch: 6 | Iteration: 49 | Classification loss: 0.03874 | Regression loss: 0.16741 | Running loss: 0.36675\n",
            "Epoch: 6 | Iteration: 50 | Classification loss: 0.04918 | Regression loss: 0.16992 | Running loss: 0.36626\n",
            "Epoch: 6 | Iteration: 51 | Classification loss: 0.15443 | Regression loss: 0.28415 | Running loss: 0.36708\n",
            "Epoch: 6 | Iteration: 52 | Classification loss: 0.15642 | Regression loss: 0.36088 | Running loss: 0.36806\n",
            "Epoch: 6 | Iteration: 53 | Classification loss: 0.01630 | Regression loss: 0.08293 | Running loss: 0.36747\n",
            "Epoch: 6 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.36744\n",
            "Epoch: 6 | Iteration: 55 | Classification loss: 0.17506 | Regression loss: 0.40088 | Running loss: 0.36729\n",
            "Epoch: 6 | Iteration: 56 | Classification loss: 0.06465 | Regression loss: 0.16929 | Running loss: 0.36665\n",
            "Epoch: 6 | Iteration: 57 | Classification loss: 0.31482 | Regression loss: 0.42322 | Running loss: 0.36723\n",
            "Epoch: 6 | Iteration: 58 | Classification loss: 0.27179 | Regression loss: 0.39325 | Running loss: 0.36685\n",
            "Epoch: 6 | Iteration: 59 | Classification loss: 0.11608 | Regression loss: 0.32653 | Running loss: 0.36720\n",
            "Epoch: 6 | Iteration: 60 | Classification loss: 0.06606 | Regression loss: 0.24493 | Running loss: 0.36684\n",
            "Epoch: 6 | Iteration: 61 | Classification loss: 0.06920 | Regression loss: 0.36114 | Running loss: 0.36695\n",
            "Epoch: 6 | Iteration: 62 | Classification loss: 0.07473 | Regression loss: 0.33506 | Running loss: 0.36658\n",
            "Epoch: 6 | Iteration: 63 | Classification loss: 0.04785 | Regression loss: 0.18418 | Running loss: 0.36654\n",
            "Epoch: 6 | Iteration: 64 | Classification loss: 0.08732 | Regression loss: 0.33485 | Running loss: 0.36702\n",
            "Epoch: 6 | Iteration: 65 | Classification loss: 0.13509 | Regression loss: 0.33779 | Running loss: 0.36748\n",
            "Epoch: 6 | Iteration: 66 | Classification loss: 0.73424 | Regression loss: 0.22224 | Running loss: 0.36875\n",
            "Epoch: 6 | Iteration: 67 | Classification loss: 0.05629 | Regression loss: 0.22148 | Running loss: 0.36825\n",
            "Epoch: 6 | Iteration: 68 | Classification loss: 0.00014 | Regression loss: 0.05312 | Running loss: 0.36679\n",
            "Epoch: 6 | Iteration: 69 | Classification loss: 0.13286 | Regression loss: 0.37698 | Running loss: 0.36706\n",
            "Epoch: 6 | Iteration: 70 | Classification loss: 0.07209 | Regression loss: 0.20664 | Running loss: 0.36702\n",
            "Epoch: 6 | Iteration: 71 | Classification loss: 0.12939 | Regression loss: 0.17315 | Running loss: 0.36694\n",
            "Epoch: 6 | Iteration: 72 | Classification loss: 0.02868 | Regression loss: 0.11122 | Running loss: 0.36633\n",
            "Epoch: 6 | Iteration: 73 | Classification loss: 0.00009 | Regression loss: 0.01258 | Running loss: 0.36553\n",
            "Epoch: 6 | Iteration: 74 | Classification loss: 0.03713 | Regression loss: 0.14566 | Running loss: 0.36589\n",
            "Epoch: 6 | Iteration: 75 | Classification loss: 0.00026 | Regression loss: 0.04158 | Running loss: 0.36591\n",
            "Epoch: 6 | Iteration: 76 | Classification loss: 0.04472 | Regression loss: 0.18259 | Running loss: 0.36632\n",
            "Epoch: 6 | Iteration: 77 | Classification loss: 0.13665 | Regression loss: 0.13556 | Running loss: 0.36679\n",
            "Epoch: 6 | Iteration: 78 | Classification loss: 0.08705 | Regression loss: 0.43379 | Running loss: 0.36585\n",
            "Epoch: 6 | Iteration: 79 | Classification loss: 0.05495 | Regression loss: 0.23440 | Running loss: 0.36574\n",
            "Epoch: 6 | Iteration: 80 | Classification loss: 0.05990 | Regression loss: 0.17883 | Running loss: 0.36564\n",
            "Epoch: 6 | Iteration: 81 | Classification loss: 0.00359 | Regression loss: 0.00980 | Running loss: 0.36446\n",
            "Epoch: 6 | Iteration: 82 | Classification loss: 0.06307 | Regression loss: 0.25740 | Running loss: 0.36506\n",
            "Epoch: 6 | Iteration: 83 | Classification loss: 0.12142 | Regression loss: 0.35368 | Running loss: 0.36491\n",
            "Epoch: 6 | Iteration: 84 | Classification loss: 0.67472 | Regression loss: 0.31592 | Running loss: 0.36684\n",
            "Epoch: 6 | Iteration: 85 | Classification loss: 0.03818 | Regression loss: 0.21328 | Running loss: 0.36730\n",
            "Epoch: 6 | Iteration: 86 | Classification loss: 0.00013 | Regression loss: 0.02229 | Running loss: 0.36700\n",
            "Epoch: 6 | Iteration: 87 | Classification loss: 0.03466 | Regression loss: 0.14854 | Running loss: 0.36734\n",
            "Epoch: 6 | Iteration: 88 | Classification loss: 0.10707 | Regression loss: 0.34346 | Running loss: 0.36790\n",
            "Epoch: 6 | Iteration: 89 | Classification loss: 0.09303 | Regression loss: 0.26208 | Running loss: 0.36745\n",
            "Epoch: 6 | Iteration: 90 | Classification loss: 0.04370 | Regression loss: 0.19800 | Running loss: 0.36788\n",
            "Epoch: 6 | Iteration: 91 | Classification loss: 0.00010 | Regression loss: 0.02258 | Running loss: 0.36764\n",
            "Epoch: 6 | Iteration: 92 | Classification loss: 0.30136 | Regression loss: 0.57278 | Running loss: 0.36866\n",
            "Epoch: 6 | Iteration: 93 | Classification loss: 0.07116 | Regression loss: 0.31250 | Running loss: 0.36860\n",
            "Epoch: 6 | Iteration: 94 | Classification loss: 0.10149 | Regression loss: 0.18321 | Running loss: 0.36807\n",
            "Epoch: 6 | Iteration: 95 | Classification loss: 0.07312 | Regression loss: 0.29434 | Running loss: 0.36879\n",
            "Epoch: 6 | Iteration: 96 | Classification loss: 0.00024 | Regression loss: 0.02425 | Running loss: 0.36857\n",
            "Epoch: 6 | Iteration: 97 | Classification loss: 0.02317 | Regression loss: 0.11876 | Running loss: 0.36714\n",
            "Epoch: 6 | Iteration: 98 | Classification loss: 0.20056 | Regression loss: 0.43407 | Running loss: 0.36757\n",
            "Epoch: 6 | Iteration: 99 | Classification loss: 0.17629 | Regression loss: 0.31921 | Running loss: 0.36822\n",
            "Epoch: 6 | Iteration: 100 | Classification loss: 0.15928 | Regression loss: 0.41165 | Running loss: 0.36932\n",
            "Epoch: 6 | Iteration: 101 | Classification loss: 0.04056 | Regression loss: 0.14606 | Running loss: 0.36873\n",
            "Epoch: 6 | Iteration: 102 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.36791\n",
            "Epoch: 6 | Iteration: 103 | Classification loss: 0.00032 | Regression loss: 0.00615 | Running loss: 0.36748\n",
            "Epoch: 6 | Iteration: 104 | Classification loss: 0.04598 | Regression loss: 0.27798 | Running loss: 0.36776\n",
            "Epoch: 6 | Iteration: 105 | Classification loss: 0.00014 | Regression loss: 0.00660 | Running loss: 0.36646\n",
            "Epoch: 6 | Iteration: 106 | Classification loss: 0.10376 | Regression loss: 0.33706 | Running loss: 0.36652\n",
            "Epoch: 6 | Iteration: 107 | Classification loss: 0.17605 | Regression loss: 0.45808 | Running loss: 0.36727\n",
            "Epoch: 6 | Iteration: 108 | Classification loss: 0.06990 | Regression loss: 0.27716 | Running loss: 0.36794\n",
            "Epoch: 6 | Iteration: 109 | Classification loss: 0.09052 | Regression loss: 0.35103 | Running loss: 0.36805\n",
            "Epoch: 6 | Iteration: 110 | Classification loss: 0.00015 | Regression loss: 0.01837 | Running loss: 0.36726\n",
            "Epoch: 6 | Iteration: 111 | Classification loss: 0.03282 | Regression loss: 0.11424 | Running loss: 0.36715\n",
            "Epoch: 6 | Iteration: 112 | Classification loss: 0.15004 | Regression loss: 0.38666 | Running loss: 0.36743\n",
            "Epoch: 6 | Iteration: 113 | Classification loss: 0.00027 | Regression loss: 0.01680 | Running loss: 0.36746\n",
            "Epoch: 6 | Iteration: 114 | Classification loss: 0.06563 | Regression loss: 0.27118 | Running loss: 0.36730\n",
            "Epoch: 6 | Iteration: 115 | Classification loss: 0.05408 | Regression loss: 0.17264 | Running loss: 0.36670\n",
            "Epoch: 6 | Iteration: 116 | Classification loss: 0.06013 | Regression loss: 0.25346 | Running loss: 0.36700\n",
            "Epoch: 6 | Iteration: 117 | Classification loss: 0.12151 | Regression loss: 0.39477 | Running loss: 0.36718\n",
            "Epoch: 6 | Iteration: 118 | Classification loss: 0.00021 | Regression loss: 0.08630 | Running loss: 0.36604\n",
            "Epoch: 6 | Iteration: 119 | Classification loss: 0.07274 | Regression loss: 0.28370 | Running loss: 0.36675\n",
            "Epoch: 6 | Iteration: 120 | Classification loss: 0.08056 | Regression loss: 0.18757 | Running loss: 0.36651\n",
            "Epoch: 6 | Iteration: 121 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.36544\n",
            "Epoch: 6 | Iteration: 122 | Classification loss: 0.16416 | Regression loss: 0.30751 | Running loss: 0.36612\n",
            "Epoch: 6 | Iteration: 123 | Classification loss: 0.05467 | Regression loss: 0.25488 | Running loss: 0.36626\n",
            "Epoch: 6 | Iteration: 124 | Classification loss: 0.00148 | Regression loss: 0.01091 | Running loss: 0.36627\n",
            "Epoch: 6 | Iteration: 125 | Classification loss: 0.13148 | Regression loss: 0.36567 | Running loss: 0.36668\n",
            "Epoch: 6 | Iteration: 126 | Classification loss: 0.07894 | Regression loss: 0.25816 | Running loss: 0.36579\n",
            "Epoch: 6 | Iteration: 127 | Classification loss: 0.35190 | Regression loss: 0.34017 | Running loss: 0.36628\n",
            "Epoch: 6 | Iteration: 128 | Classification loss: 0.10806 | Regression loss: 0.34362 | Running loss: 0.36714\n",
            "Epoch: 6 | Iteration: 129 | Classification loss: 0.05680 | Regression loss: 0.32030 | Running loss: 0.36706\n",
            "Epoch: 6 | Iteration: 130 | Classification loss: 0.03523 | Regression loss: 0.19069 | Running loss: 0.36661\n",
            "Epoch: 6 | Iteration: 131 | Classification loss: 0.19903 | Regression loss: 0.47472 | Running loss: 0.36794\n",
            "Epoch: 6 | Iteration: 132 | Classification loss: 0.00069 | Regression loss: 0.06478 | Running loss: 0.36716\n",
            "Epoch: 6 | Iteration: 133 | Classification loss: 0.07299 | Regression loss: 0.34982 | Running loss: 0.36764\n",
            "Epoch: 6 | Iteration: 134 | Classification loss: 0.07824 | Regression loss: 0.38891 | Running loss: 0.36741\n",
            "Epoch: 6 | Iteration: 135 | Classification loss: 0.05695 | Regression loss: 0.15135 | Running loss: 0.36698\n",
            "Epoch: 6 | Iteration: 136 | Classification loss: 0.09429 | Regression loss: 0.36053 | Running loss: 0.36787\n",
            "Epoch: 6 | Iteration: 137 | Classification loss: 0.26145 | Regression loss: 0.60253 | Running loss: 0.36864\n",
            "Epoch: 6 | Iteration: 138 | Classification loss: 0.00929 | Regression loss: 0.00000 | Running loss: 0.36822\n",
            "Epoch: 6 | Iteration: 139 | Classification loss: 0.12439 | Regression loss: 0.18936 | Running loss: 0.36844\n",
            "Epoch: 6 | Iteration: 140 | Classification loss: 0.09049 | Regression loss: 0.40977 | Running loss: 0.36876\n",
            "Epoch: 6 | Iteration: 141 | Classification loss: 0.01999 | Regression loss: 0.09988 | Running loss: 0.36772\n",
            "Epoch: 6 | Iteration: 142 | Classification loss: 0.03314 | Regression loss: 0.11807 | Running loss: 0.36772\n",
            "Epoch: 6 | Iteration: 143 | Classification loss: 0.00015 | Regression loss: 0.04076 | Running loss: 0.36730\n",
            "Epoch: 6 | Iteration: 144 | Classification loss: 0.30692 | Regression loss: 0.61690 | Running loss: 0.36817\n",
            "Epoch: 6 | Iteration: 145 | Classification loss: 0.16771 | Regression loss: 0.47645 | Running loss: 0.36891\n",
            "Epoch: 6 | Iteration: 146 | Classification loss: 0.04098 | Regression loss: 0.16484 | Running loss: 0.36787\n",
            "Epoch: 6 | Iteration: 147 | Classification loss: 0.09616 | Regression loss: 0.25373 | Running loss: 0.36775\n",
            "Epoch: 6 | Iteration: 148 | Classification loss: 0.12523 | Regression loss: 0.36348 | Running loss: 0.36873\n",
            "Epoch: 6 | Iteration: 149 | Classification loss: 0.08778 | Regression loss: 0.16158 | Running loss: 0.36858\n",
            "Epoch: 6 | Iteration: 150 | Classification loss: 0.10693 | Regression loss: 0.35302 | Running loss: 0.36913\n",
            "Epoch: 6 | Iteration: 151 | Classification loss: 0.28376 | Regression loss: 0.49824 | Running loss: 0.36938\n",
            "Epoch: 6 | Iteration: 152 | Classification loss: 0.06672 | Regression loss: 0.15276 | Running loss: 0.36913\n",
            "Epoch: 6 | Iteration: 153 | Classification loss: 0.08164 | Regression loss: 0.30923 | Running loss: 0.36872\n",
            "Epoch: 6 | Iteration: 154 | Classification loss: 0.00018 | Regression loss: 0.04215 | Running loss: 0.36707\n",
            "Epoch: 6 | Iteration: 155 | Classification loss: 0.13919 | Regression loss: 0.36240 | Running loss: 0.36728\n",
            "Epoch: 6 | Iteration: 156 | Classification loss: 0.08589 | Regression loss: 0.31333 | Running loss: 0.36702\n",
            "Epoch: 6 | Iteration: 157 | Classification loss: 0.09706 | Regression loss: 0.22711 | Running loss: 0.36690\n",
            "Epoch: 6 | Iteration: 158 | Classification loss: 0.16003 | Regression loss: 0.30238 | Running loss: 0.36777\n",
            "Epoch: 6 | Iteration: 159 | Classification loss: 0.00009 | Regression loss: 0.00928 | Running loss: 0.36641\n",
            "Epoch: 6 | Iteration: 160 | Classification loss: 0.03163 | Regression loss: 0.17522 | Running loss: 0.36529\n",
            "Epoch: 6 | Iteration: 161 | Classification loss: 0.00025 | Regression loss: 0.01096 | Running loss: 0.36500\n",
            "Epoch: 6 | Iteration: 162 | Classification loss: 0.10357 | Regression loss: 0.30932 | Running loss: 0.36575\n",
            "Epoch: 6 | Iteration: 163 | Classification loss: 0.03061 | Regression loss: 0.12919 | Running loss: 0.36500\n",
            "Epoch: 6 | Iteration: 164 | Classification loss: 0.23237 | Regression loss: 0.45080 | Running loss: 0.36552\n",
            "Epoch: 6 | Iteration: 165 | Classification loss: 0.03665 | Regression loss: 0.16606 | Running loss: 0.36451\n",
            "Epoch: 6 | Iteration: 166 | Classification loss: 0.11165 | Regression loss: 0.28378 | Running loss: 0.36529\n",
            "Epoch: 6 | Iteration: 167 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.36437\n",
            "Epoch: 6 | Iteration: 168 | Classification loss: 0.02654 | Regression loss: 0.14109 | Running loss: 0.36351\n",
            "Epoch: 6 | Iteration: 169 | Classification loss: 0.11025 | Regression loss: 0.33436 | Running loss: 0.36440\n",
            "Epoch: 6 | Iteration: 170 | Classification loss: 0.12474 | Regression loss: 0.32881 | Running loss: 0.36405\n",
            "Epoch: 6 | Iteration: 171 | Classification loss: 0.06623 | Regression loss: 0.17881 | Running loss: 0.36357\n",
            "Epoch: 6 | Iteration: 172 | Classification loss: 0.08930 | Regression loss: 0.30655 | Running loss: 0.36329\n",
            "Epoch: 6 | Iteration: 173 | Classification loss: 0.12718 | Regression loss: 0.38564 | Running loss: 0.36299\n",
            "Epoch: 6 | Iteration: 174 | Classification loss: 0.05638 | Regression loss: 0.25366 | Running loss: 0.36279\n",
            "Epoch: 6 | Iteration: 175 | Classification loss: 0.05740 | Regression loss: 0.22504 | Running loss: 0.36189\n",
            "Epoch: 6 | Iteration: 176 | Classification loss: 0.12964 | Regression loss: 0.41775 | Running loss: 0.36212\n",
            "Epoch: 6 | Iteration: 177 | Classification loss: 0.17561 | Regression loss: 0.43765 | Running loss: 0.36151\n",
            "Epoch: 6 | Iteration: 178 | Classification loss: 0.20605 | Regression loss: 0.42648 | Running loss: 0.36265\n",
            "Epoch: 6 | Iteration: 179 | Classification loss: 0.11707 | Regression loss: 0.24133 | Running loss: 0.36237\n",
            "Epoch: 6 | Iteration: 180 | Classification loss: 0.05370 | Regression loss: 0.22152 | Running loss: 0.36227\n",
            "Epoch: 6 | Iteration: 181 | Classification loss: 0.00026 | Regression loss: 0.07968 | Running loss: 0.36229\n",
            "Epoch: 6 | Iteration: 182 | Classification loss: 0.14243 | Regression loss: 0.39651 | Running loss: 0.36207\n",
            "Epoch: 6 | Iteration: 183 | Classification loss: 0.12871 | Regression loss: 0.17448 | Running loss: 0.36221\n",
            "Epoch: 6 | Iteration: 184 | Classification loss: 0.11332 | Regression loss: 0.31176 | Running loss: 0.36089\n",
            "Epoch: 6 | Iteration: 185 | Classification loss: 0.28145 | Regression loss: 0.42855 | Running loss: 0.36221\n",
            "Epoch: 6 | Iteration: 186 | Classification loss: 0.00033 | Regression loss: 0.02640 | Running loss: 0.36169\n",
            "Epoch: 6 | Iteration: 187 | Classification loss: 0.21379 | Regression loss: 0.47127 | Running loss: 0.36153\n",
            "Epoch: 6 | Iteration: 188 | Classification loss: 0.09058 | Regression loss: 0.30462 | Running loss: 0.36083\n",
            "Epoch: 6 | Iteration: 189 | Classification loss: 0.03006 | Regression loss: 0.12870 | Running loss: 0.36046\n",
            "Epoch: 6 | Iteration: 190 | Classification loss: 0.05696 | Regression loss: 0.15281 | Running loss: 0.35996\n",
            "Epoch: 6 | Iteration: 191 | Classification loss: 0.02679 | Regression loss: 0.13833 | Running loss: 0.35982\n",
            "Epoch: 6 | Iteration: 192 | Classification loss: 0.08587 | Regression loss: 0.16964 | Running loss: 0.35936\n",
            "Epoch: 6 | Iteration: 193 | Classification loss: 0.00075 | Regression loss: 0.02587 | Running loss: 0.35794\n",
            "Epoch: 6 | Iteration: 194 | Classification loss: 0.06479 | Regression loss: 0.19378 | Running loss: 0.35740\n",
            "Epoch: 6 | Iteration: 195 | Classification loss: 0.19876 | Regression loss: 0.35098 | Running loss: 0.35824\n",
            "Epoch: 6 | Iteration: 196 | Classification loss: 0.04600 | Regression loss: 0.18598 | Running loss: 0.35825\n",
            "Epoch: 6 | Iteration: 197 | Classification loss: 0.10687 | Regression loss: 0.30661 | Running loss: 0.35895\n",
            "Epoch: 6 | Iteration: 198 | Classification loss: 0.00012 | Regression loss: 0.03308 | Running loss: 0.35728\n",
            "Epoch: 6 | Iteration: 199 | Classification loss: 0.12643 | Regression loss: 0.42362 | Running loss: 0.35778\n",
            "Epoch: 6 | Iteration: 200 | Classification loss: 0.00025 | Regression loss: 0.01211 | Running loss: 0.35706\n",
            "Epoch: 6 | Iteration: 201 | Classification loss: 0.04968 | Regression loss: 0.17696 | Running loss: 0.35614\n",
            "Epoch: 6 | Iteration: 202 | Classification loss: 0.00019 | Regression loss: 0.00825 | Running loss: 0.35513\n",
            "Epoch: 6 | Iteration: 203 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35435\n",
            "Epoch: 6 | Iteration: 204 | Classification loss: 0.02432 | Regression loss: 0.12284 | Running loss: 0.35310\n",
            "Epoch: 6 | Iteration: 205 | Classification loss: 0.12400 | Regression loss: 0.40868 | Running loss: 0.35351\n",
            "Epoch: 6 | Iteration: 206 | Classification loss: 0.19744 | Regression loss: 0.42270 | Running loss: 0.35430\n",
            "Epoch: 6 | Iteration: 207 | Classification loss: 0.16206 | Regression loss: 0.51180 | Running loss: 0.35484\n",
            "Epoch: 6 | Iteration: 208 | Classification loss: 0.42019 | Regression loss: 0.34215 | Running loss: 0.35539\n",
            "Epoch: 6 | Iteration: 209 | Classification loss: 0.02682 | Regression loss: 0.09547 | Running loss: 0.35450\n",
            "Epoch: 6 | Iteration: 210 | Classification loss: 0.11597 | Regression loss: 0.34838 | Running loss: 0.35435\n",
            "Epoch: 6 | Iteration: 211 | Classification loss: 0.08347 | Regression loss: 0.30511 | Running loss: 0.35480\n",
            "Epoch: 6 | Iteration: 212 | Classification loss: 0.00016 | Regression loss: 0.04129 | Running loss: 0.35486\n",
            "Epoch: 6 | Iteration: 213 | Classification loss: 0.09586 | Regression loss: 0.40413 | Running loss: 0.35537\n",
            "Epoch: 6 | Iteration: 214 | Classification loss: 0.07049 | Regression loss: 0.29385 | Running loss: 0.35507\n",
            "Epoch: 6 | Iteration: 215 | Classification loss: 0.17472 | Regression loss: 0.39968 | Running loss: 0.35566\n",
            "Epoch: 6 | Iteration: 216 | Classification loss: 0.02140 | Regression loss: 0.12533 | Running loss: 0.35589\n",
            "Epoch: 6 | Iteration: 217 | Classification loss: 0.08565 | Regression loss: 0.32134 | Running loss: 0.35627\n",
            "Epoch: 6 | Iteration: 218 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35554\n",
            "Epoch: 6 | Iteration: 219 | Classification loss: 0.00012 | Regression loss: 0.07059 | Running loss: 0.35566\n",
            "Epoch: 6 | Iteration: 220 | Classification loss: 0.04136 | Regression loss: 0.11191 | Running loss: 0.35481\n",
            "Epoch: 6 | Iteration: 221 | Classification loss: 0.03590 | Regression loss: 0.12197 | Running loss: 0.35432\n",
            "Epoch: 6 | Iteration: 222 | Classification loss: 0.22906 | Regression loss: 0.39931 | Running loss: 0.35464\n",
            "Epoch: 6 | Iteration: 223 | Classification loss: 0.13965 | Regression loss: 0.36192 | Running loss: 0.35499\n",
            "Epoch: 6 | Iteration: 224 | Classification loss: 0.13474 | Regression loss: 0.44696 | Running loss: 0.35615\n",
            "Epoch: 6 | Iteration: 225 | Classification loss: 0.05838 | Regression loss: 0.27767 | Running loss: 0.35544\n",
            "Epoch: 6 | Iteration: 226 | Classification loss: 0.14905 | Regression loss: 0.32984 | Running loss: 0.35634\n",
            "Epoch: 6 | Iteration: 227 | Classification loss: 0.11599 | Regression loss: 0.39516 | Running loss: 0.35732\n",
            "Epoch: 6 | Iteration: 228 | Classification loss: 0.04425 | Regression loss: 0.12073 | Running loss: 0.35724\n",
            "Epoch: 6 | Iteration: 229 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35648\n",
            "Epoch: 6 | Iteration: 230 | Classification loss: 0.05005 | Regression loss: 0.30344 | Running loss: 0.35671\n",
            "Epoch: 6 | Iteration: 231 | Classification loss: 0.10926 | Regression loss: 0.38153 | Running loss: 0.35695\n",
            "Epoch: 6 | Iteration: 232 | Classification loss: 0.04075 | Regression loss: 0.17432 | Running loss: 0.35617\n",
            "Epoch: 6 | Iteration: 233 | Classification loss: 0.00020 | Regression loss: 0.04789 | Running loss: 0.35558\n",
            "Epoch: 6 | Iteration: 234 | Classification loss: 0.08453 | Regression loss: 0.21976 | Running loss: 0.35579\n",
            "Epoch: 6 | Iteration: 235 | Classification loss: 0.04851 | Regression loss: 0.23490 | Running loss: 0.35597\n",
            "Epoch: 6 | Iteration: 236 | Classification loss: 0.05270 | Regression loss: 0.13575 | Running loss: 0.35566\n",
            "Epoch: 6 | Iteration: 237 | Classification loss: 0.09626 | Regression loss: 0.34451 | Running loss: 0.35646\n",
            "Epoch: 6 | Iteration: 238 | Classification loss: 0.05616 | Regression loss: 0.21839 | Running loss: 0.35612\n",
            "Epoch: 6 | Iteration: 239 | Classification loss: 0.06569 | Regression loss: 0.23186 | Running loss: 0.35593\n",
            "Epoch: 6 | Iteration: 240 | Classification loss: 0.08529 | Regression loss: 0.33952 | Running loss: 0.35673\n",
            "Epoch: 6 | Iteration: 241 | Classification loss: 0.07803 | Regression loss: 0.25690 | Running loss: 0.35648\n",
            "Epoch: 6 | Iteration: 242 | Classification loss: 0.13659 | Regression loss: 0.35613 | Running loss: 0.35742\n",
            "Epoch: 6 | Iteration: 243 | Classification loss: 0.03215 | Regression loss: 0.15807 | Running loss: 0.35745\n",
            "Epoch: 6 | Iteration: 244 | Classification loss: 0.07247 | Regression loss: 0.25004 | Running loss: 0.35749\n",
            "Epoch: 6 | Iteration: 245 | Classification loss: 0.06097 | Regression loss: 0.25329 | Running loss: 0.35736\n",
            "Epoch: 6 | Iteration: 246 | Classification loss: 0.05267 | Regression loss: 0.31007 | Running loss: 0.35793\n",
            "Epoch: 6 | Iteration: 247 | Classification loss: 0.01981 | Regression loss: 0.16119 | Running loss: 0.35717\n",
            "Epoch: 6 | Iteration: 248 | Classification loss: 0.09914 | Regression loss: 0.32814 | Running loss: 0.35753\n",
            "Epoch: 6 | Iteration: 249 | Classification loss: 0.03748 | Regression loss: 0.16257 | Running loss: 0.35752\n",
            "Epoch: 6 | Iteration: 250 | Classification loss: 0.00014 | Regression loss: 0.01521 | Running loss: 0.35626\n",
            "Epoch: 6 | Iteration: 251 | Classification loss: 0.20140 | Regression loss: 0.34534 | Running loss: 0.35684\n",
            "Epoch: 6 | Iteration: 252 | Classification loss: 0.16581 | Regression loss: 0.38124 | Running loss: 0.35675\n",
            "Epoch: 6 | Iteration: 253 | Classification loss: 0.00015 | Regression loss: 0.02582 | Running loss: 0.35558\n",
            "Epoch: 6 | Iteration: 254 | Classification loss: 0.10190 | Regression loss: 0.32653 | Running loss: 0.35582\n",
            "Epoch: 6 | Iteration: 255 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35491\n",
            "Epoch: 6 | Iteration: 256 | Classification loss: 0.09267 | Regression loss: 0.33786 | Running loss: 0.35451\n",
            "Epoch: 6 | Iteration: 257 | Classification loss: 0.04379 | Regression loss: 0.23023 | Running loss: 0.35428\n",
            "Epoch: 6 | Iteration: 258 | Classification loss: 0.04675 | Regression loss: 0.13521 | Running loss: 0.35458\n",
            "Epoch: 6 | Iteration: 259 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35394\n",
            "Epoch: 6 | Iteration: 260 | Classification loss: 0.04409 | Regression loss: 0.22221 | Running loss: 0.35379\n",
            "Epoch: 6 | Iteration: 261 | Classification loss: 0.06777 | Regression loss: 0.18329 | Running loss: 0.35399\n",
            "Epoch: 6 | Iteration: 262 | Classification loss: 0.00012 | Regression loss: 0.03815 | Running loss: 0.35342\n",
            "Epoch: 6 | Iteration: 263 | Classification loss: 0.07175 | Regression loss: 0.25323 | Running loss: 0.35252\n",
            "Epoch: 6 | Iteration: 264 | Classification loss: 0.07641 | Regression loss: 0.30894 | Running loss: 0.35242\n",
            "Epoch: 6 | Iteration: 265 | Classification loss: 0.07965 | Regression loss: 0.33421 | Running loss: 0.35283\n",
            "Epoch: 6 | Iteration: 266 | Classification loss: 0.04231 | Regression loss: 0.21258 | Running loss: 0.35215\n",
            "Epoch: 6 | Iteration: 267 | Classification loss: 0.25800 | Regression loss: 0.50206 | Running loss: 0.35367\n",
            "Epoch: 6 | Iteration: 268 | Classification loss: 0.07652 | Regression loss: 0.32688 | Running loss: 0.35411\n",
            "Epoch: 6 | Iteration: 269 | Classification loss: 0.13697 | Regression loss: 0.40352 | Running loss: 0.35516\n",
            "Epoch: 6 | Iteration: 270 | Classification loss: 0.02174 | Regression loss: 0.11769 | Running loss: 0.35494\n",
            "Epoch: 6 | Iteration: 271 | Classification loss: 0.02299 | Regression loss: 0.12818 | Running loss: 0.35433\n",
            "Epoch: 6 | Iteration: 272 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.35358\n",
            "Epoch: 6 | Iteration: 273 | Classification loss: 0.12245 | Regression loss: 0.27937 | Running loss: 0.35369\n",
            "Epoch: 6 | Iteration: 274 | Classification loss: 0.10408 | Regression loss: 0.32828 | Running loss: 0.35418\n",
            "Epoch: 6 | Iteration: 275 | Classification loss: 0.02402 | Regression loss: 0.13595 | Running loss: 0.35379\n",
            "Epoch: 6 | Iteration: 276 | Classification loss: 0.02506 | Regression loss: 0.12416 | Running loss: 0.35338\n",
            "Epoch: 6 | Iteration: 277 | Classification loss: 0.00577 | Regression loss: 0.01788 | Running loss: 0.35300\n",
            "Epoch: 6 | Iteration: 278 | Classification loss: 0.22080 | Regression loss: 0.20418 | Running loss: 0.35302\n",
            "Epoch: 6 | Iteration: 279 | Classification loss: 0.11367 | Regression loss: 0.40334 | Running loss: 0.35255\n",
            "Epoch: 6 | Iteration: 280 | Classification loss: 0.00010 | Regression loss: 0.03181 | Running loss: 0.35224\n",
            "Epoch: 6 | Iteration: 281 | Classification loss: 0.00009 | Regression loss: 0.03299 | Running loss: 0.35180\n",
            "Epoch: 6 | Iteration: 282 | Classification loss: 0.12455 | Regression loss: 0.33066 | Running loss: 0.35197\n",
            "Epoch: 6 | Iteration: 283 | Classification loss: 0.04635 | Regression loss: 0.15590 | Running loss: 0.35189\n",
            "Epoch: 6 | Iteration: 284 | Classification loss: 0.17152 | Regression loss: 0.27177 | Running loss: 0.35180\n",
            "Epoch: 6 | Iteration: 285 | Classification loss: 0.09509 | Regression loss: 0.37519 | Running loss: 0.35146\n",
            "Epoch: 6 | Iteration: 286 | Classification loss: 0.06364 | Regression loss: 0.23653 | Running loss: 0.35165\n",
            "Epoch: 6 | Iteration: 287 | Classification loss: 0.10804 | Regression loss: 0.33512 | Running loss: 0.35157\n",
            "Epoch: 6 | Iteration: 288 | Classification loss: 0.15129 | Regression loss: 0.20308 | Running loss: 0.35189\n",
            "Epoch: 6 | Iteration: 289 | Classification loss: 0.38426 | Regression loss: 0.55009 | Running loss: 0.35226\n",
            "Epoch: 6 | Iteration: 290 | Classification loss: 0.00643 | Regression loss: 0.02674 | Running loss: 0.35223\n",
            "Epoch: 6 | Iteration: 291 | Classification loss: 0.00009 | Regression loss: 0.00809 | Running loss: 0.35113\n",
            "Epoch: 6 | Iteration: 292 | Classification loss: 0.21073 | Regression loss: 0.48303 | Running loss: 0.35160\n",
            "Epoch: 6 | Iteration: 293 | Classification loss: 0.00006 | Regression loss: 0.05410 | Running loss: 0.35145\n",
            "Epoch: 6 | Iteration: 294 | Classification loss: 0.02420 | Regression loss: 0.16549 | Running loss: 0.35119\n",
            "Epoch: 6 | Iteration: 295 | Classification loss: 0.13181 | Regression loss: 0.39705 | Running loss: 0.35151\n",
            "Epoch: 6 | Iteration: 296 | Classification loss: 0.10199 | Regression loss: 0.37922 | Running loss: 0.35199\n",
            "Epoch: 6 | Iteration: 297 | Classification loss: 0.28093 | Regression loss: 0.63631 | Running loss: 0.35329\n",
            "Epoch: 6 | Iteration: 298 | Classification loss: 0.02965 | Regression loss: 0.13490 | Running loss: 0.35315\n",
            "Epoch: 6 | Iteration: 299 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.35254\n",
            "Epoch: 6 | Iteration: 300 | Classification loss: 0.05751 | Regression loss: 0.12022 | Running loss: 0.35167\n",
            "Epoch: 6 | Iteration: 301 | Classification loss: 0.15366 | Regression loss: 0.38971 | Running loss: 0.35275\n",
            "Epoch: 6 | Iteration: 302 | Classification loss: 0.04672 | Regression loss: 0.17564 | Running loss: 0.35159\n",
            "Epoch: 6 | Iteration: 303 | Classification loss: 0.00019 | Regression loss: 0.01325 | Running loss: 0.35155\n",
            "Epoch: 6 | Iteration: 304 | Classification loss: 0.15656 | Regression loss: 0.41182 | Running loss: 0.35131\n",
            "Epoch: 6 | Iteration: 305 | Classification loss: 0.06034 | Regression loss: 0.18131 | Running loss: 0.35079\n",
            "Epoch: 6 | Iteration: 306 | Classification loss: 0.38610 | Regression loss: 0.46613 | Running loss: 0.35041\n",
            "Epoch: 6 | Iteration: 307 | Classification loss: 0.04826 | Regression loss: 0.16083 | Running loss: 0.34971\n",
            "Epoch: 6 | Iteration: 308 | Classification loss: 0.09817 | Regression loss: 0.23220 | Running loss: 0.34921\n",
            "Epoch: 6 | Iteration: 309 | Classification loss: 0.08657 | Regression loss: 0.31542 | Running loss: 0.34837\n",
            "Epoch: 6 | Iteration: 310 | Classification loss: 0.08060 | Regression loss: 0.32394 | Running loss: 0.34669\n",
            "Epoch: 6 | Iteration: 311 | Classification loss: 0.02105 | Regression loss: 0.16290 | Running loss: 0.34692\n",
            "Epoch: 6 | Iteration: 312 | Classification loss: 0.00016 | Regression loss: 0.06561 | Running loss: 0.34520\n",
            "Epoch: 6 | Iteration: 313 | Classification loss: 0.00122 | Regression loss: 0.04263 | Running loss: 0.34461\n",
            "Epoch: 6 | Iteration: 314 | Classification loss: 0.00097 | Regression loss: 0.03063 | Running loss: 0.34129\n",
            "Epoch: 6 | Iteration: 315 | Classification loss: 0.14129 | Regression loss: 0.35761 | Running loss: 0.34197\n",
            "Epoch: 6 | Iteration: 316 | Classification loss: 0.03222 | Regression loss: 0.15991 | Running loss: 0.34152\n",
            "Epoch: 6 | Iteration: 317 | Classification loss: 0.02294 | Regression loss: 0.11645 | Running loss: 0.34075\n",
            "Epoch: 6 | Iteration: 318 | Classification loss: 0.25496 | Regression loss: 0.55357 | Running loss: 0.34107\n",
            "Epoch: 6 | Iteration: 319 | Classification loss: 0.14192 | Regression loss: 0.21479 | Running loss: 0.34092\n",
            "Epoch: 6 | Iteration: 320 | Classification loss: 0.10819 | Regression loss: 0.33979 | Running loss: 0.34110\n",
            "Epoch: 6 | Iteration: 321 | Classification loss: 0.13875 | Regression loss: 0.32884 | Running loss: 0.34117\n",
            "Epoch: 6 | Iteration: 322 | Classification loss: 0.13133 | Regression loss: 0.33955 | Running loss: 0.34161\n",
            "Epoch: 6 | Iteration: 323 | Classification loss: 0.13304 | Regression loss: 0.40496 | Running loss: 0.34215\n",
            "Epoch: 6 | Iteration: 324 | Classification loss: 0.11502 | Regression loss: 0.43954 | Running loss: 0.34152\n",
            "Epoch: 6 | Iteration: 325 | Classification loss: 0.02137 | Regression loss: 0.24864 | Running loss: 0.34121\n",
            "Epoch: 6 | Iteration: 326 | Classification loss: 0.06322 | Regression loss: 0.26959 | Running loss: 0.34187\n",
            "Epoch: 6 | Iteration: 327 | Classification loss: 0.10590 | Regression loss: 0.43620 | Running loss: 0.34206\n",
            "Epoch: 6 | Iteration: 328 | Classification loss: 0.05735 | Regression loss: 0.28753 | Running loss: 0.34108\n",
            "Epoch: 6 | Iteration: 329 | Classification loss: 0.00997 | Regression loss: 0.07186 | Running loss: 0.34115\n",
            "Epoch: 6 | Iteration: 330 | Classification loss: 0.04363 | Regression loss: 0.14920 | Running loss: 0.33924\n",
            "Epoch: 6 | Iteration: 331 | Classification loss: 0.00192 | Regression loss: 0.01860 | Running loss: 0.33929\n",
            "Epoch: 6 | Iteration: 332 | Classification loss: 0.10680 | Regression loss: 0.46578 | Running loss: 0.34042\n",
            "Epoch: 6 | Iteration: 333 | Classification loss: 0.13734 | Regression loss: 0.39333 | Running loss: 0.34141\n",
            "Epoch: 6 | Iteration: 334 | Classification loss: 0.22762 | Regression loss: 0.28334 | Running loss: 0.34101\n",
            "Epoch: 6 | Iteration: 335 | Classification loss: 0.05502 | Regression loss: 0.04708 | Running loss: 0.33945\n",
            "Epoch: 6 | Iteration: 336 | Classification loss: 0.05044 | Regression loss: 0.22600 | Running loss: 0.33897\n",
            "Epoch: 6 | Iteration: 337 | Classification loss: 0.33764 | Regression loss: 0.55526 | Running loss: 0.33944\n",
            "Epoch: 6 | Iteration: 338 | Classification loss: 0.04807 | Regression loss: 0.17745 | Running loss: 0.33877\n",
            "Epoch: 6 | Iteration: 339 | Classification loss: 0.07322 | Regression loss: 0.35863 | Running loss: 0.33868\n",
            "Epoch: 6 | Iteration: 340 | Classification loss: 0.21010 | Regression loss: 0.35374 | Running loss: 0.33830\n",
            "Epoch: 6 | Iteration: 341 | Classification loss: 0.03164 | Regression loss: 0.21864 | Running loss: 0.33772\n",
            "Epoch: 6 | Iteration: 342 | Classification loss: 0.00059 | Regression loss: 0.06879 | Running loss: 0.33683\n",
            "Epoch: 6 | Iteration: 343 | Classification loss: 0.06357 | Regression loss: 0.17952 | Running loss: 0.33632\n",
            "Epoch: 6 | Iteration: 344 | Classification loss: 0.15812 | Regression loss: 0.49784 | Running loss: 0.33608\n",
            "Epoch: 6 | Iteration: 345 | Classification loss: 0.03707 | Regression loss: 0.11033 | Running loss: 0.33577\n",
            "Epoch: 6 | Iteration: 346 | Classification loss: 0.06161 | Regression loss: 0.16882 | Running loss: 0.33557\n",
            "Epoch: 6 | Iteration: 347 | Classification loss: 0.04263 | Regression loss: 0.18566 | Running loss: 0.33576\n",
            "Epoch: 6 | Iteration: 348 | Classification loss: 0.05344 | Regression loss: 0.30631 | Running loss: 0.33614\n",
            "Epoch: 6 | Iteration: 349 | Classification loss: 0.12285 | Regression loss: 0.41311 | Running loss: 0.33673\n",
            "Epoch: 6 | Iteration: 350 | Classification loss: 0.06068 | Regression loss: 0.27941 | Running loss: 0.33615\n",
            "Epoch: 6 | Iteration: 351 | Classification loss: 0.03786 | Regression loss: 0.19874 | Running loss: 0.33505\n",
            "Epoch: 6 | Iteration: 352 | Classification loss: 0.06498 | Regression loss: 0.22447 | Running loss: 0.33555\n",
            "Epoch: 6 | Iteration: 353 | Classification loss: 0.00104 | Regression loss: 0.03622 | Running loss: 0.33459\n",
            "Epoch: 6 | Iteration: 354 | Classification loss: 0.07539 | Regression loss: 0.35384 | Running loss: 0.33446\n",
            "Epoch: 6 | Iteration: 355 | Classification loss: 0.08268 | Regression loss: 0.27519 | Running loss: 0.33427\n",
            "Epoch: 6 | Iteration: 356 | Classification loss: 0.02624 | Regression loss: 0.09470 | Running loss: 0.33369\n",
            "Epoch: 6 | Iteration: 357 | Classification loss: 0.03447 | Regression loss: 0.19932 | Running loss: 0.33393\n",
            "Epoch: 6 | Iteration: 358 | Classification loss: 0.05478 | Regression loss: 0.25827 | Running loss: 0.33412\n",
            "Epoch: 6 | Iteration: 359 | Classification loss: 0.14846 | Regression loss: 0.32735 | Running loss: 0.33422\n",
            "Epoch: 6 | Iteration: 360 | Classification loss: 0.17025 | Regression loss: 0.38865 | Running loss: 0.33504\n",
            "Epoch: 6 | Iteration: 361 | Classification loss: 0.00024 | Regression loss: 0.04975 | Running loss: 0.33433\n",
            "Epoch: 6 | Iteration: 362 | Classification loss: 0.04605 | Regression loss: 0.17095 | Running loss: 0.33473\n",
            "Epoch: 6 | Iteration: 363 | Classification loss: 0.12428 | Regression loss: 0.35739 | Running loss: 0.33489\n",
            "Epoch: 6 | Iteration: 364 | Classification loss: 0.05627 | Regression loss: 0.22724 | Running loss: 0.33479\n",
            "Epoch: 6 | Iteration: 365 | Classification loss: 0.06061 | Regression loss: 0.13129 | Running loss: 0.33487\n",
            "Epoch: 6 | Iteration: 366 | Classification loss: 0.01803 | Regression loss: 0.09461 | Running loss: 0.33502\n",
            "Epoch: 6 | Iteration: 367 | Classification loss: 0.09774 | Regression loss: 0.33911 | Running loss: 0.33494\n",
            "Epoch: 6 | Iteration: 368 | Classification loss: 0.05227 | Regression loss: 0.22471 | Running loss: 0.33509\n",
            "Epoch: 6 | Iteration: 369 | Classification loss: 0.00025 | Regression loss: 0.01340 | Running loss: 0.33380\n",
            "Epoch: 6 | Iteration: 370 | Classification loss: 0.10509 | Regression loss: 0.21581 | Running loss: 0.33297\n",
            "Epoch: 6 | Iteration: 371 | Classification loss: 0.19846 | Regression loss: 0.33068 | Running loss: 0.33247\n",
            "Epoch: 6 | Iteration: 372 | Classification loss: 0.11931 | Regression loss: 0.34183 | Running loss: 0.33209\n",
            "Epoch: 6 | Iteration: 373 | Classification loss: 0.04117 | Regression loss: 0.17135 | Running loss: 0.33101\n",
            "Epoch: 6 | Iteration: 374 | Classification loss: 0.00021 | Regression loss: 0.04350 | Running loss: 0.32967\n",
            "Epoch: 6 | Iteration: 375 | Classification loss: 0.32321 | Regression loss: 0.50309 | Running loss: 0.33125\n",
            "Epoch: 6 | Iteration: 376 | Classification loss: 0.09300 | Regression loss: 0.31189 | Running loss: 0.33111\n",
            "Epoch: 6 | Iteration: 377 | Classification loss: 0.00353 | Regression loss: 0.04053 | Running loss: 0.33035\n",
            "Epoch: 6 | Iteration: 378 | Classification loss: 0.15031 | Regression loss: 0.41392 | Running loss: 0.33004\n",
            "Epoch: 6 | Iteration: 379 | Classification loss: 0.00004 | Regression loss: 0.00000 | Running loss: 0.32998\n",
            "Epoch: 6 | Iteration: 380 | Classification loss: 0.11490 | Regression loss: 0.29797 | Running loss: 0.33078\n",
            "Epoch: 6 | Iteration: 381 | Classification loss: 0.17364 | Regression loss: 0.35564 | Running loss: 0.33177\n",
            "Epoch: 6 | Iteration: 382 | Classification loss: 0.21541 | Regression loss: 0.42620 | Running loss: 0.33208\n",
            "Epoch: 6 | Iteration: 383 | Classification loss: 0.14773 | Regression loss: 0.31378 | Running loss: 0.33218\n",
            "Epoch: 6 | Iteration: 384 | Classification loss: 0.11652 | Regression loss: 0.20677 | Running loss: 0.33140\n",
            "Epoch: 6 | Iteration: 385 | Classification loss: 0.00006 | Regression loss: 0.05657 | Running loss: 0.33058\n",
            "Epoch: 6 | Iteration: 386 | Classification loss: 0.03786 | Regression loss: 0.10763 | Running loss: 0.32937\n",
            "Epoch: 6 | Iteration: 387 | Classification loss: 0.03873 | Regression loss: 0.09396 | Running loss: 0.32957\n",
            "Epoch: 6 | Iteration: 388 | Classification loss: 0.06790 | Regression loss: 0.19602 | Running loss: 0.32945\n",
            "Epoch: 6 | Iteration: 389 | Classification loss: 0.00047 | Regression loss: 0.03050 | Running loss: 0.32850\n",
            "Epoch: 6 | Iteration: 390 | Classification loss: 0.09367 | Regression loss: 0.22271 | Running loss: 0.32846\n",
            "Epoch: 6 | Iteration: 391 | Classification loss: 0.09090 | Regression loss: 0.28292 | Running loss: 0.32885\n",
            "Epoch: 6 | Iteration: 392 | Classification loss: 0.04001 | Regression loss: 0.19127 | Running loss: 0.32823\n",
            "Epoch: 6 | Iteration: 393 | Classification loss: 0.17387 | Regression loss: 0.46913 | Running loss: 0.32860\n",
            "Epoch: 6 | Iteration: 394 | Classification loss: 0.00022 | Regression loss: 0.04694 | Running loss: 0.32796\n",
            "Epoch: 6 | Iteration: 395 | Classification loss: 0.00009 | Regression loss: 0.00933 | Running loss: 0.32591\n",
            "Epoch: 6 | Iteration: 396 | Classification loss: 0.14104 | Regression loss: 0.77389 | Running loss: 0.32691\n",
            "Epoch: 6 | Iteration: 397 | Classification loss: 0.14871 | Regression loss: 0.45167 | Running loss: 0.32678\n",
            "Epoch: 6 | Iteration: 398 | Classification loss: 0.00018 | Regression loss: 0.01707 | Running loss: 0.32643\n",
            "Epoch: 6 | Iteration: 399 | Classification loss: 0.04591 | Regression loss: 0.28708 | Running loss: 0.32641\n",
            "Epoch: 6 | Iteration: 400 | Classification loss: 0.10289 | Regression loss: 0.32076 | Running loss: 0.32644\n",
            "Epoch: 6 | Iteration: 401 | Classification loss: 0.05148 | Regression loss: 0.30571 | Running loss: 0.32576\n",
            "Epoch: 6 | Iteration: 402 | Classification loss: 0.03197 | Regression loss: 0.03254 | Running loss: 0.32519\n",
            "Epoch: 6 | Iteration: 403 | Classification loss: 0.05574 | Regression loss: 0.26910 | Running loss: 0.32443\n",
            "Epoch: 6 | Iteration: 404 | Classification loss: 0.09629 | Regression loss: 0.30751 | Running loss: 0.32478\n",
            "Epoch: 6 | Iteration: 405 | Classification loss: 0.06601 | Regression loss: 0.28610 | Running loss: 0.32448\n",
            "Epoch: 6 | Iteration: 406 | Classification loss: 0.05656 | Regression loss: 0.23283 | Running loss: 0.32454\n",
            "Epoch: 6 | Iteration: 407 | Classification loss: 0.06408 | Regression loss: 0.33058 | Running loss: 0.32497\n",
            "Epoch: 6 | Iteration: 408 | Classification loss: 0.00028 | Regression loss: 0.05223 | Running loss: 0.32396\n",
            "Epoch: 6 | Iteration: 409 | Classification loss: 0.00010 | Regression loss: 0.02872 | Running loss: 0.32313\n",
            "Epoch: 6 | Iteration: 410 | Classification loss: 0.06450 | Regression loss: 0.27634 | Running loss: 0.32312\n",
            "Epoch: 6 | Iteration: 411 | Classification loss: 0.06939 | Regression loss: 0.22052 | Running loss: 0.32325\n",
            "Epoch: 6 | Iteration: 412 | Classification loss: 0.05297 | Regression loss: 0.22693 | Running loss: 0.32285\n",
            "Epoch: 6 | Iteration: 413 | Classification loss: 0.07239 | Regression loss: 0.24934 | Running loss: 0.32279\n",
            "Epoch: 6 | Iteration: 414 | Classification loss: 0.09637 | Regression loss: 0.39567 | Running loss: 0.32306\n",
            "Epoch: 6 | Iteration: 415 | Classification loss: 0.07060 | Regression loss: 0.30330 | Running loss: 0.32265\n",
            "Epoch: 6 | Iteration: 416 | Classification loss: 0.04186 | Regression loss: 0.22979 | Running loss: 0.32258\n",
            "Epoch: 6 | Iteration: 417 | Classification loss: 0.21153 | Regression loss: 0.55984 | Running loss: 0.32410\n",
            "Epoch: 6 | Iteration: 418 | Classification loss: 0.23111 | Regression loss: 0.54272 | Running loss: 0.32504\n",
            "Epoch: 6 | Iteration: 419 | Classification loss: 0.09653 | Regression loss: 0.26022 | Running loss: 0.32461\n",
            "Epoch: 6 | Iteration: 420 | Classification loss: 0.11154 | Regression loss: 0.35852 | Running loss: 0.32483\n",
            "Epoch: 6 | Iteration: 421 | Classification loss: 0.00087 | Regression loss: 0.14205 | Running loss: 0.32458\n",
            "Epoch: 6 | Iteration: 422 | Classification loss: 0.19456 | Regression loss: 0.49089 | Running loss: 0.32562\n",
            "Epoch: 6 | Iteration: 423 | Classification loss: 0.00016 | Regression loss: 0.02033 | Running loss: 0.32464\n",
            "Epoch: 6 | Iteration: 424 | Classification loss: 0.15258 | Regression loss: 0.16696 | Running loss: 0.32497\n",
            "Epoch: 6 | Iteration: 425 | Classification loss: 0.12703 | Regression loss: 0.34351 | Running loss: 0.32541\n",
            "Epoch: 6 | Iteration: 426 | Classification loss: 0.25514 | Regression loss: 0.16673 | Running loss: 0.32591\n",
            "Epoch: 6 | Iteration: 427 | Classification loss: 0.15074 | Regression loss: 0.45020 | Running loss: 0.32705\n",
            "Epoch: 6 | Iteration: 428 | Classification loss: 0.00019 | Regression loss: 0.04232 | Running loss: 0.32638\n",
            "Epoch: 6 | Iteration: 429 | Classification loss: 0.02278 | Regression loss: 0.14114 | Running loss: 0.32545\n",
            "Epoch: 6 | Iteration: 430 | Classification loss: 0.08215 | Regression loss: 0.33635 | Running loss: 0.32621\n",
            "Epoch: 6 | Iteration: 431 | Classification loss: 0.21169 | Regression loss: 0.49201 | Running loss: 0.32755\n",
            "Epoch: 6 | Iteration: 432 | Classification loss: 0.11935 | Regression loss: 0.23338 | Running loss: 0.32663\n",
            "Epoch: 6 | Iteration: 433 | Classification loss: 0.03737 | Regression loss: 0.15101 | Running loss: 0.32600\n",
            "Epoch: 6 | Iteration: 434 | Classification loss: 0.04368 | Regression loss: 0.20890 | Running loss: 0.32642\n",
            "Epoch: 6 | Iteration: 435 | Classification loss: 0.08021 | Regression loss: 0.17438 | Running loss: 0.32612\n",
            "Epoch: 6 | Iteration: 436 | Classification loss: 0.08044 | Regression loss: 0.36697 | Running loss: 0.32668\n",
            "Epoch: 6 | Iteration: 437 | Classification loss: 0.05288 | Regression loss: 0.28193 | Running loss: 0.32727\n",
            "Epoch: 6 | Iteration: 438 | Classification loss: 0.32293 | Regression loss: 0.52842 | Running loss: 0.32893\n",
            "Epoch: 6 | Iteration: 439 | Classification loss: 0.11834 | Regression loss: 0.37273 | Running loss: 0.32922\n",
            "Epoch: 6 | Iteration: 440 | Classification loss: 0.02484 | Regression loss: 0.15098 | Running loss: 0.32905\n",
            "Epoch: 6 | Iteration: 441 | Classification loss: 0.19436 | Regression loss: 0.40067 | Running loss: 0.33012\n",
            "Epoch: 6 | Iteration: 442 | Classification loss: 0.03944 | Regression loss: 0.14277 | Running loss: 0.33010\n",
            "Epoch: 6 | Iteration: 443 | Classification loss: 0.23183 | Regression loss: 0.50125 | Running loss: 0.33014\n",
            "Epoch: 6 | Iteration: 444 | Classification loss: 0.25502 | Regression loss: 0.47224 | Running loss: 0.33084\n",
            "Epoch: 6 | Iteration: 445 | Classification loss: 0.00025 | Regression loss: 0.01861 | Running loss: 0.33050\n",
            "Epoch: 6 | Iteration: 446 | Classification loss: 0.05968 | Regression loss: 0.16069 | Running loss: 0.33033\n",
            "Epoch: 6 | Iteration: 447 | Classification loss: 0.05516 | Regression loss: 0.20553 | Running loss: 0.33030\n",
            "Epoch: 6 | Iteration: 448 | Classification loss: 0.18753 | Regression loss: 0.35360 | Running loss: 0.33027\n",
            "Epoch: 6 | Iteration: 449 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.33027\n",
            "Epoch: 6 | Iteration: 450 | Classification loss: 0.00017 | Regression loss: 0.03284 | Running loss: 0.32966\n",
            "Epoch: 6 | Iteration: 451 | Classification loss: 0.09244 | Regression loss: 0.30135 | Running loss: 0.32904\n",
            "Epoch: 6 | Iteration: 452 | Classification loss: 0.28942 | Regression loss: 0.52688 | Running loss: 0.32979\n",
            "Epoch: 6 | Iteration: 453 | Classification loss: 0.08742 | Regression loss: 0.16060 | Running loss: 0.32951\n",
            "Epoch: 6 | Iteration: 454 | Classification loss: 0.02965 | Regression loss: 0.19505 | Running loss: 0.32894\n",
            "Epoch: 6 | Iteration: 455 | Classification loss: 0.05355 | Regression loss: 0.20300 | Running loss: 0.32850\n",
            "Epoch: 6 | Iteration: 456 | Classification loss: 0.03010 | Regression loss: 0.11847 | Running loss: 0.32832\n",
            "Epoch: 6 | Iteration: 457 | Classification loss: 0.01807 | Regression loss: 0.13436 | Running loss: 0.32732\n",
            "Epoch: 6 | Iteration: 458 | Classification loss: 0.00024 | Regression loss: 0.02036 | Running loss: 0.32640\n",
            "Epoch: 6 | Iteration: 459 | Classification loss: 0.18448 | Regression loss: 0.46392 | Running loss: 0.32683\n",
            "Epoch: 6 | Iteration: 460 | Classification loss: 0.09853 | Regression loss: 0.30570 | Running loss: 0.32723\n",
            "Epoch: 6 | Iteration: 461 | Classification loss: 0.17285 | Regression loss: 0.23739 | Running loss: 0.32724\n",
            "Epoch: 6 | Iteration: 462 | Classification loss: 0.26481 | Regression loss: 0.59303 | Running loss: 0.32892\n",
            "Epoch: 6 | Iteration: 463 | Classification loss: 0.01405 | Regression loss: 0.00378 | Running loss: 0.32706\n",
            "Epoch: 6 | Iteration: 464 | Classification loss: 0.07005 | Regression loss: 0.15858 | Running loss: 0.32716\n",
            "Epoch: 6 | Iteration: 465 | Classification loss: 0.06015 | Regression loss: 0.15281 | Running loss: 0.32646\n",
            "Epoch: 6 | Iteration: 466 | Classification loss: 0.14350 | Regression loss: 0.27720 | Running loss: 0.32647\n",
            "Epoch: 6 | Iteration: 467 | Classification loss: 0.04492 | Regression loss: 0.18299 | Running loss: 0.32600\n",
            "Epoch: 6 | Iteration: 468 | Classification loss: 0.05661 | Regression loss: 0.19400 | Running loss: 0.32537\n",
            "Epoch: 6 | Iteration: 469 | Classification loss: 0.02701 | Regression loss: 0.10370 | Running loss: 0.32516\n",
            "Epoch: 6 | Iteration: 470 | Classification loss: 0.15088 | Regression loss: 0.30143 | Running loss: 0.32543\n",
            "Epoch: 6 | Iteration: 471 | Classification loss: 0.10482 | Regression loss: 0.27178 | Running loss: 0.32531\n",
            "Epoch: 6 | Iteration: 472 | Classification loss: 0.09881 | Regression loss: 0.27373 | Running loss: 0.32562\n",
            "Epoch: 6 | Iteration: 473 | Classification loss: 0.10205 | Regression loss: 0.02996 | Running loss: 0.32581\n",
            "Epoch: 6 | Iteration: 474 | Classification loss: 0.09079 | Regression loss: 0.25253 | Running loss: 0.32575\n",
            "Epoch: 6 | Iteration: 475 | Classification loss: 0.12187 | Regression loss: 0.25871 | Running loss: 0.32649\n",
            "Epoch: 6 | Iteration: 476 | Classification loss: 0.02776 | Regression loss: 0.18950 | Running loss: 0.32588\n",
            "Epoch: 6 | Iteration: 477 | Classification loss: 0.02045 | Regression loss: 0.09524 | Running loss: 0.32561\n",
            "Epoch: 6 | Iteration: 478 | Classification loss: 0.16303 | Regression loss: 0.47619 | Running loss: 0.32612\n",
            "Epoch: 6 | Iteration: 479 | Classification loss: 0.17082 | Regression loss: 0.42594 | Running loss: 0.32725\n",
            "Epoch: 6 | Iteration: 480 | Classification loss: 0.10383 | Regression loss: 0.10424 | Running loss: 0.32765\n",
            "Epoch: 6 | Iteration: 481 | Classification loss: 0.00008 | Regression loss: 0.00000 | Running loss: 0.32723\n",
            "Epoch: 6 | Iteration: 482 | Classification loss: 0.11917 | Regression loss: 0.33907 | Running loss: 0.32814\n",
            "Epoch: 6 | Iteration: 483 | Classification loss: 0.14360 | Regression loss: 0.32278 | Running loss: 0.32785\n",
            "Epoch: 6 | Iteration: 484 | Classification loss: 0.16870 | Regression loss: 0.50668 | Running loss: 0.32919\n",
            "Epoch: 6 | Iteration: 485 | Classification loss: 0.27818 | Regression loss: 0.48072 | Running loss: 0.32940\n",
            "Epoch: 6 | Iteration: 486 | Classification loss: 0.24714 | Regression loss: 0.06621 | Running loss: 0.32869\n",
            "Epoch: 6 | Iteration: 487 | Classification loss: 0.15355 | Regression loss: 0.38994 | Running loss: 0.32900\n",
            "Epoch: 6 | Iteration: 488 | Classification loss: 0.05685 | Regression loss: 0.23591 | Running loss: 0.32901\n",
            "Epoch: 6 | Iteration: 489 | Classification loss: 0.00711 | Regression loss: 0.06157 | Running loss: 0.32904\n",
            "Epoch: 6 | Iteration: 490 | Classification loss: 0.19101 | Regression loss: 0.43541 | Running loss: 0.32957\n",
            "Epoch: 6 | Iteration: 491 | Classification loss: 0.15239 | Regression loss: 0.41187 | Running loss: 0.33038\n",
            "Epoch: 6 | Iteration: 492 | Classification loss: 0.13971 | Regression loss: 0.39765 | Running loss: 0.33018\n",
            "Epoch: 6 | Iteration: 493 | Classification loss: 0.09101 | Regression loss: 0.23618 | Running loss: 0.33039\n",
            "Epoch: 6 | Iteration: 494 | Classification loss: 0.07459 | Regression loss: 0.37187 | Running loss: 0.33128\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.16s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.40s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.551\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.873\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.612\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.460\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.622\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.629\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 7 | Iteration: 0 | Classification loss: 0.03518 | Regression loss: 0.15573 | Running loss: 0.33125\n",
            "Epoch: 7 | Iteration: 1 | Classification loss: 0.12228 | Regression loss: 0.34054 | Running loss: 0.33089\n",
            "Epoch: 7 | Iteration: 2 | Classification loss: 0.02609 | Regression loss: 0.13775 | Running loss: 0.33112\n",
            "Epoch: 7 | Iteration: 3 | Classification loss: 0.13799 | Regression loss: 0.32733 | Running loss: 0.33120\n",
            "Epoch: 7 | Iteration: 4 | Classification loss: 0.07545 | Regression loss: 0.32468 | Running loss: 0.33088\n",
            "Epoch: 7 | Iteration: 5 | Classification loss: 0.03544 | Regression loss: 0.18372 | Running loss: 0.33033\n",
            "Epoch: 7 | Iteration: 6 | Classification loss: 0.06923 | Regression loss: 0.27125 | Running loss: 0.33010\n",
            "Epoch: 7 | Iteration: 7 | Classification loss: 0.16559 | Regression loss: 0.40454 | Running loss: 0.32985\n",
            "Epoch: 7 | Iteration: 8 | Classification loss: 0.00090 | Regression loss: 0.03104 | Running loss: 0.32912\n",
            "Epoch: 7 | Iteration: 9 | Classification loss: 0.08507 | Regression loss: 0.23567 | Running loss: 0.32881\n",
            "Epoch: 7 | Iteration: 10 | Classification loss: 0.06265 | Regression loss: 0.24845 | Running loss: 0.32910\n",
            "Epoch: 7 | Iteration: 11 | Classification loss: 0.12467 | Regression loss: 0.42062 | Running loss: 0.32978\n",
            "Epoch: 7 | Iteration: 12 | Classification loss: 0.08612 | Regression loss: 0.22930 | Running loss: 0.32889\n",
            "Epoch: 7 | Iteration: 13 | Classification loss: 0.13803 | Regression loss: 0.41974 | Running loss: 0.32997\n",
            "Epoch: 7 | Iteration: 14 | Classification loss: 0.17463 | Regression loss: 0.42241 | Running loss: 0.32976\n",
            "Epoch: 7 | Iteration: 15 | Classification loss: 0.07387 | Regression loss: 0.28103 | Running loss: 0.33044\n",
            "Epoch: 7 | Iteration: 16 | Classification loss: 0.00153 | Regression loss: 0.01599 | Running loss: 0.32910\n",
            "Epoch: 7 | Iteration: 17 | Classification loss: 0.04302 | Regression loss: 0.16963 | Running loss: 0.32830\n",
            "Epoch: 7 | Iteration: 18 | Classification loss: 0.03238 | Regression loss: 0.12710 | Running loss: 0.32823\n",
            "Epoch: 7 | Iteration: 19 | Classification loss: 0.03266 | Regression loss: 0.13390 | Running loss: 0.32782\n",
            "Epoch: 7 | Iteration: 20 | Classification loss: 0.03108 | Regression loss: 0.11311 | Running loss: 0.32800\n",
            "Epoch: 7 | Iteration: 21 | Classification loss: 0.17756 | Regression loss: 0.34702 | Running loss: 0.32898\n",
            "Epoch: 7 | Iteration: 22 | Classification loss: 0.09268 | Regression loss: 0.31174 | Running loss: 0.32884\n",
            "Epoch: 7 | Iteration: 23 | Classification loss: 0.07174 | Regression loss: 0.24342 | Running loss: 0.32947\n",
            "Epoch: 7 | Iteration: 24 | Classification loss: 0.08377 | Regression loss: 0.21013 | Running loss: 0.32912\n",
            "Epoch: 7 | Iteration: 25 | Classification loss: 0.07673 | Regression loss: 0.37473 | Running loss: 0.32967\n",
            "Epoch: 7 | Iteration: 26 | Classification loss: 0.00334 | Regression loss: 0.01260 | Running loss: 0.32940\n",
            "Epoch: 7 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.32873\n",
            "Epoch: 7 | Iteration: 28 | Classification loss: 0.00052 | Regression loss: 0.02642 | Running loss: 0.32758\n",
            "Epoch: 7 | Iteration: 29 | Classification loss: 0.12088 | Regression loss: 0.47554 | Running loss: 0.32791\n",
            "Epoch: 7 | Iteration: 30 | Classification loss: 0.01829 | Regression loss: 0.09862 | Running loss: 0.32757\n",
            "Epoch: 7 | Iteration: 31 | Classification loss: 0.10987 | Regression loss: 0.24956 | Running loss: 0.32723\n",
            "Epoch: 7 | Iteration: 32 | Classification loss: 0.05934 | Regression loss: 0.27421 | Running loss: 0.32747\n",
            "Epoch: 7 | Iteration: 33 | Classification loss: 0.07836 | Regression loss: 0.27298 | Running loss: 0.32691\n",
            "Epoch: 7 | Iteration: 34 | Classification loss: 0.02270 | Regression loss: 0.11512 | Running loss: 0.32562\n",
            "Epoch: 7 | Iteration: 35 | Classification loss: 0.07784 | Regression loss: 0.20405 | Running loss: 0.32543\n",
            "Epoch: 7 | Iteration: 36 | Classification loss: 0.00065 | Regression loss: 0.04551 | Running loss: 0.32525\n",
            "Epoch: 7 | Iteration: 37 | Classification loss: 0.00071 | Regression loss: 0.00000 | Running loss: 0.32472\n",
            "Epoch: 7 | Iteration: 38 | Classification loss: 0.12522 | Regression loss: 0.33611 | Running loss: 0.32564\n",
            "Epoch: 7 | Iteration: 39 | Classification loss: 0.03206 | Regression loss: 0.10663 | Running loss: 0.32561\n",
            "Epoch: 7 | Iteration: 40 | Classification loss: 0.08914 | Regression loss: 0.25142 | Running loss: 0.32522\n",
            "Epoch: 7 | Iteration: 41 | Classification loss: 0.11026 | Regression loss: 0.33470 | Running loss: 0.32485\n",
            "Epoch: 7 | Iteration: 42 | Classification loss: 0.00031 | Regression loss: 0.04529 | Running loss: 0.32433\n",
            "Epoch: 7 | Iteration: 43 | Classification loss: 0.02137 | Regression loss: 0.15368 | Running loss: 0.32460\n",
            "Epoch: 7 | Iteration: 44 | Classification loss: 0.00094 | Regression loss: 0.00919 | Running loss: 0.32424\n",
            "Epoch: 7 | Iteration: 45 | Classification loss: 0.02802 | Regression loss: 0.15261 | Running loss: 0.32456\n",
            "Epoch: 7 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.32351\n",
            "Epoch: 7 | Iteration: 47 | Classification loss: 0.05942 | Regression loss: 0.15312 | Running loss: 0.32336\n",
            "Epoch: 7 | Iteration: 48 | Classification loss: 0.27253 | Regression loss: 0.25882 | Running loss: 0.32439\n",
            "Epoch: 7 | Iteration: 49 | Classification loss: 0.20241 | Regression loss: 0.47218 | Running loss: 0.32572\n",
            "Epoch: 7 | Iteration: 50 | Classification loss: 0.01475 | Regression loss: 0.13375 | Running loss: 0.32513\n",
            "Epoch: 7 | Iteration: 51 | Classification loss: 0.02561 | Regression loss: 0.20724 | Running loss: 0.32537\n",
            "Epoch: 7 | Iteration: 52 | Classification loss: 0.06001 | Regression loss: 0.20216 | Running loss: 0.32590\n",
            "Epoch: 7 | Iteration: 53 | Classification loss: 0.10994 | Regression loss: 0.34888 | Running loss: 0.32670\n",
            "Epoch: 7 | Iteration: 54 | Classification loss: 0.00013 | Regression loss: 0.03342 | Running loss: 0.32635\n",
            "Epoch: 7 | Iteration: 55 | Classification loss: 0.04140 | Regression loss: 0.18465 | Running loss: 0.32637\n",
            "Epoch: 7 | Iteration: 56 | Classification loss: 0.11978 | Regression loss: 0.26914 | Running loss: 0.32627\n",
            "Epoch: 7 | Iteration: 57 | Classification loss: 0.07717 | Regression loss: 0.25314 | Running loss: 0.32589\n",
            "Epoch: 7 | Iteration: 58 | Classification loss: 0.04622 | Regression loss: 0.26293 | Running loss: 0.32631\n",
            "Epoch: 7 | Iteration: 59 | Classification loss: 0.07709 | Regression loss: 0.23573 | Running loss: 0.32694\n",
            "Epoch: 7 | Iteration: 60 | Classification loss: 0.04231 | Regression loss: 0.26599 | Running loss: 0.32640\n",
            "Epoch: 7 | Iteration: 61 | Classification loss: 0.00021 | Regression loss: 0.10818 | Running loss: 0.32615\n",
            "Epoch: 7 | Iteration: 62 | Classification loss: 0.06139 | Regression loss: 0.23471 | Running loss: 0.32527\n",
            "Epoch: 7 | Iteration: 63 | Classification loss: 0.02870 | Regression loss: 0.19496 | Running loss: 0.32439\n",
            "Epoch: 7 | Iteration: 64 | Classification loss: 0.05400 | Regression loss: 0.14256 | Running loss: 0.32389\n",
            "Epoch: 7 | Iteration: 65 | Classification loss: 0.02900 | Regression loss: 0.10827 | Running loss: 0.32355\n",
            "Epoch: 7 | Iteration: 66 | Classification loss: 0.18978 | Regression loss: 0.51158 | Running loss: 0.32409\n",
            "Epoch: 7 | Iteration: 67 | Classification loss: 0.00039 | Regression loss: 0.09560 | Running loss: 0.32346\n",
            "Epoch: 7 | Iteration: 68 | Classification loss: 0.10084 | Regression loss: 0.36678 | Running loss: 0.32393\n",
            "Epoch: 7 | Iteration: 69 | Classification loss: 0.05691 | Regression loss: 0.32368 | Running loss: 0.32385\n",
            "Epoch: 7 | Iteration: 70 | Classification loss: 0.06612 | Regression loss: 0.26663 | Running loss: 0.32357\n",
            "Epoch: 7 | Iteration: 71 | Classification loss: 0.00028 | Regression loss: 0.01146 | Running loss: 0.32168\n",
            "Epoch: 7 | Iteration: 72 | Classification loss: 0.09364 | Regression loss: 0.35270 | Running loss: 0.32202\n",
            "Epoch: 7 | Iteration: 73 | Classification loss: 0.03182 | Regression loss: 0.13601 | Running loss: 0.32225\n",
            "Epoch: 7 | Iteration: 74 | Classification loss: 0.11448 | Regression loss: 0.32474 | Running loss: 0.32210\n",
            "Epoch: 7 | Iteration: 75 | Classification loss: 0.03595 | Regression loss: 0.16607 | Running loss: 0.32195\n",
            "Epoch: 7 | Iteration: 76 | Classification loss: 0.11585 | Regression loss: 0.25685 | Running loss: 0.32209\n",
            "Epoch: 7 | Iteration: 77 | Classification loss: 0.13828 | Regression loss: 0.22699 | Running loss: 0.32254\n",
            "Epoch: 7 | Iteration: 78 | Classification loss: 0.04735 | Regression loss: 0.17678 | Running loss: 0.32297\n",
            "Epoch: 7 | Iteration: 79 | Classification loss: 0.00025 | Regression loss: 0.01911 | Running loss: 0.32264\n",
            "Epoch: 7 | Iteration: 80 | Classification loss: 0.15888 | Regression loss: 0.44243 | Running loss: 0.32376\n",
            "Epoch: 7 | Iteration: 81 | Classification loss: 0.04445 | Regression loss: 0.21621 | Running loss: 0.32382\n",
            "Epoch: 7 | Iteration: 82 | Classification loss: 0.03517 | Regression loss: 0.15089 | Running loss: 0.32365\n",
            "Epoch: 7 | Iteration: 83 | Classification loss: 0.07747 | Regression loss: 0.32967 | Running loss: 0.32342\n",
            "Epoch: 7 | Iteration: 84 | Classification loss: 0.02072 | Regression loss: 0.16484 | Running loss: 0.32322\n",
            "Epoch: 7 | Iteration: 85 | Classification loss: 0.00039 | Regression loss: 0.04135 | Running loss: 0.32282\n",
            "Epoch: 7 | Iteration: 86 | Classification loss: 0.01677 | Regression loss: 0.09811 | Running loss: 0.32303\n",
            "Epoch: 7 | Iteration: 87 | Classification loss: 0.01410 | Regression loss: 0.07093 | Running loss: 0.32255\n",
            "Epoch: 7 | Iteration: 88 | Classification loss: 0.04507 | Regression loss: 0.23440 | Running loss: 0.32216\n",
            "Epoch: 7 | Iteration: 89 | Classification loss: 0.00039 | Regression loss: 0.02788 | Running loss: 0.32024\n",
            "Epoch: 7 | Iteration: 90 | Classification loss: 0.08408 | Regression loss: 0.25041 | Running loss: 0.32040\n",
            "Epoch: 7 | Iteration: 91 | Classification loss: 0.00042 | Regression loss: 0.04836 | Running loss: 0.32046\n",
            "Epoch: 7 | Iteration: 92 | Classification loss: 0.24112 | Regression loss: 0.58611 | Running loss: 0.32175\n",
            "Epoch: 7 | Iteration: 93 | Classification loss: 0.15052 | Regression loss: 0.46400 | Running loss: 0.32207\n",
            "Epoch: 7 | Iteration: 94 | Classification loss: 0.06984 | Regression loss: 0.19151 | Running loss: 0.32189\n",
            "Epoch: 7 | Iteration: 95 | Classification loss: 0.03649 | Regression loss: 0.19624 | Running loss: 0.32187\n",
            "Epoch: 7 | Iteration: 96 | Classification loss: 0.12693 | Regression loss: 0.49281 | Running loss: 0.32306\n",
            "Epoch: 7 | Iteration: 97 | Classification loss: 0.07911 | Regression loss: 0.25369 | Running loss: 0.32198\n",
            "Epoch: 7 | Iteration: 98 | Classification loss: 0.06801 | Regression loss: 0.20433 | Running loss: 0.32176\n",
            "Epoch: 7 | Iteration: 99 | Classification loss: 0.03740 | Regression loss: 0.18844 | Running loss: 0.32164\n",
            "Epoch: 7 | Iteration: 100 | Classification loss: 0.00082 | Regression loss: 0.06313 | Running loss: 0.32103\n",
            "Epoch: 7 | Iteration: 101 | Classification loss: 0.05059 | Regression loss: 0.17277 | Running loss: 0.32143\n",
            "Epoch: 7 | Iteration: 102 | Classification loss: 0.03429 | Regression loss: 0.15835 | Running loss: 0.32153\n",
            "Epoch: 7 | Iteration: 103 | Classification loss: 0.00042 | Regression loss: 0.01738 | Running loss: 0.32030\n",
            "Epoch: 7 | Iteration: 104 | Classification loss: 0.05455 | Regression loss: 0.30667 | Running loss: 0.32003\n",
            "Epoch: 7 | Iteration: 105 | Classification loss: 0.05680 | Regression loss: 0.36911 | Running loss: 0.31974\n",
            "Epoch: 7 | Iteration: 106 | Classification loss: 0.03577 | Regression loss: 0.19229 | Running loss: 0.31982\n",
            "Epoch: 7 | Iteration: 107 | Classification loss: 0.07242 | Regression loss: 0.36114 | Running loss: 0.32069\n",
            "Epoch: 7 | Iteration: 108 | Classification loss: 0.05730 | Regression loss: 0.27706 | Running loss: 0.32134\n",
            "Epoch: 7 | Iteration: 109 | Classification loss: 0.05135 | Regression loss: 0.14968 | Running loss: 0.32110\n",
            "Epoch: 7 | Iteration: 110 | Classification loss: 0.08880 | Regression loss: 0.37966 | Running loss: 0.32202\n",
            "Epoch: 7 | Iteration: 111 | Classification loss: 0.15249 | Regression loss: 0.33020 | Running loss: 0.32211\n",
            "Epoch: 7 | Iteration: 112 | Classification loss: 0.14063 | Regression loss: 0.39829 | Running loss: 0.32192\n",
            "Epoch: 7 | Iteration: 113 | Classification loss: 0.00028 | Regression loss: 0.17124 | Running loss: 0.32156\n",
            "Epoch: 7 | Iteration: 114 | Classification loss: 0.06433 | Regression loss: 0.27171 | Running loss: 0.32135\n",
            "Epoch: 7 | Iteration: 115 | Classification loss: 0.08760 | Regression loss: 0.33391 | Running loss: 0.32216\n",
            "Epoch: 7 | Iteration: 116 | Classification loss: 0.18868 | Regression loss: 0.67054 | Running loss: 0.32358\n",
            "Epoch: 7 | Iteration: 117 | Classification loss: 0.02945 | Regression loss: 0.21652 | Running loss: 0.32300\n",
            "Epoch: 7 | Iteration: 118 | Classification loss: 0.00025 | Regression loss: 0.06388 | Running loss: 0.32310\n",
            "Epoch: 7 | Iteration: 119 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.32242\n",
            "Epoch: 7 | Iteration: 120 | Classification loss: 0.04649 | Regression loss: 0.24682 | Running loss: 0.32256\n",
            "Epoch: 7 | Iteration: 121 | Classification loss: 0.20947 | Regression loss: 0.42717 | Running loss: 0.32320\n",
            "Epoch: 7 | Iteration: 122 | Classification loss: 0.10083 | Regression loss: 0.17327 | Running loss: 0.32272\n",
            "Epoch: 7 | Iteration: 123 | Classification loss: 0.07790 | Regression loss: 0.31374 | Running loss: 0.32333\n",
            "Epoch: 7 | Iteration: 124 | Classification loss: 0.10575 | Regression loss: 0.31096 | Running loss: 0.32345\n",
            "Epoch: 7 | Iteration: 125 | Classification loss: 0.25488 | Regression loss: 0.55326 | Running loss: 0.32453\n",
            "Epoch: 7 | Iteration: 126 | Classification loss: 0.00010 | Regression loss: 0.15793 | Running loss: 0.32484\n",
            "Epoch: 7 | Iteration: 127 | Classification loss: 0.00008 | Regression loss: 0.06441 | Running loss: 0.32403\n",
            "Epoch: 7 | Iteration: 128 | Classification loss: 0.04360 | Regression loss: 0.25480 | Running loss: 0.32401\n",
            "Epoch: 7 | Iteration: 129 | Classification loss: 0.02415 | Regression loss: 0.18545 | Running loss: 0.32440\n",
            "Epoch: 7 | Iteration: 130 | Classification loss: 0.08753 | Regression loss: 0.36406 | Running loss: 0.32431\n",
            "Epoch: 7 | Iteration: 131 | Classification loss: 0.04289 | Regression loss: 0.17258 | Running loss: 0.32407\n",
            "Epoch: 7 | Iteration: 132 | Classification loss: 0.11348 | Regression loss: 0.31427 | Running loss: 0.32354\n",
            "Epoch: 7 | Iteration: 133 | Classification loss: 0.08761 | Regression loss: 0.33883 | Running loss: 0.32349\n",
            "Epoch: 7 | Iteration: 134 | Classification loss: 0.05376 | Regression loss: 0.17480 | Running loss: 0.32319\n",
            "Epoch: 7 | Iteration: 135 | Classification loss: 0.00376 | Regression loss: 0.12411 | Running loss: 0.32300\n",
            "Epoch: 7 | Iteration: 136 | Classification loss: 0.05711 | Regression loss: 0.24810 | Running loss: 0.32226\n",
            "Epoch: 7 | Iteration: 137 | Classification loss: 0.23486 | Regression loss: 0.54330 | Running loss: 0.32368\n",
            "Epoch: 7 | Iteration: 138 | Classification loss: 0.06400 | Regression loss: 0.25564 | Running loss: 0.32348\n",
            "Epoch: 7 | Iteration: 139 | Classification loss: 0.07015 | Regression loss: 0.23697 | Running loss: 0.32316\n",
            "Epoch: 7 | Iteration: 140 | Classification loss: 0.00006 | Regression loss: 0.02198 | Running loss: 0.32279\n",
            "Epoch: 7 | Iteration: 141 | Classification loss: 0.00023 | Regression loss: 0.05577 | Running loss: 0.32199\n",
            "Epoch: 7 | Iteration: 142 | Classification loss: 0.00015 | Regression loss: 0.02817 | Running loss: 0.32032\n",
            "Epoch: 7 | Iteration: 143 | Classification loss: 0.03433 | Regression loss: 0.20920 | Running loss: 0.32078\n",
            "Epoch: 7 | Iteration: 144 | Classification loss: 0.00008 | Regression loss: 0.02737 | Running loss: 0.32021\n",
            "Epoch: 7 | Iteration: 145 | Classification loss: 0.11378 | Regression loss: 0.38546 | Running loss: 0.32021\n",
            "Epoch: 7 | Iteration: 146 | Classification loss: 0.00011 | Regression loss: 0.07689 | Running loss: 0.32012\n",
            "Epoch: 7 | Iteration: 147 | Classification loss: 0.13718 | Regression loss: 0.36174 | Running loss: 0.32082\n",
            "Epoch: 7 | Iteration: 148 | Classification loss: 0.04499 | Regression loss: 0.16787 | Running loss: 0.32116\n",
            "Epoch: 7 | Iteration: 149 | Classification loss: 0.00010 | Regression loss: 0.05795 | Running loss: 0.31943\n",
            "Epoch: 7 | Iteration: 150 | Classification loss: 0.02329 | Regression loss: 0.08914 | Running loss: 0.31837\n",
            "Epoch: 7 | Iteration: 151 | Classification loss: 0.11744 | Regression loss: 0.33690 | Running loss: 0.31887\n",
            "Epoch: 7 | Iteration: 152 | Classification loss: 0.07820 | Regression loss: 0.22502 | Running loss: 0.31877\n",
            "Epoch: 7 | Iteration: 153 | Classification loss: 0.17627 | Regression loss: 0.29023 | Running loss: 0.31873\n",
            "Epoch: 7 | Iteration: 154 | Classification loss: 0.07033 | Regression loss: 0.24932 | Running loss: 0.31887\n",
            "Epoch: 7 | Iteration: 155 | Classification loss: 0.13862 | Regression loss: 0.32592 | Running loss: 0.31888\n",
            "Epoch: 7 | Iteration: 156 | Classification loss: 0.22289 | Regression loss: 0.44241 | Running loss: 0.31864\n",
            "Epoch: 7 | Iteration: 157 | Classification loss: 0.15462 | Regression loss: 0.33144 | Running loss: 0.31918\n",
            "Epoch: 7 | Iteration: 158 | Classification loss: 0.08593 | Regression loss: 0.30739 | Running loss: 0.31918\n",
            "Epoch: 7 | Iteration: 159 | Classification loss: 0.05908 | Regression loss: 0.26023 | Running loss: 0.31974\n",
            "Epoch: 7 | Iteration: 160 | Classification loss: 0.02242 | Regression loss: 0.15294 | Running loss: 0.31908\n",
            "Epoch: 7 | Iteration: 161 | Classification loss: 0.07658 | Regression loss: 0.29938 | Running loss: 0.31904\n",
            "Epoch: 7 | Iteration: 162 | Classification loss: 0.07989 | Regression loss: 0.32167 | Running loss: 0.31919\n",
            "Epoch: 7 | Iteration: 163 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.31827\n",
            "Epoch: 7 | Iteration: 164 | Classification loss: 0.20658 | Regression loss: 0.39357 | Running loss: 0.31945\n",
            "Epoch: 7 | Iteration: 165 | Classification loss: 0.17072 | Regression loss: 0.44181 | Running loss: 0.32026\n",
            "Epoch: 7 | Iteration: 166 | Classification loss: 0.17618 | Regression loss: 0.19989 | Running loss: 0.32099\n",
            "Epoch: 7 | Iteration: 167 | Classification loss: 0.04656 | Regression loss: 0.25614 | Running loss: 0.32077\n",
            "Epoch: 7 | Iteration: 168 | Classification loss: 0.00015 | Regression loss: 0.04126 | Running loss: 0.32053\n",
            "Epoch: 7 | Iteration: 169 | Classification loss: 0.04044 | Regression loss: 0.20727 | Running loss: 0.31966\n",
            "Epoch: 7 | Iteration: 170 | Classification loss: 0.13587 | Regression loss: 0.45197 | Running loss: 0.32043\n",
            "Epoch: 7 | Iteration: 171 | Classification loss: 0.04752 | Regression loss: 0.23392 | Running loss: 0.32020\n",
            "Epoch: 7 | Iteration: 172 | Classification loss: 0.04735 | Regression loss: 0.23586 | Running loss: 0.32077\n",
            "Epoch: 7 | Iteration: 173 | Classification loss: 0.16679 | Regression loss: 0.35893 | Running loss: 0.32149\n",
            "Epoch: 7 | Iteration: 174 | Classification loss: 0.10427 | Regression loss: 0.22155 | Running loss: 0.32125\n",
            "Epoch: 7 | Iteration: 175 | Classification loss: 0.01710 | Regression loss: 0.13910 | Running loss: 0.32065\n",
            "Epoch: 7 | Iteration: 176 | Classification loss: 0.09263 | Regression loss: 0.13013 | Running loss: 0.32061\n",
            "Epoch: 7 | Iteration: 177 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.31982\n",
            "Epoch: 7 | Iteration: 178 | Classification loss: 0.07879 | Regression loss: 0.28363 | Running loss: 0.31952\n",
            "Epoch: 7 | Iteration: 179 | Classification loss: 0.21584 | Regression loss: 0.18532 | Running loss: 0.31970\n",
            "Epoch: 7 | Iteration: 180 | Classification loss: 0.02479 | Regression loss: 0.12381 | Running loss: 0.31943\n",
            "Epoch: 7 | Iteration: 181 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.31834\n",
            "Epoch: 7 | Iteration: 182 | Classification loss: 0.06549 | Regression loss: 0.14420 | Running loss: 0.31753\n",
            "Epoch: 7 | Iteration: 183 | Classification loss: 0.21875 | Regression loss: 0.39621 | Running loss: 0.31750\n",
            "Epoch: 7 | Iteration: 184 | Classification loss: 0.02040 | Regression loss: 0.11836 | Running loss: 0.31706\n",
            "Epoch: 7 | Iteration: 185 | Classification loss: 0.03314 | Regression loss: 0.13056 | Running loss: 0.31683\n",
            "Epoch: 7 | Iteration: 186 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.31667\n",
            "Epoch: 7 | Iteration: 187 | Classification loss: 0.00017 | Regression loss: 0.01200 | Running loss: 0.31562\n",
            "Epoch: 7 | Iteration: 188 | Classification loss: 0.06525 | Regression loss: 0.22160 | Running loss: 0.31559\n",
            "Epoch: 7 | Iteration: 189 | Classification loss: 0.12538 | Regression loss: 0.40636 | Running loss: 0.31580\n",
            "Epoch: 7 | Iteration: 190 | Classification loss: 0.12575 | Regression loss: 0.30264 | Running loss: 0.31524\n",
            "Epoch: 7 | Iteration: 191 | Classification loss: 0.07217 | Regression loss: 0.17491 | Running loss: 0.31568\n",
            "Epoch: 7 | Iteration: 192 | Classification loss: 0.76319 | Regression loss: 0.50928 | Running loss: 0.31685\n",
            "Epoch: 7 | Iteration: 193 | Classification loss: 0.02598 | Regression loss: 0.14321 | Running loss: 0.31640\n",
            "Epoch: 7 | Iteration: 194 | Classification loss: 0.00041 | Regression loss: 0.01489 | Running loss: 0.31611\n",
            "Epoch: 7 | Iteration: 195 | Classification loss: 0.00032 | Regression loss: 0.03270 | Running loss: 0.31576\n",
            "Epoch: 7 | Iteration: 196 | Classification loss: 0.00049 | Regression loss: 0.00932 | Running loss: 0.31545\n",
            "Epoch: 7 | Iteration: 197 | Classification loss: 0.08184 | Regression loss: 0.38895 | Running loss: 0.31588\n",
            "Epoch: 7 | Iteration: 198 | Classification loss: 0.02837 | Regression loss: 0.10721 | Running loss: 0.31610\n",
            "Epoch: 7 | Iteration: 199 | Classification loss: 0.00020 | Regression loss: 0.02032 | Running loss: 0.31562\n",
            "Epoch: 7 | Iteration: 200 | Classification loss: 0.08455 | Regression loss: 0.33028 | Running loss: 0.31535\n",
            "Epoch: 7 | Iteration: 201 | Classification loss: 0.11255 | Regression loss: 0.33974 | Running loss: 0.31579\n",
            "Epoch: 7 | Iteration: 202 | Classification loss: 0.18159 | Regression loss: 0.45685 | Running loss: 0.31624\n",
            "Epoch: 7 | Iteration: 203 | Classification loss: 0.07110 | Regression loss: 0.21730 | Running loss: 0.31675\n",
            "Epoch: 7 | Iteration: 204 | Classification loss: 0.03961 | Regression loss: 0.10827 | Running loss: 0.31595\n",
            "Epoch: 7 | Iteration: 205 | Classification loss: 0.14207 | Regression loss: 0.34341 | Running loss: 0.31689\n",
            "Epoch: 7 | Iteration: 206 | Classification loss: 0.15386 | Regression loss: 0.39224 | Running loss: 0.31753\n",
            "Epoch: 7 | Iteration: 207 | Classification loss: 0.03020 | Regression loss: 0.13018 | Running loss: 0.31784\n",
            "Epoch: 7 | Iteration: 208 | Classification loss: 0.00037 | Regression loss: 0.02384 | Running loss: 0.31789\n",
            "Epoch: 7 | Iteration: 209 | Classification loss: 0.02449 | Regression loss: 0.19495 | Running loss: 0.31803\n",
            "Epoch: 7 | Iteration: 210 | Classification loss: 0.05365 | Regression loss: 0.10830 | Running loss: 0.31729\n",
            "Epoch: 7 | Iteration: 211 | Classification loss: 0.11187 | Regression loss: 0.32219 | Running loss: 0.31692\n",
            "Epoch: 7 | Iteration: 212 | Classification loss: 0.03468 | Regression loss: 0.14562 | Running loss: 0.31593\n",
            "Epoch: 7 | Iteration: 213 | Classification loss: 0.16290 | Regression loss: 0.39191 | Running loss: 0.31551\n",
            "Epoch: 7 | Iteration: 214 | Classification loss: 0.00018 | Regression loss: 0.02267 | Running loss: 0.31532\n",
            "Epoch: 7 | Iteration: 215 | Classification loss: 0.05663 | Regression loss: 0.21343 | Running loss: 0.31493\n",
            "Epoch: 7 | Iteration: 216 | Classification loss: 0.19186 | Regression loss: 0.40369 | Running loss: 0.31534\n",
            "Epoch: 7 | Iteration: 217 | Classification loss: 0.00021 | Regression loss: 0.01033 | Running loss: 0.31528\n",
            "Epoch: 7 | Iteration: 218 | Classification loss: 0.00121 | Regression loss: 0.00432 | Running loss: 0.31429\n",
            "Epoch: 7 | Iteration: 219 | Classification loss: 0.00019 | Regression loss: 0.00664 | Running loss: 0.31358\n",
            "Epoch: 7 | Iteration: 220 | Classification loss: 0.08399 | Regression loss: 0.26493 | Running loss: 0.31312\n",
            "Epoch: 7 | Iteration: 221 | Classification loss: 0.02831 | Regression loss: 0.08669 | Running loss: 0.31306\n",
            "Epoch: 7 | Iteration: 222 | Classification loss: 0.03749 | Regression loss: 0.14993 | Running loss: 0.31262\n",
            "Epoch: 7 | Iteration: 223 | Classification loss: 0.01157 | Regression loss: 0.09113 | Running loss: 0.31283\n",
            "Epoch: 7 | Iteration: 224 | Classification loss: 0.17378 | Regression loss: 0.51308 | Running loss: 0.31406\n",
            "Epoch: 7 | Iteration: 225 | Classification loss: 0.07047 | Regression loss: 0.28981 | Running loss: 0.31447\n",
            "Epoch: 7 | Iteration: 226 | Classification loss: 0.00017 | Regression loss: 0.01334 | Running loss: 0.31418\n",
            "Epoch: 7 | Iteration: 227 | Classification loss: 0.07671 | Regression loss: 0.34629 | Running loss: 0.31377\n",
            "Epoch: 7 | Iteration: 228 | Classification loss: 0.14442 | Regression loss: 0.31539 | Running loss: 0.31369\n",
            "Epoch: 7 | Iteration: 229 | Classification loss: 0.13103 | Regression loss: 0.32922 | Running loss: 0.31345\n",
            "Epoch: 7 | Iteration: 230 | Classification loss: 0.00011 | Regression loss: 0.00847 | Running loss: 0.31279\n",
            "Epoch: 7 | Iteration: 231 | Classification loss: 0.11728 | Regression loss: 0.46469 | Running loss: 0.31300\n",
            "Epoch: 7 | Iteration: 232 | Classification loss: 0.13118 | Regression loss: 0.35634 | Running loss: 0.31295\n",
            "Epoch: 7 | Iteration: 233 | Classification loss: 0.13609 | Regression loss: 0.33357 | Running loss: 0.31356\n",
            "Epoch: 7 | Iteration: 234 | Classification loss: 0.05924 | Regression loss: 0.17363 | Running loss: 0.31403\n",
            "Epoch: 7 | Iteration: 235 | Classification loss: 0.03288 | Regression loss: 0.11743 | Running loss: 0.31362\n",
            "Epoch: 7 | Iteration: 236 | Classification loss: 0.14096 | Regression loss: 0.32589 | Running loss: 0.31357\n",
            "Epoch: 7 | Iteration: 237 | Classification loss: 0.02880 | Regression loss: 0.11414 | Running loss: 0.31343\n",
            "Epoch: 7 | Iteration: 238 | Classification loss: 0.08738 | Regression loss: 0.34442 | Running loss: 0.31420\n",
            "Epoch: 7 | Iteration: 239 | Classification loss: 0.00018 | Regression loss: 0.02632 | Running loss: 0.31364\n",
            "Epoch: 7 | Iteration: 240 | Classification loss: 0.17872 | Regression loss: 0.51417 | Running loss: 0.31446\n",
            "Epoch: 7 | Iteration: 241 | Classification loss: 0.03523 | Regression loss: 0.09630 | Running loss: 0.31435\n",
            "Epoch: 7 | Iteration: 242 | Classification loss: 0.11520 | Regression loss: 0.31704 | Running loss: 0.31433\n",
            "Epoch: 7 | Iteration: 243 | Classification loss: 0.20229 | Regression loss: 0.52831 | Running loss: 0.31524\n",
            "Epoch: 7 | Iteration: 244 | Classification loss: 0.00034 | Regression loss: 0.02270 | Running loss: 0.31469\n",
            "Epoch: 7 | Iteration: 245 | Classification loss: 0.04614 | Regression loss: 0.17564 | Running loss: 0.31429\n",
            "Epoch: 7 | Iteration: 246 | Classification loss: 0.07205 | Regression loss: 0.24748 | Running loss: 0.31425\n",
            "Epoch: 7 | Iteration: 247 | Classification loss: 0.01929 | Regression loss: 0.13335 | Running loss: 0.31357\n",
            "Epoch: 7 | Iteration: 248 | Classification loss: 0.25105 | Regression loss: 0.41889 | Running loss: 0.31453\n",
            "Epoch: 7 | Iteration: 249 | Classification loss: 0.08305 | Regression loss: 0.28040 | Running loss: 0.31462\n",
            "Epoch: 7 | Iteration: 250 | Classification loss: 0.10325 | Regression loss: 0.22924 | Running loss: 0.31465\n",
            "Epoch: 7 | Iteration: 251 | Classification loss: 0.03521 | Regression loss: 0.14160 | Running loss: 0.31428\n",
            "Epoch: 7 | Iteration: 252 | Classification loss: 0.05053 | Regression loss: 0.22208 | Running loss: 0.31446\n",
            "Epoch: 7 | Iteration: 253 | Classification loss: 0.06120 | Regression loss: 0.28242 | Running loss: 0.31430\n",
            "Epoch: 7 | Iteration: 254 | Classification loss: 0.00008 | Regression loss: 0.01713 | Running loss: 0.31393\n",
            "Epoch: 7 | Iteration: 255 | Classification loss: 0.05350 | Regression loss: 0.21610 | Running loss: 0.31444\n",
            "Epoch: 7 | Iteration: 256 | Classification loss: 0.23456 | Regression loss: 0.49817 | Running loss: 0.31481\n",
            "Epoch: 7 | Iteration: 257 | Classification loss: 0.10920 | Regression loss: 0.29418 | Running loss: 0.31452\n",
            "Epoch: 7 | Iteration: 258 | Classification loss: 0.00161 | Regression loss: 0.00610 | Running loss: 0.31449\n",
            "Epoch: 7 | Iteration: 259 | Classification loss: 0.07266 | Regression loss: 0.14508 | Running loss: 0.31407\n",
            "Epoch: 7 | Iteration: 260 | Classification loss: 0.02972 | Regression loss: 0.05292 | Running loss: 0.31423\n",
            "Epoch: 7 | Iteration: 261 | Classification loss: 0.00007 | Regression loss: 0.01032 | Running loss: 0.31339\n",
            "Epoch: 7 | Iteration: 262 | Classification loss: 0.04614 | Regression loss: 0.17751 | Running loss: 0.31329\n",
            "Epoch: 7 | Iteration: 263 | Classification loss: 0.05345 | Regression loss: 0.22375 | Running loss: 0.31348\n",
            "Epoch: 7 | Iteration: 264 | Classification loss: 0.05680 | Regression loss: 0.16057 | Running loss: 0.31391\n",
            "Epoch: 7 | Iteration: 265 | Classification loss: 0.08605 | Regression loss: 0.34764 | Running loss: 0.31425\n",
            "Epoch: 7 | Iteration: 266 | Classification loss: 0.00089 | Regression loss: 0.01865 | Running loss: 0.31379\n",
            "Epoch: 7 | Iteration: 267 | Classification loss: 0.00008 | Regression loss: 0.02201 | Running loss: 0.31375\n",
            "Epoch: 7 | Iteration: 268 | Classification loss: 0.05368 | Regression loss: 0.18097 | Running loss: 0.31357\n",
            "Epoch: 7 | Iteration: 269 | Classification loss: 0.23700 | Regression loss: 0.19596 | Running loss: 0.31367\n",
            "Epoch: 7 | Iteration: 270 | Classification loss: 0.11820 | Regression loss: 0.18464 | Running loss: 0.31345\n",
            "Epoch: 7 | Iteration: 271 | Classification loss: 0.00015 | Regression loss: 0.06261 | Running loss: 0.31306\n",
            "Epoch: 7 | Iteration: 272 | Classification loss: 0.17112 | Regression loss: 0.27197 | Running loss: 0.31243\n",
            "Epoch: 7 | Iteration: 273 | Classification loss: 0.10372 | Regression loss: 0.37239 | Running loss: 0.31257\n",
            "Epoch: 7 | Iteration: 274 | Classification loss: 0.00018 | Regression loss: 0.05366 | Running loss: 0.31160\n",
            "Epoch: 7 | Iteration: 275 | Classification loss: 0.02666 | Regression loss: 0.20605 | Running loss: 0.31179\n",
            "Epoch: 7 | Iteration: 276 | Classification loss: 0.07674 | Regression loss: 0.26071 | Running loss: 0.31216\n",
            "Epoch: 7 | Iteration: 277 | Classification loss: 0.06743 | Regression loss: 0.23094 | Running loss: 0.31276\n",
            "Epoch: 7 | Iteration: 278 | Classification loss: 0.00017 | Regression loss: 0.02796 | Running loss: 0.31201\n",
            "Epoch: 7 | Iteration: 279 | Classification loss: 0.07602 | Regression loss: 0.32667 | Running loss: 0.31195\n",
            "Epoch: 7 | Iteration: 280 | Classification loss: 0.10124 | Regression loss: 0.26466 | Running loss: 0.31236\n",
            "Epoch: 7 | Iteration: 281 | Classification loss: 0.00016 | Regression loss: 0.00600 | Running loss: 0.31208\n",
            "Epoch: 7 | Iteration: 282 | Classification loss: 0.17468 | Regression loss: 0.38153 | Running loss: 0.31314\n",
            "Epoch: 7 | Iteration: 283 | Classification loss: 0.00012 | Regression loss: 0.00383 | Running loss: 0.31230\n",
            "Epoch: 7 | Iteration: 284 | Classification loss: 0.10424 | Regression loss: 0.40797 | Running loss: 0.31229\n",
            "Epoch: 7 | Iteration: 285 | Classification loss: 0.05341 | Regression loss: 0.19380 | Running loss: 0.31272\n",
            "Epoch: 7 | Iteration: 286 | Classification loss: 0.02191 | Regression loss: 0.14299 | Running loss: 0.31298\n",
            "Epoch: 7 | Iteration: 287 | Classification loss: 0.07388 | Regression loss: 0.12235 | Running loss: 0.31247\n",
            "Epoch: 7 | Iteration: 288 | Classification loss: 0.04080 | Regression loss: 0.16290 | Running loss: 0.31247\n",
            "Epoch: 7 | Iteration: 289 | Classification loss: 0.06272 | Regression loss: 0.22543 | Running loss: 0.31216\n",
            "Epoch: 7 | Iteration: 290 | Classification loss: 0.24923 | Regression loss: 0.32076 | Running loss: 0.31236\n",
            "Epoch: 7 | Iteration: 291 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.31176\n",
            "Epoch: 7 | Iteration: 292 | Classification loss: 0.00017 | Regression loss: 0.02956 | Running loss: 0.31093\n",
            "Epoch: 7 | Iteration: 293 | Classification loss: 0.09007 | Regression loss: 0.29556 | Running loss: 0.31099\n",
            "Epoch: 7 | Iteration: 294 | Classification loss: 0.12028 | Regression loss: 0.39527 | Running loss: 0.31016\n",
            "Epoch: 7 | Iteration: 295 | Classification loss: 0.00006 | Regression loss: 0.00000 | Running loss: 0.31009\n",
            "Epoch: 7 | Iteration: 296 | Classification loss: 0.00017 | Regression loss: 0.05082 | Running loss: 0.31017\n",
            "Epoch: 7 | Iteration: 297 | Classification loss: 0.00013 | Regression loss: 0.02819 | Running loss: 0.30884\n",
            "Epoch: 7 | Iteration: 298 | Classification loss: 0.05540 | Regression loss: 0.27389 | Running loss: 0.30939\n",
            "Epoch: 7 | Iteration: 299 | Classification loss: 0.02316 | Regression loss: 0.15326 | Running loss: 0.30937\n",
            "Epoch: 7 | Iteration: 300 | Classification loss: 0.02682 | Regression loss: 0.11141 | Running loss: 0.30859\n",
            "Epoch: 7 | Iteration: 301 | Classification loss: 0.06978 | Regression loss: 0.19888 | Running loss: 0.30816\n",
            "Epoch: 7 | Iteration: 302 | Classification loss: 0.04190 | Regression loss: 0.23240 | Running loss: 0.30688\n",
            "Epoch: 7 | Iteration: 303 | Classification loss: 0.02998 | Regression loss: 0.15851 | Running loss: 0.30692\n",
            "Epoch: 7 | Iteration: 304 | Classification loss: 0.03487 | Regression loss: 0.16127 | Running loss: 0.30732\n",
            "Epoch: 7 | Iteration: 305 | Classification loss: 0.00015 | Regression loss: 0.07293 | Running loss: 0.30711\n",
            "Epoch: 7 | Iteration: 306 | Classification loss: 0.02766 | Regression loss: 0.10258 | Running loss: 0.30628\n",
            "Epoch: 7 | Iteration: 307 | Classification loss: 0.58358 | Regression loss: 0.44381 | Running loss: 0.30789\n",
            "Epoch: 7 | Iteration: 308 | Classification loss: 0.20156 | Regression loss: 0.11901 | Running loss: 0.30850\n",
            "Epoch: 7 | Iteration: 309 | Classification loss: 0.16422 | Regression loss: 0.48503 | Running loss: 0.30867\n",
            "Epoch: 7 | Iteration: 310 | Classification loss: 0.06083 | Regression loss: 0.30304 | Running loss: 0.30891\n",
            "Epoch: 7 | Iteration: 311 | Classification loss: 0.03797 | Regression loss: 0.24039 | Running loss: 0.30776\n",
            "Epoch: 7 | Iteration: 312 | Classification loss: 0.07700 | Regression loss: 0.26208 | Running loss: 0.30802\n",
            "Epoch: 7 | Iteration: 313 | Classification loss: 0.04279 | Regression loss: 0.24930 | Running loss: 0.30795\n",
            "Epoch: 7 | Iteration: 314 | Classification loss: 0.07823 | Regression loss: 0.37376 | Running loss: 0.30805\n",
            "Epoch: 7 | Iteration: 315 | Classification loss: 0.00011 | Regression loss: 0.06538 | Running loss: 0.30737\n",
            "Epoch: 7 | Iteration: 316 | Classification loss: 0.05016 | Regression loss: 0.18618 | Running loss: 0.30747\n",
            "Epoch: 7 | Iteration: 317 | Classification loss: 0.00009 | Regression loss: 0.02963 | Running loss: 0.30740\n",
            "Epoch: 7 | Iteration: 318 | Classification loss: 0.13670 | Regression loss: 0.45359 | Running loss: 0.30849\n",
            "Epoch: 7 | Iteration: 319 | Classification loss: 0.10130 | Regression loss: 0.33461 | Running loss: 0.30930\n",
            "Epoch: 7 | Iteration: 320 | Classification loss: 0.21553 | Regression loss: 0.51385 | Running loss: 0.30976\n",
            "Epoch: 7 | Iteration: 321 | Classification loss: 0.11275 | Regression loss: 0.39283 | Running loss: 0.31039\n",
            "Epoch: 7 | Iteration: 322 | Classification loss: 0.05407 | Regression loss: 0.12261 | Running loss: 0.31046\n",
            "Epoch: 7 | Iteration: 323 | Classification loss: 0.11412 | Regression loss: 0.25484 | Running loss: 0.30959\n",
            "Epoch: 7 | Iteration: 324 | Classification loss: 0.08245 | Regression loss: 0.21573 | Running loss: 0.30947\n",
            "Epoch: 7 | Iteration: 325 | Classification loss: 0.04509 | Regression loss: 0.28449 | Running loss: 0.30923\n",
            "Epoch: 7 | Iteration: 326 | Classification loss: 0.11767 | Regression loss: 0.29108 | Running loss: 0.30911\n",
            "Epoch: 7 | Iteration: 327 | Classification loss: 0.07677 | Regression loss: 0.29399 | Running loss: 0.30891\n",
            "Epoch: 7 | Iteration: 328 | Classification loss: 0.03997 | Regression loss: 0.22783 | Running loss: 0.30837\n",
            "Epoch: 7 | Iteration: 329 | Classification loss: 0.09295 | Regression loss: 0.32396 | Running loss: 0.30810\n",
            "Epoch: 7 | Iteration: 330 | Classification loss: 0.10374 | Regression loss: 0.29455 | Running loss: 0.30835\n",
            "Epoch: 7 | Iteration: 331 | Classification loss: 0.16767 | Regression loss: 0.40569 | Running loss: 0.30884\n",
            "Epoch: 7 | Iteration: 332 | Classification loss: 0.05850 | Regression loss: 0.34327 | Running loss: 0.30855\n",
            "Epoch: 7 | Iteration: 333 | Classification loss: 0.09854 | Regression loss: 0.28753 | Running loss: 0.30864\n",
            "Epoch: 7 | Iteration: 334 | Classification loss: 0.12228 | Regression loss: 0.18057 | Running loss: 0.30908\n",
            "Epoch: 7 | Iteration: 335 | Classification loss: 0.10158 | Regression loss: 0.31597 | Running loss: 0.30953\n",
            "Epoch: 7 | Iteration: 336 | Classification loss: 0.05895 | Regression loss: 0.25719 | Running loss: 0.31012\n",
            "Epoch: 7 | Iteration: 337 | Classification loss: 0.16912 | Regression loss: 0.26401 | Running loss: 0.30984\n",
            "Epoch: 7 | Iteration: 338 | Classification loss: 0.02625 | Regression loss: 0.20111 | Running loss: 0.30923\n",
            "Epoch: 7 | Iteration: 339 | Classification loss: 0.05912 | Regression loss: 0.30842 | Running loss: 0.30895\n",
            "Epoch: 7 | Iteration: 340 | Classification loss: 0.09745 | Regression loss: 0.18636 | Running loss: 0.30931\n",
            "Epoch: 7 | Iteration: 341 | Classification loss: 0.08386 | Regression loss: 0.18366 | Running loss: 0.30929\n",
            "Epoch: 7 | Iteration: 342 | Classification loss: 0.10207 | Regression loss: 0.34660 | Running loss: 0.30840\n",
            "Epoch: 7 | Iteration: 343 | Classification loss: 0.27168 | Regression loss: 0.43524 | Running loss: 0.30937\n",
            "Epoch: 7 | Iteration: 344 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30850\n",
            "Epoch: 7 | Iteration: 345 | Classification loss: 0.12863 | Regression loss: 0.14453 | Running loss: 0.30792\n",
            "Epoch: 7 | Iteration: 346 | Classification loss: 0.26839 | Regression loss: 0.30633 | Running loss: 0.30857\n",
            "Epoch: 7 | Iteration: 347 | Classification loss: 0.15388 | Regression loss: 0.35387 | Running loss: 0.30945\n",
            "Epoch: 7 | Iteration: 348 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30896\n",
            "Epoch: 7 | Iteration: 349 | Classification loss: 0.07196 | Regression loss: 0.31285 | Running loss: 0.30842\n",
            "Epoch: 7 | Iteration: 350 | Classification loss: 0.01743 | Regression loss: 0.07982 | Running loss: 0.30832\n",
            "Epoch: 7 | Iteration: 351 | Classification loss: 0.12778 | Regression loss: 0.28278 | Running loss: 0.30868\n",
            "Epoch: 7 | Iteration: 352 | Classification loss: 0.07910 | Regression loss: 0.24622 | Running loss: 0.30887\n",
            "Epoch: 7 | Iteration: 353 | Classification loss: 0.09219 | Regression loss: 0.29526 | Running loss: 0.30893\n",
            "Epoch: 7 | Iteration: 354 | Classification loss: 0.04730 | Regression loss: 0.14581 | Running loss: 0.30824\n",
            "Epoch: 7 | Iteration: 355 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30756\n",
            "Epoch: 7 | Iteration: 356 | Classification loss: 0.11376 | Regression loss: 0.38974 | Running loss: 0.30810\n",
            "Epoch: 7 | Iteration: 357 | Classification loss: 0.00027 | Regression loss: 0.01130 | Running loss: 0.30754\n",
            "Epoch: 7 | Iteration: 358 | Classification loss: 0.01743 | Regression loss: 0.09211 | Running loss: 0.30769\n",
            "Epoch: 7 | Iteration: 359 | Classification loss: 0.00086 | Regression loss: 0.00249 | Running loss: 0.30683\n",
            "Epoch: 7 | Iteration: 360 | Classification loss: 0.08568 | Regression loss: 0.37264 | Running loss: 0.30703\n",
            "Epoch: 7 | Iteration: 361 | Classification loss: 0.00074 | Regression loss: 0.00514 | Running loss: 0.30680\n",
            "Epoch: 7 | Iteration: 362 | Classification loss: 0.01817 | Regression loss: 0.20635 | Running loss: 0.30679\n",
            "Epoch: 7 | Iteration: 363 | Classification loss: 0.00024 | Regression loss: 0.00743 | Running loss: 0.30618\n",
            "Epoch: 7 | Iteration: 364 | Classification loss: 0.06381 | Regression loss: 0.25761 | Running loss: 0.30587\n",
            "Epoch: 7 | Iteration: 365 | Classification loss: 0.00035 | Regression loss: 0.01272 | Running loss: 0.30477\n",
            "Epoch: 7 | Iteration: 366 | Classification loss: 0.05775 | Regression loss: 0.13376 | Running loss: 0.30506\n",
            "Epoch: 7 | Iteration: 367 | Classification loss: 0.05597 | Regression loss: 0.22160 | Running loss: 0.30518\n",
            "Epoch: 7 | Iteration: 368 | Classification loss: 0.11588 | Regression loss: 0.34173 | Running loss: 0.30513\n",
            "Epoch: 7 | Iteration: 369 | Classification loss: 0.00011 | Regression loss: 0.03330 | Running loss: 0.30463\n",
            "Epoch: 7 | Iteration: 370 | Classification loss: 0.00014 | Regression loss: 0.00889 | Running loss: 0.30426\n",
            "Epoch: 7 | Iteration: 371 | Classification loss: 0.12245 | Regression loss: 0.37054 | Running loss: 0.30503\n",
            "Epoch: 7 | Iteration: 372 | Classification loss: 0.00028 | Regression loss: 0.00640 | Running loss: 0.30417\n",
            "Epoch: 7 | Iteration: 373 | Classification loss: 0.03233 | Regression loss: 0.13135 | Running loss: 0.30394\n",
            "Epoch: 7 | Iteration: 374 | Classification loss: 0.10116 | Regression loss: 0.22411 | Running loss: 0.30456\n",
            "Epoch: 7 | Iteration: 375 | Classification loss: 0.06166 | Regression loss: 0.30691 | Running loss: 0.30466\n",
            "Epoch: 7 | Iteration: 376 | Classification loss: 0.19292 | Regression loss: 0.56207 | Running loss: 0.30511\n",
            "Epoch: 7 | Iteration: 377 | Classification loss: 0.02131 | Regression loss: 0.13319 | Running loss: 0.30450\n",
            "Epoch: 7 | Iteration: 378 | Classification loss: 0.08201 | Regression loss: 0.36291 | Running loss: 0.30496\n",
            "Epoch: 7 | Iteration: 379 | Classification loss: 0.07511 | Regression loss: 0.28590 | Running loss: 0.30560\n",
            "Epoch: 7 | Iteration: 380 | Classification loss: 0.16592 | Regression loss: 0.28054 | Running loss: 0.30484\n",
            "Epoch: 7 | Iteration: 381 | Classification loss: 0.06823 | Regression loss: 0.24581 | Running loss: 0.30465\n",
            "Epoch: 7 | Iteration: 382 | Classification loss: 0.10603 | Regression loss: 0.26278 | Running loss: 0.30530\n",
            "Epoch: 7 | Iteration: 383 | Classification loss: 0.06556 | Regression loss: 0.23104 | Running loss: 0.30477\n",
            "Epoch: 7 | Iteration: 384 | Classification loss: 0.10186 | Regression loss: 0.33011 | Running loss: 0.30563\n",
            "Epoch: 7 | Iteration: 385 | Classification loss: 0.11193 | Regression loss: 0.33927 | Running loss: 0.30571\n",
            "Epoch: 7 | Iteration: 386 | Classification loss: 0.45048 | Regression loss: 0.61468 | Running loss: 0.30678\n",
            "Epoch: 7 | Iteration: 387 | Classification loss: 0.04793 | Regression loss: 0.21164 | Running loss: 0.30602\n",
            "Epoch: 7 | Iteration: 388 | Classification loss: 0.12689 | Regression loss: 0.52265 | Running loss: 0.30639\n",
            "Epoch: 7 | Iteration: 389 | Classification loss: 0.05424 | Regression loss: 0.14883 | Running loss: 0.30615\n",
            "Epoch: 7 | Iteration: 390 | Classification loss: 0.19474 | Regression loss: 0.38375 | Running loss: 0.30720\n",
            "Epoch: 7 | Iteration: 391 | Classification loss: 0.04175 | Regression loss: 0.14429 | Running loss: 0.30728\n",
            "Epoch: 7 | Iteration: 392 | Classification loss: 0.15932 | Regression loss: 0.39358 | Running loss: 0.30812\n",
            "Epoch: 7 | Iteration: 393 | Classification loss: 0.02097 | Regression loss: 0.08652 | Running loss: 0.30780\n",
            "Epoch: 7 | Iteration: 394 | Classification loss: 0.04415 | Regression loss: 0.20083 | Running loss: 0.30823\n",
            "Epoch: 7 | Iteration: 395 | Classification loss: 0.06278 | Regression loss: 0.26138 | Running loss: 0.30825\n",
            "Epoch: 7 | Iteration: 396 | Classification loss: 0.21369 | Regression loss: 0.45922 | Running loss: 0.30885\n",
            "Epoch: 7 | Iteration: 397 | Classification loss: 0.14893 | Regression loss: 0.40031 | Running loss: 0.30948\n",
            "Epoch: 7 | Iteration: 398 | Classification loss: 0.09893 | Regression loss: 0.35644 | Running loss: 0.30911\n",
            "Epoch: 7 | Iteration: 399 | Classification loss: 0.00030 | Regression loss: 0.00000 | Running loss: 0.30901\n",
            "Epoch: 7 | Iteration: 400 | Classification loss: 0.13675 | Regression loss: 0.19304 | Running loss: 0.30965\n",
            "Epoch: 7 | Iteration: 401 | Classification loss: 0.02646 | Regression loss: 0.12250 | Running loss: 0.30812\n",
            "Epoch: 7 | Iteration: 402 | Classification loss: 0.00044 | Regression loss: 0.02835 | Running loss: 0.30698\n",
            "Epoch: 7 | Iteration: 403 | Classification loss: 0.18176 | Regression loss: 0.55778 | Running loss: 0.30842\n",
            "Epoch: 7 | Iteration: 404 | Classification loss: 0.19707 | Regression loss: 0.37868 | Running loss: 0.30891\n",
            "Epoch: 7 | Iteration: 405 | Classification loss: 0.09643 | Regression loss: 0.32059 | Running loss: 0.30890\n",
            "Epoch: 7 | Iteration: 406 | Classification loss: 0.13709 | Regression loss: 0.29996 | Running loss: 0.30906\n",
            "Epoch: 7 | Iteration: 407 | Classification loss: 0.05996 | Regression loss: 0.23377 | Running loss: 0.30951\n",
            "Epoch: 7 | Iteration: 408 | Classification loss: 0.05654 | Regression loss: 0.17423 | Running loss: 0.30933\n",
            "Epoch: 7 | Iteration: 409 | Classification loss: 0.00083 | Regression loss: 0.01966 | Running loss: 0.30856\n",
            "Epoch: 7 | Iteration: 410 | Classification loss: 0.18346 | Regression loss: 0.56794 | Running loss: 0.30936\n",
            "Epoch: 7 | Iteration: 411 | Classification loss: 0.04412 | Regression loss: 0.15374 | Running loss: 0.30917\n",
            "Epoch: 7 | Iteration: 412 | Classification loss: 0.02960 | Regression loss: 0.18918 | Running loss: 0.30882\n",
            "Epoch: 7 | Iteration: 413 | Classification loss: 0.05136 | Regression loss: 0.14769 | Running loss: 0.30912\n",
            "Epoch: 7 | Iteration: 414 | Classification loss: 0.01790 | Regression loss: 0.09343 | Running loss: 0.30928\n",
            "Epoch: 7 | Iteration: 415 | Classification loss: 0.08805 | Regression loss: 0.30291 | Running loss: 0.30938\n",
            "Epoch: 7 | Iteration: 416 | Classification loss: 0.09145 | Regression loss: 0.31684 | Running loss: 0.30962\n",
            "Epoch: 7 | Iteration: 417 | Classification loss: 0.15932 | Regression loss: 0.34926 | Running loss: 0.31008\n",
            "Epoch: 7 | Iteration: 418 | Classification loss: 0.02704 | Regression loss: 0.13073 | Running loss: 0.30975\n",
            "Epoch: 7 | Iteration: 419 | Classification loss: 0.13857 | Regression loss: 0.32472 | Running loss: 0.30969\n",
            "Epoch: 7 | Iteration: 420 | Classification loss: 0.09785 | Regression loss: 0.18963 | Running loss: 0.30952\n",
            "Epoch: 7 | Iteration: 421 | Classification loss: 0.03053 | Regression loss: 0.17685 | Running loss: 0.30939\n",
            "Epoch: 7 | Iteration: 422 | Classification loss: 0.07976 | Regression loss: 0.25535 | Running loss: 0.30852\n",
            "Epoch: 7 | Iteration: 423 | Classification loss: 0.18098 | Regression loss: 0.34737 | Running loss: 0.30802\n",
            "Epoch: 7 | Iteration: 424 | Classification loss: 0.14534 | Regression loss: 0.21968 | Running loss: 0.30804\n",
            "Epoch: 7 | Iteration: 425 | Classification loss: 0.00090 | Regression loss: 0.00354 | Running loss: 0.30711\n",
            "Epoch: 7 | Iteration: 426 | Classification loss: 0.07810 | Regression loss: 0.29726 | Running loss: 0.30757\n",
            "Epoch: 7 | Iteration: 427 | Classification loss: 0.03586 | Regression loss: 0.10437 | Running loss: 0.30648\n",
            "Epoch: 7 | Iteration: 428 | Classification loss: 0.09919 | Regression loss: 0.29770 | Running loss: 0.30724\n",
            "Epoch: 7 | Iteration: 429 | Classification loss: 0.16568 | Regression loss: 0.36398 | Running loss: 0.30766\n",
            "Epoch: 7 | Iteration: 430 | Classification loss: 0.05226 | Regression loss: 0.20022 | Running loss: 0.30722\n",
            "Epoch: 7 | Iteration: 431 | Classification loss: 0.13163 | Regression loss: 0.39829 | Running loss: 0.30744\n",
            "Epoch: 7 | Iteration: 432 | Classification loss: 0.09884 | Regression loss: 0.28854 | Running loss: 0.30701\n",
            "Epoch: 7 | Iteration: 433 | Classification loss: 0.02671 | Regression loss: 0.10354 | Running loss: 0.30719\n",
            "Epoch: 7 | Iteration: 434 | Classification loss: 0.11724 | Regression loss: 0.24861 | Running loss: 0.30759\n",
            "Epoch: 7 | Iteration: 435 | Classification loss: 0.09397 | Regression loss: 0.27563 | Running loss: 0.30749\n",
            "Epoch: 7 | Iteration: 436 | Classification loss: 0.09854 | Regression loss: 0.28322 | Running loss: 0.30685\n",
            "Epoch: 7 | Iteration: 437 | Classification loss: 0.02953 | Regression loss: 0.09925 | Running loss: 0.30640\n",
            "Epoch: 7 | Iteration: 438 | Classification loss: 0.06202 | Regression loss: 0.26167 | Running loss: 0.30667\n",
            "Epoch: 7 | Iteration: 439 | Classification loss: 0.24366 | Regression loss: 0.58576 | Running loss: 0.30782\n",
            "Epoch: 7 | Iteration: 440 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.30732\n",
            "Epoch: 7 | Iteration: 441 | Classification loss: 0.00036 | Regression loss: 0.00650 | Running loss: 0.30643\n",
            "Epoch: 7 | Iteration: 442 | Classification loss: 0.12520 | Regression loss: 0.37462 | Running loss: 0.30676\n",
            "Epoch: 7 | Iteration: 443 | Classification loss: 0.00004 | Regression loss: 0.00000 | Running loss: 0.30506\n",
            "Epoch: 7 | Iteration: 444 | Classification loss: 0.08846 | Regression loss: 0.23703 | Running loss: 0.30473\n",
            "Epoch: 7 | Iteration: 445 | Classification loss: 0.00095 | Regression loss: 0.01018 | Running loss: 0.30440\n",
            "Epoch: 7 | Iteration: 446 | Classification loss: 0.00029 | Regression loss: 0.00304 | Running loss: 0.30322\n",
            "Epoch: 7 | Iteration: 447 | Classification loss: 0.11008 | Regression loss: 0.36777 | Running loss: 0.30381\n",
            "Epoch: 7 | Iteration: 448 | Classification loss: 0.17468 | Regression loss: 0.33548 | Running loss: 0.30336\n",
            "Epoch: 7 | Iteration: 449 | Classification loss: 0.03201 | Regression loss: 0.19350 | Running loss: 0.30236\n",
            "Epoch: 7 | Iteration: 450 | Classification loss: 0.12512 | Regression loss: 0.26620 | Running loss: 0.30310\n",
            "Epoch: 7 | Iteration: 451 | Classification loss: 0.16549 | Regression loss: 0.46493 | Running loss: 0.30392\n",
            "Epoch: 7 | Iteration: 452 | Classification loss: 0.11825 | Regression loss: 0.38624 | Running loss: 0.30441\n",
            "Epoch: 7 | Iteration: 453 | Classification loss: 0.00014 | Regression loss: 0.04893 | Running loss: 0.30343\n",
            "Epoch: 7 | Iteration: 454 | Classification loss: 0.00031 | Regression loss: 0.03564 | Running loss: 0.30350\n",
            "Epoch: 7 | Iteration: 455 | Classification loss: 0.06073 | Regression loss: 0.25227 | Running loss: 0.30406\n",
            "Epoch: 7 | Iteration: 456 | Classification loss: 0.11850 | Regression loss: 0.35698 | Running loss: 0.30422\n",
            "Epoch: 7 | Iteration: 457 | Classification loss: 0.05494 | Regression loss: 0.25274 | Running loss: 0.30321\n",
            "Epoch: 7 | Iteration: 458 | Classification loss: 0.04006 | Regression loss: 0.24993 | Running loss: 0.30329\n",
            "Epoch: 7 | Iteration: 459 | Classification loss: 0.03109 | Regression loss: 0.16382 | Running loss: 0.30323\n",
            "Epoch: 7 | Iteration: 460 | Classification loss: 0.03007 | Regression loss: 0.07804 | Running loss: 0.30293\n",
            "Epoch: 7 | Iteration: 461 | Classification loss: 0.00013 | Regression loss: 0.05760 | Running loss: 0.30275\n",
            "Epoch: 7 | Iteration: 462 | Classification loss: 0.19922 | Regression loss: 0.34017 | Running loss: 0.30353\n",
            "Epoch: 7 | Iteration: 463 | Classification loss: 0.06053 | Regression loss: 0.28067 | Running loss: 0.30417\n",
            "Epoch: 7 | Iteration: 464 | Classification loss: 0.03375 | Regression loss: 0.13365 | Running loss: 0.30320\n",
            "Epoch: 7 | Iteration: 465 | Classification loss: 0.05635 | Regression loss: 0.18509 | Running loss: 0.30288\n",
            "Epoch: 7 | Iteration: 466 | Classification loss: 0.11869 | Regression loss: 0.46695 | Running loss: 0.30323\n",
            "Epoch: 7 | Iteration: 467 | Classification loss: 0.11408 | Regression loss: 0.33266 | Running loss: 0.30241\n",
            "Epoch: 7 | Iteration: 468 | Classification loss: 0.03457 | Regression loss: 0.12606 | Running loss: 0.30269\n",
            "Epoch: 7 | Iteration: 469 | Classification loss: 0.05967 | Regression loss: 0.25703 | Running loss: 0.30287\n",
            "Epoch: 7 | Iteration: 470 | Classification loss: 0.13683 | Regression loss: 0.16587 | Running loss: 0.30305\n",
            "Epoch: 7 | Iteration: 471 | Classification loss: 0.16883 | Regression loss: 0.38320 | Running loss: 0.30331\n",
            "Epoch: 7 | Iteration: 472 | Classification loss: 0.09478 | Regression loss: 0.34745 | Running loss: 0.30374\n",
            "Epoch: 7 | Iteration: 473 | Classification loss: 0.09165 | Regression loss: 0.29126 | Running loss: 0.30400\n",
            "Epoch: 7 | Iteration: 474 | Classification loss: 0.07161 | Regression loss: 0.30247 | Running loss: 0.30449\n",
            "Epoch: 7 | Iteration: 475 | Classification loss: 0.04665 | Regression loss: 0.19275 | Running loss: 0.30407\n",
            "Epoch: 7 | Iteration: 476 | Classification loss: 0.03167 | Regression loss: 0.10179 | Running loss: 0.30358\n",
            "Epoch: 7 | Iteration: 477 | Classification loss: 0.01967 | Regression loss: 0.11585 | Running loss: 0.30311\n",
            "Epoch: 7 | Iteration: 478 | Classification loss: 0.03307 | Regression loss: 0.12556 | Running loss: 0.30316\n",
            "Epoch: 7 | Iteration: 479 | Classification loss: 0.14873 | Regression loss: 0.54136 | Running loss: 0.30385\n",
            "Epoch: 7 | Iteration: 480 | Classification loss: 0.06207 | Regression loss: 0.37319 | Running loss: 0.30396\n",
            "Epoch: 7 | Iteration: 481 | Classification loss: 0.13740 | Regression loss: 0.34205 | Running loss: 0.30449\n",
            "Epoch: 7 | Iteration: 482 | Classification loss: 0.19429 | Regression loss: 0.24924 | Running loss: 0.30514\n",
            "Epoch: 7 | Iteration: 483 | Classification loss: 0.11407 | Regression loss: 0.28613 | Running loss: 0.30466\n",
            "Epoch: 7 | Iteration: 484 | Classification loss: 0.05517 | Regression loss: 0.23004 | Running loss: 0.30404\n",
            "Epoch: 7 | Iteration: 485 | Classification loss: 0.08295 | Regression loss: 0.28036 | Running loss: 0.30435\n",
            "Epoch: 7 | Iteration: 486 | Classification loss: 0.05721 | Regression loss: 0.26298 | Running loss: 0.30499\n",
            "Epoch: 7 | Iteration: 487 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30407\n",
            "Epoch: 7 | Iteration: 488 | Classification loss: 0.03477 | Regression loss: 0.26338 | Running loss: 0.30374\n",
            "Epoch: 7 | Iteration: 489 | Classification loss: 0.15068 | Regression loss: 0.34179 | Running loss: 0.30337\n",
            "Epoch: 7 | Iteration: 490 | Classification loss: 0.27194 | Regression loss: 0.45828 | Running loss: 0.30332\n",
            "Epoch: 7 | Iteration: 491 | Classification loss: 0.17418 | Regression loss: 0.13147 | Running loss: 0.30330\n",
            "Epoch: 7 | Iteration: 492 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.30221\n",
            "Epoch: 7 | Iteration: 493 | Classification loss: 0.09643 | Regression loss: 0.33186 | Running loss: 0.30248\n",
            "Epoch: 7 | Iteration: 494 | Classification loss: 0.00009 | Regression loss: 0.00633 | Running loss: 0.30236\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.66s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.538\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.882\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.539\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.612\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.623\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.570\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.543\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 8 | Iteration: 0 | Classification loss: 0.02975 | Regression loss: 0.20868 | Running loss: 0.30158\n",
            "Epoch: 8 | Iteration: 1 | Classification loss: 0.00018 | Regression loss: 0.06175 | Running loss: 0.30058\n",
            "Epoch: 8 | Iteration: 2 | Classification loss: 0.02263 | Regression loss: 0.30548 | Running loss: 0.30016\n",
            "Epoch: 8 | Iteration: 3 | Classification loss: 0.00018 | Regression loss: 0.00687 | Running loss: 0.29952\n",
            "Epoch: 8 | Iteration: 4 | Classification loss: 0.06444 | Regression loss: 0.26819 | Running loss: 0.29929\n",
            "Epoch: 8 | Iteration: 5 | Classification loss: 0.16038 | Regression loss: 0.39781 | Running loss: 0.30003\n",
            "Epoch: 8 | Iteration: 6 | Classification loss: 0.06956 | Regression loss: 0.28819 | Running loss: 0.29982\n",
            "Epoch: 8 | Iteration: 7 | Classification loss: 0.10798 | Regression loss: 0.33785 | Running loss: 0.30038\n",
            "Epoch: 8 | Iteration: 8 | Classification loss: 0.02443 | Regression loss: 0.10725 | Running loss: 0.29971\n",
            "Epoch: 8 | Iteration: 9 | Classification loss: 0.11601 | Regression loss: 0.37763 | Running loss: 0.29990\n",
            "Epoch: 8 | Iteration: 10 | Classification loss: 0.02801 | Regression loss: 0.17795 | Running loss: 0.29987\n",
            "Epoch: 8 | Iteration: 11 | Classification loss: 0.04386 | Regression loss: 0.18855 | Running loss: 0.29966\n",
            "Epoch: 8 | Iteration: 12 | Classification loss: 0.00033 | Regression loss: 0.01155 | Running loss: 0.29854\n",
            "Epoch: 8 | Iteration: 13 | Classification loss: 0.00009 | Regression loss: 0.01033 | Running loss: 0.29850\n",
            "Epoch: 8 | Iteration: 14 | Classification loss: 0.01877 | Regression loss: 0.16445 | Running loss: 0.29822\n",
            "Epoch: 8 | Iteration: 15 | Classification loss: 0.03385 | Regression loss: 0.14644 | Running loss: 0.29796\n",
            "Epoch: 8 | Iteration: 16 | Classification loss: 0.03812 | Regression loss: 0.26727 | Running loss: 0.29748\n",
            "Epoch: 8 | Iteration: 17 | Classification loss: 0.03284 | Regression loss: 0.18483 | Running loss: 0.29729\n",
            "Epoch: 8 | Iteration: 18 | Classification loss: 0.14896 | Regression loss: 0.30260 | Running loss: 0.29707\n",
            "Epoch: 8 | Iteration: 19 | Classification loss: 0.06354 | Regression loss: 0.31361 | Running loss: 0.29663\n",
            "Epoch: 8 | Iteration: 20 | Classification loss: 0.09593 | Regression loss: 0.29288 | Running loss: 0.29670\n",
            "Epoch: 8 | Iteration: 21 | Classification loss: 0.07871 | Regression loss: 0.36628 | Running loss: 0.29756\n",
            "Epoch: 8 | Iteration: 22 | Classification loss: 0.09773 | Regression loss: 0.33965 | Running loss: 0.29801\n",
            "Epoch: 8 | Iteration: 23 | Classification loss: 0.05298 | Regression loss: 0.19481 | Running loss: 0.29818\n",
            "Epoch: 8 | Iteration: 24 | Classification loss: 0.11790 | Regression loss: 0.31310 | Running loss: 0.29871\n",
            "Epoch: 8 | Iteration: 25 | Classification loss: 0.05275 | Regression loss: 0.23221 | Running loss: 0.29899\n",
            "Epoch: 8 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29794\n",
            "Epoch: 8 | Iteration: 27 | Classification loss: 0.05604 | Regression loss: 0.25429 | Running loss: 0.29776\n",
            "Epoch: 8 | Iteration: 28 | Classification loss: 0.08597 | Regression loss: 0.28208 | Running loss: 0.29786\n",
            "Epoch: 8 | Iteration: 29 | Classification loss: 0.13278 | Regression loss: 0.35967 | Running loss: 0.29826\n",
            "Epoch: 8 | Iteration: 30 | Classification loss: 0.06691 | Regression loss: 0.33575 | Running loss: 0.29816\n",
            "Epoch: 8 | Iteration: 31 | Classification loss: 0.08561 | Regression loss: 0.31340 | Running loss: 0.29893\n",
            "Epoch: 8 | Iteration: 32 | Classification loss: 0.09806 | Regression loss: 0.30943 | Running loss: 0.29974\n",
            "Epoch: 8 | Iteration: 33 | Classification loss: 0.07187 | Regression loss: 0.24273 | Running loss: 0.30032\n",
            "Epoch: 8 | Iteration: 34 | Classification loss: 0.05499 | Regression loss: 0.28119 | Running loss: 0.29980\n",
            "Epoch: 8 | Iteration: 35 | Classification loss: 0.05204 | Regression loss: 0.32629 | Running loss: 0.30032\n",
            "Epoch: 8 | Iteration: 36 | Classification loss: 0.04644 | Regression loss: 0.22427 | Running loss: 0.30014\n",
            "Epoch: 8 | Iteration: 37 | Classification loss: 0.04872 | Regression loss: 0.28297 | Running loss: 0.30014\n",
            "Epoch: 8 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29944\n",
            "Epoch: 8 | Iteration: 39 | Classification loss: 0.00017 | Regression loss: 0.05631 | Running loss: 0.29927\n",
            "Epoch: 8 | Iteration: 40 | Classification loss: 0.07699 | Regression loss: 0.21275 | Running loss: 0.29929\n",
            "Epoch: 8 | Iteration: 41 | Classification loss: 0.00017 | Regression loss: 0.03530 | Running loss: 0.29927\n",
            "Epoch: 8 | Iteration: 42 | Classification loss: 0.03045 | Regression loss: 0.12331 | Running loss: 0.29957\n",
            "Epoch: 8 | Iteration: 43 | Classification loss: 0.06152 | Regression loss: 0.27047 | Running loss: 0.29932\n",
            "Epoch: 8 | Iteration: 44 | Classification loss: 0.02488 | Regression loss: 0.11859 | Running loss: 0.29932\n",
            "Epoch: 8 | Iteration: 45 | Classification loss: 0.12325 | Regression loss: 0.28575 | Running loss: 0.29946\n",
            "Epoch: 8 | Iteration: 46 | Classification loss: 0.00012 | Regression loss: 0.00594 | Running loss: 0.29858\n",
            "Epoch: 8 | Iteration: 47 | Classification loss: 0.18835 | Regression loss: 0.38751 | Running loss: 0.29964\n",
            "Epoch: 8 | Iteration: 48 | Classification loss: 0.03911 | Regression loss: 0.18515 | Running loss: 0.29974\n",
            "Epoch: 8 | Iteration: 49 | Classification loss: 0.03573 | Regression loss: 0.22507 | Running loss: 0.30024\n",
            "Epoch: 8 | Iteration: 50 | Classification loss: 0.09356 | Regression loss: 0.30965 | Running loss: 0.30069\n",
            "Epoch: 8 | Iteration: 51 | Classification loss: 0.16151 | Regression loss: 0.30154 | Running loss: 0.30162\n",
            "Epoch: 8 | Iteration: 52 | Classification loss: 0.08168 | Regression loss: 0.32666 | Running loss: 0.30201\n",
            "Epoch: 8 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.03454 | Running loss: 0.30101\n",
            "Epoch: 8 | Iteration: 54 | Classification loss: 0.04546 | Regression loss: 0.23535 | Running loss: 0.30023\n",
            "Epoch: 8 | Iteration: 55 | Classification loss: 0.04747 | Regression loss: 0.17276 | Running loss: 0.30037\n",
            "Epoch: 8 | Iteration: 56 | Classification loss: 0.07701 | Regression loss: 0.24551 | Running loss: 0.30055\n",
            "Epoch: 8 | Iteration: 57 | Classification loss: 0.02223 | Regression loss: 0.11696 | Running loss: 0.30030\n",
            "Epoch: 8 | Iteration: 58 | Classification loss: 0.00006 | Regression loss: 0.01042 | Running loss: 0.29941\n",
            "Epoch: 8 | Iteration: 59 | Classification loss: 0.23497 | Regression loss: 0.51318 | Running loss: 0.30084\n",
            "Epoch: 8 | Iteration: 60 | Classification loss: 0.00078 | Regression loss: 0.00699 | Running loss: 0.30040\n",
            "Epoch: 8 | Iteration: 61 | Classification loss: 0.05568 | Regression loss: 0.23255 | Running loss: 0.30020\n",
            "Epoch: 8 | Iteration: 62 | Classification loss: 0.02403 | Regression loss: 0.13901 | Running loss: 0.29986\n",
            "Epoch: 8 | Iteration: 63 | Classification loss: 0.07696 | Regression loss: 0.14414 | Running loss: 0.29969\n",
            "Epoch: 8 | Iteration: 64 | Classification loss: 0.17522 | Regression loss: 0.41680 | Running loss: 0.30025\n",
            "Epoch: 8 | Iteration: 65 | Classification loss: 0.09871 | Regression loss: 0.25497 | Running loss: 0.30034\n",
            "Epoch: 8 | Iteration: 66 | Classification loss: 0.00053 | Regression loss: 0.00897 | Running loss: 0.30014\n",
            "Epoch: 8 | Iteration: 67 | Classification loss: 0.12172 | Regression loss: 0.32727 | Running loss: 0.30044\n",
            "Epoch: 8 | Iteration: 68 | Classification loss: 0.05313 | Regression loss: 0.13515 | Running loss: 0.30037\n",
            "Epoch: 8 | Iteration: 69 | Classification loss: 0.20774 | Regression loss: 0.47415 | Running loss: 0.30134\n",
            "Epoch: 8 | Iteration: 70 | Classification loss: 0.05774 | Regression loss: 0.25199 | Running loss: 0.30169\n",
            "Epoch: 8 | Iteration: 71 | Classification loss: 0.15588 | Regression loss: 0.42082 | Running loss: 0.30144\n",
            "Epoch: 8 | Iteration: 72 | Classification loss: 0.08329 | Regression loss: 0.34211 | Running loss: 0.30210\n",
            "Epoch: 8 | Iteration: 73 | Classification loss: 0.00016 | Regression loss: 0.01645 | Running loss: 0.30120\n",
            "Epoch: 8 | Iteration: 74 | Classification loss: 0.02700 | Regression loss: 0.12702 | Running loss: 0.30074\n",
            "Epoch: 8 | Iteration: 75 | Classification loss: 0.04295 | Regression loss: 0.20914 | Running loss: 0.30058\n",
            "Epoch: 8 | Iteration: 76 | Classification loss: 0.07770 | Regression loss: 0.21440 | Running loss: 0.30114\n",
            "Epoch: 8 | Iteration: 77 | Classification loss: 0.13995 | Regression loss: 0.34618 | Running loss: 0.30122\n",
            "Epoch: 8 | Iteration: 78 | Classification loss: 0.20240 | Regression loss: 0.30745 | Running loss: 0.30191\n",
            "Epoch: 8 | Iteration: 79 | Classification loss: 0.00010 | Regression loss: 0.01864 | Running loss: 0.30107\n",
            "Epoch: 8 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30066\n",
            "Epoch: 8 | Iteration: 81 | Classification loss: 0.05807 | Regression loss: 0.27266 | Running loss: 0.30058\n",
            "Epoch: 8 | Iteration: 82 | Classification loss: 0.00149 | Regression loss: 0.00577 | Running loss: 0.29986\n",
            "Epoch: 8 | Iteration: 83 | Classification loss: 0.20737 | Regression loss: 0.41386 | Running loss: 0.30066\n",
            "Epoch: 8 | Iteration: 84 | Classification loss: 0.15070 | Regression loss: 0.41405 | Running loss: 0.30175\n",
            "Epoch: 8 | Iteration: 85 | Classification loss: 0.06359 | Regression loss: 0.27367 | Running loss: 0.30122\n",
            "Epoch: 8 | Iteration: 86 | Classification loss: 0.08556 | Regression loss: 0.27875 | Running loss: 0.30143\n",
            "Epoch: 8 | Iteration: 87 | Classification loss: 0.04092 | Regression loss: 0.23045 | Running loss: 0.30160\n",
            "Epoch: 8 | Iteration: 88 | Classification loss: 0.02834 | Regression loss: 0.17807 | Running loss: 0.30119\n",
            "Epoch: 8 | Iteration: 89 | Classification loss: 0.00018 | Regression loss: 0.03813 | Running loss: 0.30090\n",
            "Epoch: 8 | Iteration: 90 | Classification loss: 0.02962 | Regression loss: 0.12763 | Running loss: 0.30113\n",
            "Epoch: 8 | Iteration: 91 | Classification loss: 0.07542 | Regression loss: 0.24693 | Running loss: 0.30155\n",
            "Epoch: 8 | Iteration: 92 | Classification loss: 0.04173 | Regression loss: 0.22706 | Running loss: 0.30191\n",
            "Epoch: 8 | Iteration: 93 | Classification loss: 0.05222 | Regression loss: 0.13264 | Running loss: 0.30172\n",
            "Epoch: 8 | Iteration: 94 | Classification loss: 0.04953 | Regression loss: 0.15710 | Running loss: 0.30208\n",
            "Epoch: 8 | Iteration: 95 | Classification loss: 0.05993 | Regression loss: 0.19818 | Running loss: 0.30193\n",
            "Epoch: 8 | Iteration: 96 | Classification loss: 0.04177 | Regression loss: 0.26324 | Running loss: 0.30244\n",
            "Epoch: 8 | Iteration: 97 | Classification loss: 0.12716 | Regression loss: 0.39819 | Running loss: 0.30184\n",
            "Epoch: 8 | Iteration: 98 | Classification loss: 0.04174 | Regression loss: 0.11193 | Running loss: 0.30092\n",
            "Epoch: 8 | Iteration: 99 | Classification loss: 0.04631 | Regression loss: 0.22957 | Running loss: 0.30094\n",
            "Epoch: 8 | Iteration: 100 | Classification loss: 0.04070 | Regression loss: 0.21922 | Running loss: 0.30100\n",
            "Epoch: 8 | Iteration: 101 | Classification loss: 0.02533 | Regression loss: 0.18867 | Running loss: 0.30019\n",
            "Epoch: 8 | Iteration: 102 | Classification loss: 0.05528 | Regression loss: 0.20603 | Running loss: 0.30004\n",
            "Epoch: 8 | Iteration: 103 | Classification loss: 0.01664 | Regression loss: 0.13921 | Running loss: 0.29981\n",
            "Epoch: 8 | Iteration: 104 | Classification loss: 0.00009 | Regression loss: 0.06069 | Running loss: 0.29948\n",
            "Epoch: 8 | Iteration: 105 | Classification loss: 0.17815 | Regression loss: 0.47675 | Running loss: 0.30066\n",
            "Epoch: 8 | Iteration: 106 | Classification loss: 0.16197 | Regression loss: 0.48446 | Running loss: 0.30151\n",
            "Epoch: 8 | Iteration: 107 | Classification loss: 0.02666 | Regression loss: 0.17324 | Running loss: 0.30152\n",
            "Epoch: 8 | Iteration: 108 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30149\n",
            "Epoch: 8 | Iteration: 109 | Classification loss: 0.00044 | Regression loss: 0.07894 | Running loss: 0.30092\n",
            "Epoch: 8 | Iteration: 110 | Classification loss: 0.00044 | Regression loss: 0.06486 | Running loss: 0.30020\n",
            "Epoch: 8 | Iteration: 111 | Classification loss: 0.13373 | Regression loss: 0.47413 | Running loss: 0.30096\n",
            "Epoch: 8 | Iteration: 112 | Classification loss: 0.07461 | Regression loss: 0.32289 | Running loss: 0.30089\n",
            "Epoch: 8 | Iteration: 113 | Classification loss: 0.01969 | Regression loss: 0.14126 | Running loss: 0.30054\n",
            "Epoch: 8 | Iteration: 114 | Classification loss: 0.06416 | Regression loss: 0.23717 | Running loss: 0.30074\n",
            "Epoch: 8 | Iteration: 115 | Classification loss: 0.05396 | Regression loss: 0.20247 | Running loss: 0.30032\n",
            "Epoch: 8 | Iteration: 116 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29936\n",
            "Epoch: 8 | Iteration: 117 | Classification loss: 0.06239 | Regression loss: 0.30645 | Running loss: 0.29901\n",
            "Epoch: 8 | Iteration: 118 | Classification loss: 0.17771 | Regression loss: 0.42888 | Running loss: 0.29989\n",
            "Epoch: 8 | Iteration: 119 | Classification loss: 0.02095 | Regression loss: 0.13841 | Running loss: 0.29953\n",
            "Epoch: 8 | Iteration: 120 | Classification loss: 0.09119 | Regression loss: 0.38637 | Running loss: 0.29964\n",
            "Epoch: 8 | Iteration: 121 | Classification loss: 0.14173 | Regression loss: 0.48505 | Running loss: 0.29918\n",
            "Epoch: 8 | Iteration: 122 | Classification loss: 0.13824 | Regression loss: 0.37696 | Running loss: 0.29972\n",
            "Epoch: 8 | Iteration: 123 | Classification loss: 0.00010 | Regression loss: 0.06995 | Running loss: 0.29973\n",
            "Epoch: 8 | Iteration: 124 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29973\n",
            "Epoch: 8 | Iteration: 125 | Classification loss: 0.00011 | Regression loss: 0.04603 | Running loss: 0.29923\n",
            "Epoch: 8 | Iteration: 126 | Classification loss: 0.02858 | Regression loss: 0.10868 | Running loss: 0.29824\n",
            "Epoch: 8 | Iteration: 127 | Classification loss: 0.07671 | Regression loss: 0.30638 | Running loss: 0.29845\n",
            "Epoch: 8 | Iteration: 128 | Classification loss: 0.11413 | Regression loss: 0.37935 | Running loss: 0.29866\n",
            "Epoch: 8 | Iteration: 129 | Classification loss: 0.07017 | Regression loss: 0.25526 | Running loss: 0.29848\n",
            "Epoch: 8 | Iteration: 130 | Classification loss: 0.12981 | Regression loss: 0.36482 | Running loss: 0.29785\n",
            "Epoch: 8 | Iteration: 131 | Classification loss: 0.04976 | Regression loss: 0.31285 | Running loss: 0.29826\n",
            "Epoch: 8 | Iteration: 132 | Classification loss: 0.00036 | Regression loss: 0.13650 | Running loss: 0.29840\n",
            "Epoch: 8 | Iteration: 133 | Classification loss: 0.02381 | Regression loss: 0.11458 | Running loss: 0.29808\n",
            "Epoch: 8 | Iteration: 134 | Classification loss: 0.15695 | Regression loss: 0.49199 | Running loss: 0.29896\n",
            "Epoch: 8 | Iteration: 135 | Classification loss: 0.10992 | Regression loss: 0.31187 | Running loss: 0.29890\n",
            "Epoch: 8 | Iteration: 136 | Classification loss: 0.06853 | Regression loss: 0.32705 | Running loss: 0.29926\n",
            "Epoch: 8 | Iteration: 137 | Classification loss: 0.03676 | Regression loss: 0.15293 | Running loss: 0.29879\n",
            "Epoch: 8 | Iteration: 138 | Classification loss: 0.17400 | Regression loss: 0.43428 | Running loss: 0.29915\n",
            "Epoch: 8 | Iteration: 139 | Classification loss: 0.08147 | Regression loss: 0.24584 | Running loss: 0.29935\n",
            "Epoch: 8 | Iteration: 140 | Classification loss: 0.05330 | Regression loss: 0.22199 | Running loss: 0.29964\n",
            "Epoch: 8 | Iteration: 141 | Classification loss: 0.01784 | Regression loss: 0.14609 | Running loss: 0.29936\n",
            "Epoch: 8 | Iteration: 142 | Classification loss: 0.18929 | Regression loss: 0.57651 | Running loss: 0.29933\n",
            "Epoch: 8 | Iteration: 143 | Classification loss: 0.04003 | Regression loss: 0.20892 | Running loss: 0.29919\n",
            "Epoch: 8 | Iteration: 144 | Classification loss: 0.12621 | Regression loss: 0.61801 | Running loss: 0.30007\n",
            "Epoch: 8 | Iteration: 145 | Classification loss: 0.01682 | Regression loss: 0.12006 | Running loss: 0.30030\n",
            "Epoch: 8 | Iteration: 146 | Classification loss: 0.01453 | Regression loss: 0.11903 | Running loss: 0.30045\n",
            "Epoch: 8 | Iteration: 147 | Classification loss: 0.00012 | Regression loss: 0.02206 | Running loss: 0.30044\n",
            "Epoch: 8 | Iteration: 148 | Classification loss: 0.06724 | Regression loss: 0.25567 | Running loss: 0.30060\n",
            "Epoch: 8 | Iteration: 149 | Classification loss: 0.10074 | Regression loss: 0.15626 | Running loss: 0.30106\n",
            "Epoch: 8 | Iteration: 150 | Classification loss: 0.06331 | Regression loss: 0.26307 | Running loss: 0.30071\n",
            "Epoch: 8 | Iteration: 151 | Classification loss: 0.07345 | Regression loss: 0.36406 | Running loss: 0.30143\n",
            "Epoch: 8 | Iteration: 152 | Classification loss: 0.06565 | Regression loss: 0.33038 | Running loss: 0.30123\n",
            "Epoch: 8 | Iteration: 153 | Classification loss: 0.06760 | Regression loss: 0.19070 | Running loss: 0.30132\n",
            "Epoch: 8 | Iteration: 154 | Classification loss: 0.00011 | Regression loss: 0.02016 | Running loss: 0.30124\n",
            "Epoch: 8 | Iteration: 155 | Classification loss: 0.10547 | Regression loss: 0.19742 | Running loss: 0.30162\n",
            "Epoch: 8 | Iteration: 156 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.30071\n",
            "Epoch: 8 | Iteration: 157 | Classification loss: 0.22426 | Regression loss: 0.48032 | Running loss: 0.30152\n",
            "Epoch: 8 | Iteration: 158 | Classification loss: 0.09728 | Regression loss: 0.37158 | Running loss: 0.30152\n",
            "Epoch: 8 | Iteration: 159 | Classification loss: 0.10860 | Regression loss: 0.32822 | Running loss: 0.30176\n",
            "Epoch: 8 | Iteration: 160 | Classification loss: 0.06943 | Regression loss: 0.22800 | Running loss: 0.30142\n",
            "Epoch: 8 | Iteration: 161 | Classification loss: 0.00016 | Regression loss: 0.02083 | Running loss: 0.30013\n",
            "Epoch: 8 | Iteration: 162 | Classification loss: 0.08288 | Regression loss: 0.13079 | Running loss: 0.29959\n",
            "Epoch: 8 | Iteration: 163 | Classification loss: 0.00023 | Regression loss: 0.02760 | Running loss: 0.29886\n",
            "Epoch: 8 | Iteration: 164 | Classification loss: 0.06095 | Regression loss: 0.26718 | Running loss: 0.29888\n",
            "Epoch: 8 | Iteration: 165 | Classification loss: 0.14818 | Regression loss: 0.43311 | Running loss: 0.29969\n",
            "Epoch: 8 | Iteration: 166 | Classification loss: 0.04633 | Regression loss: 0.22860 | Running loss: 0.29948\n",
            "Epoch: 8 | Iteration: 167 | Classification loss: 0.00014 | Regression loss: 0.01344 | Running loss: 0.29871\n",
            "Epoch: 8 | Iteration: 168 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29871\n",
            "Epoch: 8 | Iteration: 169 | Classification loss: 0.00009 | Regression loss: 0.00893 | Running loss: 0.29753\n",
            "Epoch: 8 | Iteration: 170 | Classification loss: 0.25594 | Regression loss: 0.55903 | Running loss: 0.29793\n",
            "Epoch: 8 | Iteration: 171 | Classification loss: 0.06875 | Regression loss: 0.37024 | Running loss: 0.29806\n",
            "Epoch: 8 | Iteration: 172 | Classification loss: 0.00008 | Regression loss: 0.01129 | Running loss: 0.29747\n",
            "Epoch: 8 | Iteration: 173 | Classification loss: 0.05213 | Regression loss: 0.24717 | Running loss: 0.29799\n",
            "Epoch: 8 | Iteration: 174 | Classification loss: 0.07428 | Regression loss: 0.17834 | Running loss: 0.29800\n",
            "Epoch: 8 | Iteration: 175 | Classification loss: 0.08686 | Regression loss: 0.28074 | Running loss: 0.29756\n",
            "Epoch: 8 | Iteration: 176 | Classification loss: 0.09448 | Regression loss: 0.24257 | Running loss: 0.29767\n",
            "Epoch: 8 | Iteration: 177 | Classification loss: 0.03397 | Regression loss: 0.19495 | Running loss: 0.29756\n",
            "Epoch: 8 | Iteration: 178 | Classification loss: 0.06345 | Regression loss: 0.27007 | Running loss: 0.29718\n",
            "Epoch: 8 | Iteration: 179 | Classification loss: 0.07356 | Regression loss: 0.25802 | Running loss: 0.29719\n",
            "Epoch: 8 | Iteration: 180 | Classification loss: 0.00004 | Regression loss: 0.00000 | Running loss: 0.29688\n",
            "Epoch: 8 | Iteration: 181 | Classification loss: 0.01473 | Regression loss: 0.11636 | Running loss: 0.29669\n",
            "Epoch: 8 | Iteration: 182 | Classification loss: 0.11356 | Regression loss: 0.26983 | Running loss: 0.29746\n",
            "Epoch: 8 | Iteration: 183 | Classification loss: 0.08082 | Regression loss: 0.17855 | Running loss: 0.29725\n",
            "Epoch: 8 | Iteration: 184 | Classification loss: 0.06797 | Regression loss: 0.34270 | Running loss: 0.29727\n",
            "Epoch: 8 | Iteration: 185 | Classification loss: 0.04978 | Regression loss: 0.08769 | Running loss: 0.29725\n",
            "Epoch: 8 | Iteration: 186 | Classification loss: 0.10265 | Regression loss: 0.35502 | Running loss: 0.29817\n",
            "Epoch: 8 | Iteration: 187 | Classification loss: 0.12441 | Regression loss: 0.25086 | Running loss: 0.29850\n",
            "Epoch: 8 | Iteration: 188 | Classification loss: 0.15797 | Regression loss: 0.34706 | Running loss: 0.29828\n",
            "Epoch: 8 | Iteration: 189 | Classification loss: 0.06046 | Regression loss: 0.27912 | Running loss: 0.29868\n",
            "Epoch: 8 | Iteration: 190 | Classification loss: 0.05516 | Regression loss: 0.14406 | Running loss: 0.29875\n",
            "Epoch: 8 | Iteration: 191 | Classification loss: 0.02207 | Regression loss: 0.09398 | Running loss: 0.29898\n",
            "Epoch: 8 | Iteration: 192 | Classification loss: 0.00019 | Regression loss: 0.02746 | Running loss: 0.29901\n",
            "Epoch: 8 | Iteration: 193 | Classification loss: 0.02428 | Regression loss: 0.18247 | Running loss: 0.29885\n",
            "Epoch: 8 | Iteration: 194 | Classification loss: 0.02937 | Regression loss: 0.19432 | Running loss: 0.29824\n",
            "Epoch: 8 | Iteration: 195 | Classification loss: 0.00026 | Regression loss: 0.01708 | Running loss: 0.29742\n",
            "Epoch: 8 | Iteration: 196 | Classification loss: 0.00021 | Regression loss: 0.01630 | Running loss: 0.29695\n",
            "Epoch: 8 | Iteration: 197 | Classification loss: 0.05819 | Regression loss: 0.25091 | Running loss: 0.29503\n",
            "Epoch: 8 | Iteration: 198 | Classification loss: 0.12389 | Regression loss: 0.34887 | Running loss: 0.29563\n",
            "Epoch: 8 | Iteration: 199 | Classification loss: 0.20824 | Regression loss: 0.37529 | Running loss: 0.29677\n",
            "Epoch: 8 | Iteration: 200 | Classification loss: 0.18751 | Regression loss: 0.19311 | Running loss: 0.29747\n",
            "Epoch: 8 | Iteration: 201 | Classification loss: 0.03607 | Regression loss: 0.14739 | Running loss: 0.29781\n",
            "Epoch: 8 | Iteration: 202 | Classification loss: 0.06283 | Regression loss: 0.30660 | Running loss: 0.29761\n",
            "Epoch: 8 | Iteration: 203 | Classification loss: 0.02271 | Regression loss: 0.09524 | Running loss: 0.29758\n",
            "Epoch: 8 | Iteration: 204 | Classification loss: 0.09499 | Regression loss: 0.33998 | Running loss: 0.29840\n",
            "Epoch: 8 | Iteration: 205 | Classification loss: 0.09875 | Regression loss: 0.25676 | Running loss: 0.29829\n",
            "Epoch: 8 | Iteration: 206 | Classification loss: 0.09163 | Regression loss: 0.32505 | Running loss: 0.29821\n",
            "Epoch: 8 | Iteration: 207 | Classification loss: 0.01493 | Regression loss: 0.10864 | Running loss: 0.29718\n",
            "Epoch: 8 | Iteration: 208 | Classification loss: 0.00004 | Regression loss: 0.04392 | Running loss: 0.29670\n",
            "Epoch: 8 | Iteration: 209 | Classification loss: 0.00033 | Regression loss: 0.05103 | Running loss: 0.29650\n",
            "Epoch: 8 | Iteration: 210 | Classification loss: 0.03126 | Regression loss: 0.19426 | Running loss: 0.29598\n",
            "Epoch: 8 | Iteration: 211 | Classification loss: 0.05813 | Regression loss: 0.13907 | Running loss: 0.29529\n",
            "Epoch: 8 | Iteration: 212 | Classification loss: 0.00006 | Regression loss: 0.01246 | Running loss: 0.29499\n",
            "Epoch: 8 | Iteration: 213 | Classification loss: 0.06835 | Regression loss: 0.32773 | Running loss: 0.29573\n",
            "Epoch: 8 | Iteration: 214 | Classification loss: 0.05049 | Regression loss: 0.24560 | Running loss: 0.29589\n",
            "Epoch: 8 | Iteration: 215 | Classification loss: 0.08721 | Regression loss: 0.32920 | Running loss: 0.29640\n",
            "Epoch: 8 | Iteration: 216 | Classification loss: 0.02147 | Regression loss: 0.13157 | Running loss: 0.29583\n",
            "Epoch: 8 | Iteration: 217 | Classification loss: 0.11819 | Regression loss: 0.19483 | Running loss: 0.29610\n",
            "Epoch: 8 | Iteration: 218 | Classification loss: 0.07689 | Regression loss: 0.28869 | Running loss: 0.29572\n",
            "Epoch: 8 | Iteration: 219 | Classification loss: 0.02012 | Regression loss: 0.23561 | Running loss: 0.29619\n",
            "Epoch: 8 | Iteration: 220 | Classification loss: 0.04147 | Regression loss: 0.17769 | Running loss: 0.29608\n",
            "Epoch: 8 | Iteration: 221 | Classification loss: 0.04266 | Regression loss: 0.16063 | Running loss: 0.29530\n",
            "Epoch: 8 | Iteration: 222 | Classification loss: 0.10707 | Regression loss: 0.19128 | Running loss: 0.29588\n",
            "Epoch: 8 | Iteration: 223 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29586\n",
            "Epoch: 8 | Iteration: 224 | Classification loss: 0.02655 | Regression loss: 0.13857 | Running loss: 0.29618\n",
            "Epoch: 8 | Iteration: 225 | Classification loss: 0.14445 | Regression loss: 0.47567 | Running loss: 0.29672\n",
            "Epoch: 8 | Iteration: 226 | Classification loss: 0.00013 | Regression loss: 0.04522 | Running loss: 0.29658\n",
            "Epoch: 8 | Iteration: 227 | Classification loss: 0.05501 | Regression loss: 0.12942 | Running loss: 0.29658\n",
            "Epoch: 8 | Iteration: 228 | Classification loss: 0.01093 | Regression loss: 0.09812 | Running loss: 0.29659\n",
            "Epoch: 8 | Iteration: 229 | Classification loss: 0.06657 | Regression loss: 0.29834 | Running loss: 0.29595\n",
            "Epoch: 8 | Iteration: 230 | Classification loss: 0.32198 | Regression loss: 0.55314 | Running loss: 0.29698\n",
            "Epoch: 8 | Iteration: 231 | Classification loss: 0.05746 | Regression loss: 0.22293 | Running loss: 0.29751\n",
            "Epoch: 8 | Iteration: 232 | Classification loss: 0.04189 | Regression loss: 0.11534 | Running loss: 0.29698\n",
            "Epoch: 8 | Iteration: 233 | Classification loss: 0.22067 | Regression loss: 0.38654 | Running loss: 0.29727\n",
            "Epoch: 8 | Iteration: 234 | Classification loss: 0.01811 | Regression loss: 0.18193 | Running loss: 0.29675\n",
            "Epoch: 8 | Iteration: 235 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29674\n",
            "Epoch: 8 | Iteration: 236 | Classification loss: 0.08074 | Regression loss: 0.27603 | Running loss: 0.29629\n",
            "Epoch: 8 | Iteration: 237 | Classification loss: 0.04628 | Regression loss: 0.14755 | Running loss: 0.29570\n",
            "Epoch: 8 | Iteration: 238 | Classification loss: 0.19992 | Regression loss: 0.44604 | Running loss: 0.29605\n",
            "Epoch: 8 | Iteration: 239 | Classification loss: 0.01952 | Regression loss: 0.17155 | Running loss: 0.29597\n",
            "Epoch: 8 | Iteration: 240 | Classification loss: 0.06439 | Regression loss: 0.23051 | Running loss: 0.29626\n",
            "Epoch: 8 | Iteration: 241 | Classification loss: 0.08275 | Regression loss: 0.26766 | Running loss: 0.29602\n",
            "Epoch: 8 | Iteration: 242 | Classification loss: 0.00009 | Regression loss: 0.09795 | Running loss: 0.29593\n",
            "Epoch: 8 | Iteration: 243 | Classification loss: 0.09917 | Regression loss: 0.29131 | Running loss: 0.29585\n",
            "Epoch: 8 | Iteration: 244 | Classification loss: 0.01540 | Regression loss: 0.07920 | Running loss: 0.29599\n",
            "Epoch: 8 | Iteration: 245 | Classification loss: 0.09903 | Regression loss: 0.13799 | Running loss: 0.29508\n",
            "Epoch: 8 | Iteration: 246 | Classification loss: 0.14790 | Regression loss: 0.38322 | Running loss: 0.29587\n",
            "Epoch: 8 | Iteration: 247 | Classification loss: 0.05236 | Regression loss: 0.23529 | Running loss: 0.29559\n",
            "Epoch: 8 | Iteration: 248 | Classification loss: 0.00018 | Regression loss: 0.07302 | Running loss: 0.29427\n",
            "Epoch: 8 | Iteration: 249 | Classification loss: 0.00029 | Regression loss: 0.07079 | Running loss: 0.29437\n",
            "Epoch: 8 | Iteration: 250 | Classification loss: 0.00283 | Regression loss: 0.00000 | Running loss: 0.29393\n",
            "Epoch: 8 | Iteration: 251 | Classification loss: 0.12941 | Regression loss: 0.37406 | Running loss: 0.29430\n",
            "Epoch: 8 | Iteration: 252 | Classification loss: 0.04949 | Regression loss: 0.26125 | Running loss: 0.29461\n",
            "Epoch: 8 | Iteration: 253 | Classification loss: 0.03012 | Regression loss: 0.15632 | Running loss: 0.29365\n",
            "Epoch: 8 | Iteration: 254 | Classification loss: 0.00141 | Regression loss: 0.04710 | Running loss: 0.29302\n",
            "Epoch: 8 | Iteration: 255 | Classification loss: 0.03842 | Regression loss: 0.15557 | Running loss: 0.29274\n",
            "Epoch: 8 | Iteration: 256 | Classification loss: 0.11261 | Regression loss: 0.30683 | Running loss: 0.29322\n",
            "Epoch: 8 | Iteration: 257 | Classification loss: 0.11269 | Regression loss: 0.31062 | Running loss: 0.29353\n",
            "Epoch: 8 | Iteration: 258 | Classification loss: 0.07867 | Regression loss: 0.19018 | Running loss: 0.29338\n",
            "Epoch: 8 | Iteration: 259 | Classification loss: 0.24031 | Regression loss: 0.40463 | Running loss: 0.29463\n",
            "Epoch: 8 | Iteration: 260 | Classification loss: 0.07696 | Regression loss: 0.32137 | Running loss: 0.29489\n",
            "Epoch: 8 | Iteration: 261 | Classification loss: 0.04597 | Regression loss: 0.23913 | Running loss: 0.29399\n",
            "Epoch: 8 | Iteration: 262 | Classification loss: 0.02546 | Regression loss: 0.12066 | Running loss: 0.29348\n",
            "Epoch: 8 | Iteration: 263 | Classification loss: 0.00695 | Regression loss: 0.00915 | Running loss: 0.29350\n",
            "Epoch: 8 | Iteration: 264 | Classification loss: 0.16537 | Regression loss: 0.32100 | Running loss: 0.29403\n",
            "Epoch: 8 | Iteration: 265 | Classification loss: 0.09112 | Regression loss: 0.11013 | Running loss: 0.29427\n",
            "Epoch: 8 | Iteration: 266 | Classification loss: 0.05866 | Regression loss: 0.24232 | Running loss: 0.29485\n",
            "Epoch: 8 | Iteration: 267 | Classification loss: 0.05140 | Regression loss: 0.14111 | Running loss: 0.29479\n",
            "Epoch: 8 | Iteration: 268 | Classification loss: 0.04073 | Regression loss: 0.11756 | Running loss: 0.29455\n",
            "Epoch: 8 | Iteration: 269 | Classification loss: 0.04731 | Regression loss: 0.16300 | Running loss: 0.29454\n",
            "Epoch: 8 | Iteration: 270 | Classification loss: 0.16812 | Regression loss: 0.26746 | Running loss: 0.29454\n",
            "Epoch: 8 | Iteration: 271 | Classification loss: 0.04191 | Regression loss: 0.23270 | Running loss: 0.29505\n",
            "Epoch: 8 | Iteration: 272 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29501\n",
            "Epoch: 8 | Iteration: 273 | Classification loss: 0.10706 | Regression loss: 0.25393 | Running loss: 0.29526\n",
            "Epoch: 8 | Iteration: 274 | Classification loss: 0.01209 | Regression loss: 0.08488 | Running loss: 0.29459\n",
            "Epoch: 8 | Iteration: 275 | Classification loss: 0.05565 | Regression loss: 0.14511 | Running loss: 0.29438\n",
            "Epoch: 8 | Iteration: 276 | Classification loss: 0.11798 | Regression loss: 0.16691 | Running loss: 0.29483\n",
            "Epoch: 8 | Iteration: 277 | Classification loss: 0.04730 | Regression loss: 0.15290 | Running loss: 0.29434\n",
            "Epoch: 8 | Iteration: 278 | Classification loss: 0.00005 | Regression loss: 0.01975 | Running loss: 0.29343\n",
            "Epoch: 8 | Iteration: 279 | Classification loss: 0.06679 | Regression loss: 0.25001 | Running loss: 0.29396\n",
            "Epoch: 8 | Iteration: 280 | Classification loss: 0.00148 | Regression loss: 0.00346 | Running loss: 0.29350\n",
            "Epoch: 8 | Iteration: 281 | Classification loss: 0.08263 | Regression loss: 0.42737 | Running loss: 0.29385\n",
            "Epoch: 8 | Iteration: 282 | Classification loss: 0.14505 | Regression loss: 0.33380 | Running loss: 0.29421\n",
            "Epoch: 8 | Iteration: 283 | Classification loss: 0.00005 | Regression loss: 0.01095 | Running loss: 0.29417\n",
            "Epoch: 8 | Iteration: 284 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29337\n",
            "Epoch: 8 | Iteration: 285 | Classification loss: 0.12423 | Regression loss: 0.33528 | Running loss: 0.29355\n",
            "Epoch: 8 | Iteration: 286 | Classification loss: 0.13365 | Regression loss: 0.35778 | Running loss: 0.29452\n",
            "Epoch: 8 | Iteration: 287 | Classification loss: 0.00052 | Regression loss: 0.00433 | Running loss: 0.29342\n",
            "Epoch: 8 | Iteration: 288 | Classification loss: 0.07780 | Regression loss: 0.25359 | Running loss: 0.29408\n",
            "Epoch: 8 | Iteration: 289 | Classification loss: 0.02953 | Regression loss: 0.10046 | Running loss: 0.29331\n",
            "Epoch: 8 | Iteration: 290 | Classification loss: 0.01847 | Regression loss: 0.09538 | Running loss: 0.29305\n",
            "Epoch: 8 | Iteration: 291 | Classification loss: 0.16961 | Regression loss: 0.35552 | Running loss: 0.29377\n",
            "Epoch: 8 | Iteration: 292 | Classification loss: 0.01163 | Regression loss: 0.08917 | Running loss: 0.29357\n",
            "Epoch: 8 | Iteration: 293 | Classification loss: 0.02542 | Regression loss: 0.15884 | Running loss: 0.29354\n",
            "Epoch: 8 | Iteration: 294 | Classification loss: 0.01100 | Regression loss: 0.07240 | Running loss: 0.29313\n",
            "Epoch: 8 | Iteration: 295 | Classification loss: 0.05356 | Regression loss: 0.13913 | Running loss: 0.29237\n",
            "Epoch: 8 | Iteration: 296 | Classification loss: 0.03248 | Regression loss: 0.13343 | Running loss: 0.29270\n",
            "Epoch: 8 | Iteration: 297 | Classification loss: 0.04470 | Regression loss: 0.23371 | Running loss: 0.29320\n",
            "Epoch: 8 | Iteration: 298 | Classification loss: 0.05822 | Regression loss: 0.22228 | Running loss: 0.29299\n",
            "Epoch: 8 | Iteration: 299 | Classification loss: 0.11902 | Regression loss: 0.37557 | Running loss: 0.29295\n",
            "Epoch: 8 | Iteration: 300 | Classification loss: 0.05538 | Regression loss: 0.10450 | Running loss: 0.29327\n",
            "Epoch: 8 | Iteration: 301 | Classification loss: 0.07331 | Regression loss: 0.23920 | Running loss: 0.29379\n",
            "Epoch: 8 | Iteration: 302 | Classification loss: 0.09884 | Regression loss: 0.26024 | Running loss: 0.29445\n",
            "Epoch: 8 | Iteration: 303 | Classification loss: 0.21931 | Regression loss: 0.47765 | Running loss: 0.29519\n",
            "Epoch: 8 | Iteration: 304 | Classification loss: 0.04988 | Regression loss: 0.10146 | Running loss: 0.29514\n",
            "Epoch: 8 | Iteration: 305 | Classification loss: 0.05326 | Regression loss: 0.22294 | Running loss: 0.29541\n",
            "Epoch: 8 | Iteration: 306 | Classification loss: 0.06602 | Regression loss: 0.19365 | Running loss: 0.29540\n",
            "Epoch: 8 | Iteration: 307 | Classification loss: 0.00016 | Regression loss: 0.01241 | Running loss: 0.29487\n",
            "Epoch: 8 | Iteration: 308 | Classification loss: 0.00022 | Regression loss: 0.00890 | Running loss: 0.29451\n",
            "Epoch: 8 | Iteration: 309 | Classification loss: 0.12052 | Regression loss: 0.30487 | Running loss: 0.29497\n",
            "Epoch: 8 | Iteration: 310 | Classification loss: 0.04782 | Regression loss: 0.16067 | Running loss: 0.29524\n",
            "Epoch: 8 | Iteration: 311 | Classification loss: 0.06756 | Regression loss: 0.19302 | Running loss: 0.29550\n",
            "Epoch: 8 | Iteration: 312 | Classification loss: 0.02413 | Regression loss: 0.13342 | Running loss: 0.29376\n",
            "Epoch: 8 | Iteration: 313 | Classification loss: 0.12377 | Regression loss: 0.43472 | Running loss: 0.29424\n",
            "Epoch: 8 | Iteration: 314 | Classification loss: 0.00006 | Regression loss: 0.05152 | Running loss: 0.29304\n",
            "Epoch: 8 | Iteration: 315 | Classification loss: 0.05834 | Regression loss: 0.29895 | Running loss: 0.29303\n",
            "Epoch: 8 | Iteration: 316 | Classification loss: 0.07662 | Regression loss: 0.20592 | Running loss: 0.29304\n",
            "Epoch: 8 | Iteration: 317 | Classification loss: 0.11421 | Regression loss: 0.35095 | Running loss: 0.29329\n",
            "Epoch: 8 | Iteration: 318 | Classification loss: 0.04002 | Regression loss: 0.20643 | Running loss: 0.29320\n",
            "Epoch: 8 | Iteration: 319 | Classification loss: 0.00006 | Regression loss: 0.01152 | Running loss: 0.29232\n",
            "Epoch: 8 | Iteration: 320 | Classification loss: 0.04368 | Regression loss: 0.22656 | Running loss: 0.29273\n",
            "Epoch: 8 | Iteration: 321 | Classification loss: 0.02028 | Regression loss: 0.14806 | Running loss: 0.29259\n",
            "Epoch: 8 | Iteration: 322 | Classification loss: 0.12928 | Regression loss: 0.42089 | Running loss: 0.29363\n",
            "Epoch: 8 | Iteration: 323 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.29245\n",
            "Epoch: 8 | Iteration: 324 | Classification loss: 0.03028 | Regression loss: 0.14846 | Running loss: 0.29194\n",
            "Epoch: 8 | Iteration: 325 | Classification loss: 0.08501 | Regression loss: 0.27897 | Running loss: 0.29121\n",
            "Epoch: 8 | Iteration: 326 | Classification loss: 0.07409 | Regression loss: 0.30525 | Running loss: 0.29096\n",
            "Epoch: 8 | Iteration: 327 | Classification loss: 0.00014 | Regression loss: 0.01506 | Running loss: 0.29063\n",
            "Epoch: 8 | Iteration: 328 | Classification loss: 0.11899 | Regression loss: 0.30449 | Running loss: 0.29074\n",
            "Epoch: 8 | Iteration: 329 | Classification loss: 0.00009 | Regression loss: 0.02641 | Running loss: 0.29020\n",
            "Epoch: 8 | Iteration: 330 | Classification loss: 0.15008 | Regression loss: 0.39389 | Running loss: 0.29063\n",
            "Epoch: 8 | Iteration: 331 | Classification loss: 0.03136 | Regression loss: 0.15559 | Running loss: 0.29018\n",
            "Epoch: 8 | Iteration: 332 | Classification loss: 0.03661 | Regression loss: 0.15089 | Running loss: 0.28982\n",
            "Epoch: 8 | Iteration: 333 | Classification loss: 0.04011 | Regression loss: 0.14147 | Running loss: 0.28965\n",
            "Epoch: 8 | Iteration: 334 | Classification loss: 0.00016 | Regression loss: 0.00266 | Running loss: 0.28882\n",
            "Epoch: 8 | Iteration: 335 | Classification loss: 0.15987 | Regression loss: 0.38097 | Running loss: 0.28910\n",
            "Epoch: 8 | Iteration: 336 | Classification loss: 0.01698 | Regression loss: 0.14124 | Running loss: 0.28827\n",
            "Epoch: 8 | Iteration: 337 | Classification loss: 0.05736 | Regression loss: 0.16969 | Running loss: 0.28792\n",
            "Epoch: 8 | Iteration: 338 | Classification loss: 0.08840 | Regression loss: 0.28084 | Running loss: 0.28789\n",
            "Epoch: 8 | Iteration: 339 | Classification loss: 0.02833 | Regression loss: 0.13652 | Running loss: 0.28761\n",
            "Epoch: 8 | Iteration: 340 | Classification loss: 0.05162 | Regression loss: 0.16973 | Running loss: 0.28722\n",
            "Epoch: 8 | Iteration: 341 | Classification loss: 0.14476 | Regression loss: 0.39262 | Running loss: 0.28766\n",
            "Epoch: 8 | Iteration: 342 | Classification loss: 0.00081 | Regression loss: 0.00000 | Running loss: 0.28680\n",
            "Epoch: 8 | Iteration: 343 | Classification loss: 0.00013 | Regression loss: 0.01688 | Running loss: 0.28638\n",
            "Epoch: 8 | Iteration: 344 | Classification loss: 0.06652 | Regression loss: 0.19735 | Running loss: 0.28617\n",
            "Epoch: 8 | Iteration: 345 | Classification loss: 0.10592 | Regression loss: 0.29536 | Running loss: 0.28641\n",
            "Epoch: 8 | Iteration: 346 | Classification loss: 0.00007 | Regression loss: 0.06891 | Running loss: 0.28601\n",
            "Epoch: 8 | Iteration: 347 | Classification loss: 0.12568 | Regression loss: 0.35157 | Running loss: 0.28607\n",
            "Epoch: 8 | Iteration: 348 | Classification loss: 0.02048 | Regression loss: 0.15953 | Running loss: 0.28501\n",
            "Epoch: 8 | Iteration: 349 | Classification loss: 0.02233 | Regression loss: 0.07673 | Running loss: 0.28521\n",
            "Epoch: 8 | Iteration: 350 | Classification loss: 0.06023 | Regression loss: 0.24489 | Running loss: 0.28527\n",
            "Epoch: 8 | Iteration: 351 | Classification loss: 0.05040 | Regression loss: 0.15019 | Running loss: 0.28453\n",
            "Epoch: 8 | Iteration: 352 | Classification loss: 0.20986 | Regression loss: 0.38430 | Running loss: 0.28470\n",
            "Epoch: 8 | Iteration: 353 | Classification loss: 0.00039 | Regression loss: 0.00973 | Running loss: 0.28472\n",
            "Epoch: 8 | Iteration: 354 | Classification loss: 0.18889 | Regression loss: 0.40393 | Running loss: 0.28513\n",
            "Epoch: 8 | Iteration: 355 | Classification loss: 0.00038 | Regression loss: 0.01167 | Running loss: 0.28496\n",
            "Epoch: 8 | Iteration: 356 | Classification loss: 0.11239 | Regression loss: 0.35686 | Running loss: 0.28508\n",
            "Epoch: 8 | Iteration: 357 | Classification loss: 0.00016 | Regression loss: 0.00471 | Running loss: 0.28444\n",
            "Epoch: 8 | Iteration: 358 | Classification loss: 0.00066 | Regression loss: 0.00334 | Running loss: 0.28367\n",
            "Epoch: 8 | Iteration: 359 | Classification loss: 0.05210 | Regression loss: 0.24360 | Running loss: 0.28388\n",
            "Epoch: 8 | Iteration: 360 | Classification loss: 0.24688 | Regression loss: 0.36866 | Running loss: 0.28511\n",
            "Epoch: 8 | Iteration: 361 | Classification loss: 0.24733 | Regression loss: 0.34624 | Running loss: 0.28529\n",
            "Epoch: 8 | Iteration: 362 | Classification loss: 0.00006 | Regression loss: 0.03641 | Running loss: 0.28534\n",
            "Epoch: 8 | Iteration: 363 | Classification loss: 0.03880 | Regression loss: 0.24931 | Running loss: 0.28570\n",
            "Epoch: 8 | Iteration: 364 | Classification loss: 0.08877 | Regression loss: 0.25493 | Running loss: 0.28638\n",
            "Epoch: 8 | Iteration: 365 | Classification loss: 0.03872 | Regression loss: 0.13595 | Running loss: 0.28581\n",
            "Epoch: 8 | Iteration: 366 | Classification loss: 0.00008 | Regression loss: 0.01705 | Running loss: 0.28583\n",
            "Epoch: 8 | Iteration: 367 | Classification loss: 0.00020 | Regression loss: 0.01658 | Running loss: 0.28542\n",
            "Epoch: 8 | Iteration: 368 | Classification loss: 0.10862 | Regression loss: 0.35198 | Running loss: 0.28632\n",
            "Epoch: 8 | Iteration: 369 | Classification loss: 0.02166 | Regression loss: 0.10737 | Running loss: 0.28594\n",
            "Epoch: 8 | Iteration: 370 | Classification loss: 0.05518 | Regression loss: 0.24253 | Running loss: 0.28651\n",
            "Epoch: 8 | Iteration: 371 | Classification loss: 0.03968 | Regression loss: 0.23207 | Running loss: 0.28667\n",
            "Epoch: 8 | Iteration: 372 | Classification loss: 0.07979 | Regression loss: 0.24616 | Running loss: 0.28676\n",
            "Epoch: 8 | Iteration: 373 | Classification loss: 0.00008 | Regression loss: 0.04666 | Running loss: 0.28594\n",
            "Epoch: 8 | Iteration: 374 | Classification loss: 0.10955 | Regression loss: 0.12981 | Running loss: 0.28635\n",
            "Epoch: 8 | Iteration: 375 | Classification loss: 0.04234 | Regression loss: 0.11761 | Running loss: 0.28666\n",
            "Epoch: 8 | Iteration: 376 | Classification loss: 0.00008 | Regression loss: 0.02575 | Running loss: 0.28572\n",
            "Epoch: 8 | Iteration: 377 | Classification loss: 0.06199 | Regression loss: 0.14764 | Running loss: 0.28613\n",
            "Epoch: 8 | Iteration: 378 | Classification loss: 0.08825 | Regression loss: 0.38165 | Running loss: 0.28674\n",
            "Epoch: 8 | Iteration: 379 | Classification loss: 0.23342 | Regression loss: 0.53231 | Running loss: 0.28762\n",
            "Epoch: 8 | Iteration: 380 | Classification loss: 0.05917 | Regression loss: 0.35982 | Running loss: 0.28772\n",
            "Epoch: 8 | Iteration: 381 | Classification loss: 0.08336 | Regression loss: 0.37983 | Running loss: 0.28714\n",
            "Epoch: 8 | Iteration: 382 | Classification loss: 0.02715 | Regression loss: 0.13822 | Running loss: 0.28716\n",
            "Epoch: 8 | Iteration: 383 | Classification loss: 0.03000 | Regression loss: 0.18696 | Running loss: 0.28670\n",
            "Epoch: 8 | Iteration: 384 | Classification loss: 0.00015 | Regression loss: 0.02951 | Running loss: 0.28604\n",
            "Epoch: 8 | Iteration: 385 | Classification loss: 0.00121 | Regression loss: 0.01433 | Running loss: 0.28518\n",
            "Epoch: 8 | Iteration: 386 | Classification loss: 0.08856 | Regression loss: 0.50120 | Running loss: 0.28573\n",
            "Epoch: 8 | Iteration: 387 | Classification loss: 0.15798 | Regression loss: 0.44107 | Running loss: 0.28619\n",
            "Epoch: 8 | Iteration: 388 | Classification loss: 0.09230 | Regression loss: 0.31638 | Running loss: 0.28642\n",
            "Epoch: 8 | Iteration: 389 | Classification loss: 0.04009 | Regression loss: 0.20354 | Running loss: 0.28604\n",
            "Epoch: 8 | Iteration: 390 | Classification loss: 0.00017 | Regression loss: 0.17897 | Running loss: 0.28550\n",
            "Epoch: 8 | Iteration: 391 | Classification loss: 0.01664 | Regression loss: 0.26117 | Running loss: 0.28392\n",
            "Epoch: 8 | Iteration: 392 | Classification loss: 0.08504 | Regression loss: 0.28840 | Running loss: 0.28415\n",
            "Epoch: 8 | Iteration: 393 | Classification loss: 0.10559 | Regression loss: 0.29260 | Running loss: 0.28365\n",
            "Epoch: 8 | Iteration: 394 | Classification loss: 0.08441 | Regression loss: 0.17937 | Running loss: 0.28377\n",
            "Epoch: 8 | Iteration: 395 | Classification loss: 0.13391 | Regression loss: 0.36697 | Running loss: 0.28361\n",
            "Epoch: 8 | Iteration: 396 | Classification loss: 0.03554 | Regression loss: 0.21576 | Running loss: 0.28374\n",
            "Epoch: 8 | Iteration: 397 | Classification loss: 0.10242 | Regression loss: 0.29729 | Running loss: 0.28344\n",
            "Epoch: 8 | Iteration: 398 | Classification loss: 0.10805 | Regression loss: 0.33620 | Running loss: 0.28411\n",
            "Epoch: 8 | Iteration: 399 | Classification loss: 0.10290 | Regression loss: 0.34218 | Running loss: 0.28451\n",
            "Epoch: 8 | Iteration: 400 | Classification loss: 0.01926 | Regression loss: 0.11971 | Running loss: 0.28414\n",
            "Epoch: 8 | Iteration: 401 | Classification loss: 0.06275 | Regression loss: 0.27188 | Running loss: 0.28346\n",
            "Epoch: 8 | Iteration: 402 | Classification loss: 0.24627 | Regression loss: 0.45191 | Running loss: 0.28376\n",
            "Epoch: 8 | Iteration: 403 | Classification loss: 0.03315 | Regression loss: 0.19454 | Running loss: 0.28331\n",
            "Epoch: 8 | Iteration: 404 | Classification loss: 0.15724 | Regression loss: 0.30400 | Running loss: 0.28423\n",
            "Epoch: 8 | Iteration: 405 | Classification loss: 0.00017 | Regression loss: 0.03197 | Running loss: 0.28363\n",
            "Epoch: 8 | Iteration: 406 | Classification loss: 0.08615 | Regression loss: 0.30964 | Running loss: 0.28413\n",
            "Epoch: 8 | Iteration: 407 | Classification loss: 0.17155 | Regression loss: 0.40990 | Running loss: 0.28523\n",
            "Epoch: 8 | Iteration: 408 | Classification loss: 0.05554 | Regression loss: 0.17729 | Running loss: 0.28422\n",
            "Epoch: 8 | Iteration: 409 | Classification loss: 0.06038 | Regression loss: 0.24821 | Running loss: 0.28368\n",
            "Epoch: 8 | Iteration: 410 | Classification loss: 0.20661 | Regression loss: 0.42367 | Running loss: 0.28411\n",
            "Epoch: 8 | Iteration: 411 | Classification loss: 0.04220 | Regression loss: 0.21000 | Running loss: 0.28374\n",
            "Epoch: 8 | Iteration: 412 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.28315\n",
            "Epoch: 8 | Iteration: 413 | Classification loss: 0.00052 | Regression loss: 0.03434 | Running loss: 0.28276\n",
            "Epoch: 8 | Iteration: 414 | Classification loss: 0.06019 | Regression loss: 0.27331 | Running loss: 0.28339\n",
            "Epoch: 8 | Iteration: 415 | Classification loss: 0.03513 | Regression loss: 0.19290 | Running loss: 0.28234\n",
            "Epoch: 8 | Iteration: 416 | Classification loss: 0.05505 | Regression loss: 0.31219 | Running loss: 0.28268\n",
            "Epoch: 8 | Iteration: 417 | Classification loss: 0.08548 | Regression loss: 0.33597 | Running loss: 0.28308\n",
            "Epoch: 8 | Iteration: 418 | Classification loss: 0.18041 | Regression loss: 0.42188 | Running loss: 0.28389\n",
            "Epoch: 8 | Iteration: 419 | Classification loss: 0.08734 | Regression loss: 0.26057 | Running loss: 0.28436\n",
            "Epoch: 8 | Iteration: 420 | Classification loss: 0.03893 | Regression loss: 0.13303 | Running loss: 0.28393\n",
            "Epoch: 8 | Iteration: 421 | Classification loss: 0.25688 | Regression loss: 0.40233 | Running loss: 0.28443\n",
            "Epoch: 8 | Iteration: 422 | Classification loss: 0.11385 | Regression loss: 0.36532 | Running loss: 0.28437\n",
            "Epoch: 8 | Iteration: 423 | Classification loss: 0.00010 | Regression loss: 0.03739 | Running loss: 0.28413\n",
            "Epoch: 8 | Iteration: 424 | Classification loss: 0.05424 | Regression loss: 0.21342 | Running loss: 0.28374\n",
            "Epoch: 8 | Iteration: 425 | Classification loss: 0.03518 | Regression loss: 0.09335 | Running loss: 0.28342\n",
            "Epoch: 8 | Iteration: 426 | Classification loss: 0.06469 | Regression loss: 0.25687 | Running loss: 0.28365\n",
            "Epoch: 8 | Iteration: 427 | Classification loss: 0.03169 | Regression loss: 0.12777 | Running loss: 0.28330\n",
            "Epoch: 8 | Iteration: 428 | Classification loss: 0.07699 | Regression loss: 0.16035 | Running loss: 0.28271\n",
            "Epoch: 8 | Iteration: 429 | Classification loss: 0.16038 | Regression loss: 0.34209 | Running loss: 0.28299\n",
            "Epoch: 8 | Iteration: 430 | Classification loss: 0.00010 | Regression loss: 0.01054 | Running loss: 0.28300\n",
            "Epoch: 8 | Iteration: 431 | Classification loss: 0.00005 | Regression loss: 0.00495 | Running loss: 0.28226\n",
            "Epoch: 8 | Iteration: 432 | Classification loss: 0.08226 | Regression loss: 0.23181 | Running loss: 0.28261\n",
            "Epoch: 8 | Iteration: 433 | Classification loss: 0.05975 | Regression loss: 0.32357 | Running loss: 0.28258\n",
            "Epoch: 8 | Iteration: 434 | Classification loss: 0.03469 | Regression loss: 0.17836 | Running loss: 0.28195\n",
            "Epoch: 8 | Iteration: 435 | Classification loss: 0.05898 | Regression loss: 0.19332 | Running loss: 0.28195\n",
            "Epoch: 8 | Iteration: 436 | Classification loss: 0.02940 | Regression loss: 0.15525 | Running loss: 0.28126\n",
            "Epoch: 8 | Iteration: 437 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.28048\n",
            "Epoch: 8 | Iteration: 438 | Classification loss: 0.02913 | Regression loss: 0.20070 | Running loss: 0.28068\n",
            "Epoch: 8 | Iteration: 439 | Classification loss: 0.00017 | Regression loss: 0.07418 | Running loss: 0.28010\n",
            "Epoch: 8 | Iteration: 440 | Classification loss: 0.15321 | Regression loss: 0.45855 | Running loss: 0.28058\n",
            "Epoch: 8 | Iteration: 441 | Classification loss: 0.08955 | Regression loss: 0.17951 | Running loss: 0.28036\n",
            "Epoch: 8 | Iteration: 442 | Classification loss: 0.11592 | Regression loss: 0.18206 | Running loss: 0.28070\n",
            "Epoch: 8 | Iteration: 443 | Classification loss: 0.14599 | Regression loss: 0.32910 | Running loss: 0.28100\n",
            "Epoch: 8 | Iteration: 444 | Classification loss: 0.02771 | Regression loss: 0.11272 | Running loss: 0.27962\n",
            "Epoch: 8 | Iteration: 445 | Classification loss: 0.16488 | Regression loss: 0.40717 | Running loss: 0.28076\n",
            "Epoch: 8 | Iteration: 446 | Classification loss: 0.06313 | Regression loss: 0.23089 | Running loss: 0.28134\n",
            "Epoch: 8 | Iteration: 447 | Classification loss: 0.08801 | Regression loss: 0.28942 | Running loss: 0.28109\n",
            "Epoch: 8 | Iteration: 448 | Classification loss: 0.02432 | Regression loss: 0.18374 | Running loss: 0.28151\n",
            "Epoch: 8 | Iteration: 449 | Classification loss: 0.05340 | Regression loss: 0.24820 | Running loss: 0.28146\n",
            "Epoch: 8 | Iteration: 450 | Classification loss: 0.14115 | Regression loss: 0.37302 | Running loss: 0.28247\n",
            "Epoch: 8 | Iteration: 451 | Classification loss: 0.08382 | Regression loss: 0.31737 | Running loss: 0.28326\n",
            "Epoch: 8 | Iteration: 452 | Classification loss: 0.00010 | Regression loss: 0.03938 | Running loss: 0.28239\n",
            "Epoch: 8 | Iteration: 453 | Classification loss: 0.03321 | Regression loss: 0.17580 | Running loss: 0.28179\n",
            "Epoch: 8 | Iteration: 454 | Classification loss: 0.03285 | Regression loss: 0.15237 | Running loss: 0.28170\n",
            "Epoch: 8 | Iteration: 455 | Classification loss: 0.07818 | Regression loss: 0.25984 | Running loss: 0.28160\n",
            "Epoch: 8 | Iteration: 456 | Classification loss: 0.06343 | Regression loss: 0.33308 | Running loss: 0.28113\n",
            "Epoch: 8 | Iteration: 457 | Classification loss: 0.08057 | Regression loss: 0.28019 | Running loss: 0.28084\n",
            "Epoch: 8 | Iteration: 458 | Classification loss: 0.12973 | Regression loss: 0.40054 | Running loss: 0.28181\n",
            "Epoch: 8 | Iteration: 459 | Classification loss: 0.00008 | Regression loss: 0.08147 | Running loss: 0.28190\n",
            "Epoch: 8 | Iteration: 460 | Classification loss: 0.13362 | Regression loss: 0.36749 | Running loss: 0.28227\n",
            "Epoch: 8 | Iteration: 461 | Classification loss: 0.10311 | Regression loss: 0.34582 | Running loss: 0.28222\n",
            "Epoch: 8 | Iteration: 462 | Classification loss: 0.01936 | Regression loss: 0.14618 | Running loss: 0.28194\n",
            "Epoch: 8 | Iteration: 463 | Classification loss: 0.04830 | Regression loss: 0.15549 | Running loss: 0.28176\n",
            "Epoch: 8 | Iteration: 464 | Classification loss: 0.05775 | Regression loss: 0.27275 | Running loss: 0.28203\n",
            "Epoch: 8 | Iteration: 465 | Classification loss: 0.09197 | Regression loss: 0.31482 | Running loss: 0.28263\n",
            "Epoch: 8 | Iteration: 466 | Classification loss: 0.05132 | Regression loss: 0.19315 | Running loss: 0.28300\n",
            "Epoch: 8 | Iteration: 467 | Classification loss: 0.01998 | Regression loss: 0.14735 | Running loss: 0.28226\n",
            "Epoch: 8 | Iteration: 468 | Classification loss: 0.02019 | Regression loss: 0.09262 | Running loss: 0.28180\n",
            "Epoch: 8 | Iteration: 469 | Classification loss: 0.05605 | Regression loss: 0.18183 | Running loss: 0.28195\n",
            "Epoch: 8 | Iteration: 470 | Classification loss: 0.04442 | Regression loss: 0.24006 | Running loss: 0.28203\n",
            "Epoch: 8 | Iteration: 471 | Classification loss: 0.12690 | Regression loss: 0.50715 | Running loss: 0.28213\n",
            "Epoch: 8 | Iteration: 472 | Classification loss: 0.02689 | Regression loss: 0.17730 | Running loss: 0.28164\n",
            "Epoch: 8 | Iteration: 473 | Classification loss: 0.03640 | Regression loss: 0.12649 | Running loss: 0.28165\n",
            "Epoch: 8 | Iteration: 474 | Classification loss: 0.12155 | Regression loss: 0.17373 | Running loss: 0.28160\n",
            "Epoch: 8 | Iteration: 475 | Classification loss: 0.05695 | Regression loss: 0.25029 | Running loss: 0.28161\n",
            "Epoch: 8 | Iteration: 476 | Classification loss: 0.11028 | Regression loss: 0.36741 | Running loss: 0.28146\n",
            "Epoch: 8 | Iteration: 477 | Classification loss: 0.03450 | Regression loss: 0.10652 | Running loss: 0.28086\n",
            "Epoch: 8 | Iteration: 478 | Classification loss: 0.11020 | Regression loss: 0.38032 | Running loss: 0.28108\n",
            "Epoch: 8 | Iteration: 479 | Classification loss: 0.00037 | Regression loss: 0.00622 | Running loss: 0.28034\n",
            "Epoch: 8 | Iteration: 480 | Classification loss: 0.13842 | Regression loss: 0.32036 | Running loss: 0.28078\n",
            "Epoch: 8 | Iteration: 481 | Classification loss: 0.11526 | Regression loss: 0.38377 | Running loss: 0.28151\n",
            "Epoch: 8 | Iteration: 482 | Classification loss: 0.05352 | Regression loss: 0.19167 | Running loss: 0.28173\n",
            "Epoch: 8 | Iteration: 483 | Classification loss: 0.06383 | Regression loss: 0.10892 | Running loss: 0.28176\n",
            "Epoch: 8 | Iteration: 484 | Classification loss: 0.11091 | Regression loss: 0.30744 | Running loss: 0.28122\n",
            "Epoch: 8 | Iteration: 485 | Classification loss: 0.00027 | Regression loss: 0.02670 | Running loss: 0.28040\n",
            "Epoch: 8 | Iteration: 486 | Classification loss: 0.03739 | Regression loss: 0.13353 | Running loss: 0.27978\n",
            "Epoch: 8 | Iteration: 487 | Classification loss: 0.00367 | Regression loss: 0.01200 | Running loss: 0.27893\n",
            "Epoch: 8 | Iteration: 488 | Classification loss: 0.18112 | Regression loss: 0.37409 | Running loss: 0.27924\n",
            "Epoch: 8 | Iteration: 489 | Classification loss: 0.07422 | Regression loss: 0.25219 | Running loss: 0.27932\n",
            "Epoch: 8 | Iteration: 490 | Classification loss: 0.00019 | Regression loss: 0.00495 | Running loss: 0.27860\n",
            "Epoch: 8 | Iteration: 491 | Classification loss: 0.10250 | Regression loss: 0.35695 | Running loss: 0.27888\n",
            "Epoch: 8 | Iteration: 492 | Classification loss: 0.02488 | Regression loss: 0.11401 | Running loss: 0.27916\n",
            "Epoch: 8 | Iteration: 493 | Classification loss: 0.04041 | Regression loss: 0.23669 | Running loss: 0.27912\n",
            "Epoch: 8 | Iteration: 494 | Classification loss: 0.10909 | Regression loss: 0.24696 | Running loss: 0.27884\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.35s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.588\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.495\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.617\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.626\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.605\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.525\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 9 | Iteration: 0 | Classification loss: 0.05589 | Regression loss: 0.18211 | Running loss: 0.27786\n",
            "Epoch: 9 | Iteration: 1 | Classification loss: 0.11639 | Regression loss: 0.39991 | Running loss: 0.27828\n",
            "Epoch: 9 | Iteration: 2 | Classification loss: 0.11314 | Regression loss: 0.22434 | Running loss: 0.27896\n",
            "Epoch: 9 | Iteration: 3 | Classification loss: 0.06141 | Regression loss: 0.38230 | Running loss: 0.27899\n",
            "Epoch: 9 | Iteration: 4 | Classification loss: 0.06860 | Regression loss: 0.29442 | Running loss: 0.27970\n",
            "Epoch: 9 | Iteration: 5 | Classification loss: 0.03177 | Regression loss: 0.13753 | Running loss: 0.27956\n",
            "Epoch: 9 | Iteration: 6 | Classification loss: 0.05443 | Regression loss: 0.28816 | Running loss: 0.28012\n",
            "Epoch: 9 | Iteration: 7 | Classification loss: 0.17472 | Regression loss: 0.44054 | Running loss: 0.28070\n",
            "Epoch: 9 | Iteration: 8 | Classification loss: 0.04158 | Regression loss: 0.25458 | Running loss: 0.28128\n",
            "Epoch: 9 | Iteration: 9 | Classification loss: 0.14954 | Regression loss: 0.56425 | Running loss: 0.28204\n",
            "Epoch: 9 | Iteration: 10 | Classification loss: 0.11905 | Regression loss: 0.32112 | Running loss: 0.28180\n",
            "Epoch: 9 | Iteration: 11 | Classification loss: 0.04226 | Regression loss: 0.22095 | Running loss: 0.28161\n",
            "Epoch: 9 | Iteration: 12 | Classification loss: 0.16146 | Regression loss: 0.44100 | Running loss: 0.28193\n",
            "Epoch: 9 | Iteration: 13 | Classification loss: 0.03858 | Regression loss: 0.27120 | Running loss: 0.28228\n",
            "Epoch: 9 | Iteration: 14 | Classification loss: 0.02914 | Regression loss: 0.19398 | Running loss: 0.28174\n",
            "Epoch: 9 | Iteration: 15 | Classification loss: 0.05319 | Regression loss: 0.36116 | Running loss: 0.28216\n",
            "Epoch: 9 | Iteration: 16 | Classification loss: 0.00031 | Regression loss: 0.00963 | Running loss: 0.28171\n",
            "Epoch: 9 | Iteration: 17 | Classification loss: 0.09596 | Regression loss: 0.30960 | Running loss: 0.28250\n",
            "Epoch: 9 | Iteration: 18 | Classification loss: 0.14194 | Regression loss: 0.64031 | Running loss: 0.28404\n",
            "Epoch: 9 | Iteration: 19 | Classification loss: 0.02585 | Regression loss: 0.19801 | Running loss: 0.28413\n",
            "Epoch: 9 | Iteration: 20 | Classification loss: 0.00007 | Regression loss: 0.01014 | Running loss: 0.28379\n",
            "Epoch: 9 | Iteration: 21 | Classification loss: 0.05194 | Regression loss: 0.21705 | Running loss: 0.28371\n",
            "Epoch: 9 | Iteration: 22 | Classification loss: 0.08068 | Regression loss: 0.28368 | Running loss: 0.28401\n",
            "Epoch: 9 | Iteration: 23 | Classification loss: 0.02039 | Regression loss: 0.10799 | Running loss: 0.28336\n",
            "Epoch: 9 | Iteration: 24 | Classification loss: 0.07815 | Regression loss: 0.34369 | Running loss: 0.28345\n",
            "Epoch: 9 | Iteration: 25 | Classification loss: 0.06399 | Regression loss: 0.31127 | Running loss: 0.28342\n",
            "Epoch: 9 | Iteration: 26 | Classification loss: 0.06025 | Regression loss: 0.12428 | Running loss: 0.28290\n",
            "Epoch: 9 | Iteration: 27 | Classification loss: 0.08462 | Regression loss: 0.33788 | Running loss: 0.28287\n",
            "Epoch: 9 | Iteration: 28 | Classification loss: 0.04743 | Regression loss: 0.15610 | Running loss: 0.28278\n",
            "Epoch: 9 | Iteration: 29 | Classification loss: 0.05748 | Regression loss: 0.33995 | Running loss: 0.28272\n",
            "Epoch: 9 | Iteration: 30 | Classification loss: 0.13524 | Regression loss: 0.50976 | Running loss: 0.28344\n",
            "Epoch: 9 | Iteration: 31 | Classification loss: 0.00007 | Regression loss: 0.00951 | Running loss: 0.28346\n",
            "Epoch: 9 | Iteration: 32 | Classification loss: 0.11866 | Regression loss: 0.28273 | Running loss: 0.28364\n",
            "Epoch: 9 | Iteration: 33 | Classification loss: 0.04145 | Regression loss: 0.20597 | Running loss: 0.28340\n",
            "Epoch: 9 | Iteration: 34 | Classification loss: 0.05893 | Regression loss: 0.29100 | Running loss: 0.28311\n",
            "Epoch: 9 | Iteration: 35 | Classification loss: 0.04800 | Regression loss: 0.24443 | Running loss: 0.28289\n",
            "Epoch: 9 | Iteration: 36 | Classification loss: 0.00011 | Regression loss: 0.01520 | Running loss: 0.28212\n",
            "Epoch: 9 | Iteration: 37 | Classification loss: 0.04265 | Regression loss: 0.18040 | Running loss: 0.28175\n",
            "Epoch: 9 | Iteration: 38 | Classification loss: 0.09670 | Regression loss: 0.31974 | Running loss: 0.28196\n",
            "Epoch: 9 | Iteration: 39 | Classification loss: 0.16759 | Regression loss: 0.53387 | Running loss: 0.28269\n",
            "Epoch: 9 | Iteration: 40 | Classification loss: 0.06447 | Regression loss: 0.26409 | Running loss: 0.28259\n",
            "Epoch: 9 | Iteration: 41 | Classification loss: 0.01363 | Regression loss: 0.09633 | Running loss: 0.28227\n",
            "Epoch: 9 | Iteration: 42 | Classification loss: 0.05719 | Regression loss: 0.27399 | Running loss: 0.28227\n",
            "Epoch: 9 | Iteration: 43 | Classification loss: 0.07531 | Regression loss: 0.23917 | Running loss: 0.28290\n",
            "Epoch: 9 | Iteration: 44 | Classification loss: 0.02656 | Regression loss: 0.16586 | Running loss: 0.28317\n",
            "Epoch: 9 | Iteration: 45 | Classification loss: 0.02544 | Regression loss: 0.09330 | Running loss: 0.28283\n",
            "Epoch: 9 | Iteration: 46 | Classification loss: 0.05702 | Regression loss: 0.13360 | Running loss: 0.28314\n",
            "Epoch: 9 | Iteration: 47 | Classification loss: 0.06770 | Regression loss: 0.26597 | Running loss: 0.28350\n",
            "Epoch: 9 | Iteration: 48 | Classification loss: 0.08643 | Regression loss: 0.28636 | Running loss: 0.28358\n",
            "Epoch: 9 | Iteration: 49 | Classification loss: 0.11438 | Regression loss: 0.32906 | Running loss: 0.28418\n",
            "Epoch: 9 | Iteration: 50 | Classification loss: 0.06726 | Regression loss: 0.38008 | Running loss: 0.28425\n",
            "Epoch: 9 | Iteration: 51 | Classification loss: 0.05467 | Regression loss: 0.32712 | Running loss: 0.28501\n",
            "Epoch: 9 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.28385\n",
            "Epoch: 9 | Iteration: 53 | Classification loss: 0.04900 | Regression loss: 0.24134 | Running loss: 0.28399\n",
            "Epoch: 9 | Iteration: 54 | Classification loss: 0.04041 | Regression loss: 0.35648 | Running loss: 0.28426\n",
            "Epoch: 9 | Iteration: 55 | Classification loss: 0.02611 | Regression loss: 0.15076 | Running loss: 0.28381\n",
            "Epoch: 9 | Iteration: 56 | Classification loss: 0.02933 | Regression loss: 0.10746 | Running loss: 0.28315\n",
            "Epoch: 9 | Iteration: 57 | Classification loss: 0.00017 | Regression loss: 0.04694 | Running loss: 0.28243\n",
            "Epoch: 9 | Iteration: 58 | Classification loss: 0.13591 | Regression loss: 0.43022 | Running loss: 0.28349\n",
            "Epoch: 9 | Iteration: 59 | Classification loss: 0.01890 | Regression loss: 0.12290 | Running loss: 0.28322\n",
            "Epoch: 9 | Iteration: 60 | Classification loss: 0.03914 | Regression loss: 0.25504 | Running loss: 0.28336\n",
            "Epoch: 9 | Iteration: 61 | Classification loss: 0.06706 | Regression loss: 0.26904 | Running loss: 0.28339\n",
            "Epoch: 9 | Iteration: 62 | Classification loss: 0.04103 | Regression loss: 0.26153 | Running loss: 0.28372\n",
            "Epoch: 9 | Iteration: 63 | Classification loss: 0.06643 | Regression loss: 0.29547 | Running loss: 0.28442\n",
            "Epoch: 9 | Iteration: 64 | Classification loss: 0.04943 | Regression loss: 0.25232 | Running loss: 0.28353\n",
            "Epoch: 9 | Iteration: 65 | Classification loss: 0.04436 | Regression loss: 0.15592 | Running loss: 0.28391\n",
            "Epoch: 9 | Iteration: 66 | Classification loss: 0.06002 | Regression loss: 0.24921 | Running loss: 0.28395\n",
            "Epoch: 9 | Iteration: 67 | Classification loss: 0.06815 | Regression loss: 0.19120 | Running loss: 0.28415\n",
            "Epoch: 9 | Iteration: 68 | Classification loss: 0.00018 | Regression loss: 0.04960 | Running loss: 0.28380\n",
            "Epoch: 9 | Iteration: 69 | Classification loss: 0.13869 | Regression loss: 0.32695 | Running loss: 0.28355\n",
            "Epoch: 9 | Iteration: 70 | Classification loss: 0.10338 | Regression loss: 0.25089 | Running loss: 0.28355\n",
            "Epoch: 9 | Iteration: 71 | Classification loss: 0.10276 | Regression loss: 0.22411 | Running loss: 0.28419\n",
            "Epoch: 9 | Iteration: 72 | Classification loss: 0.16330 | Regression loss: 0.50696 | Running loss: 0.28463\n",
            "Epoch: 9 | Iteration: 73 | Classification loss: 0.05317 | Regression loss: 0.13942 | Running loss: 0.28464\n",
            "Epoch: 9 | Iteration: 74 | Classification loss: 0.02950 | Regression loss: 0.07890 | Running loss: 0.28349\n",
            "Epoch: 9 | Iteration: 75 | Classification loss: 0.01802 | Regression loss: 0.12541 | Running loss: 0.28316\n",
            "Epoch: 9 | Iteration: 76 | Classification loss: 0.04363 | Regression loss: 0.13121 | Running loss: 0.28235\n",
            "Epoch: 9 | Iteration: 77 | Classification loss: 0.05962 | Regression loss: 0.13048 | Running loss: 0.28188\n",
            "Epoch: 9 | Iteration: 78 | Classification loss: 0.03428 | Regression loss: 0.14719 | Running loss: 0.28221\n",
            "Epoch: 9 | Iteration: 79 | Classification loss: 0.07457 | Regression loss: 0.13368 | Running loss: 0.28232\n",
            "Epoch: 9 | Iteration: 80 | Classification loss: 0.06191 | Regression loss: 0.12983 | Running loss: 0.28220\n",
            "Epoch: 9 | Iteration: 81 | Classification loss: 0.10028 | Regression loss: 0.17668 | Running loss: 0.28217\n",
            "Epoch: 9 | Iteration: 82 | Classification loss: 0.00019 | Regression loss: 0.02595 | Running loss: 0.28125\n",
            "Epoch: 9 | Iteration: 83 | Classification loss: 0.02377 | Regression loss: 0.12803 | Running loss: 0.28054\n",
            "Epoch: 9 | Iteration: 84 | Classification loss: 0.04171 | Regression loss: 0.22567 | Running loss: 0.28103\n",
            "Epoch: 9 | Iteration: 85 | Classification loss: 0.05024 | Regression loss: 0.12206 | Running loss: 0.28138\n",
            "Epoch: 9 | Iteration: 86 | Classification loss: 0.06025 | Regression loss: 0.26095 | Running loss: 0.28136\n",
            "Epoch: 9 | Iteration: 87 | Classification loss: 0.05676 | Regression loss: 0.23301 | Running loss: 0.28192\n",
            "Epoch: 9 | Iteration: 88 | Classification loss: 0.07064 | Regression loss: 0.22667 | Running loss: 0.28128\n",
            "Epoch: 9 | Iteration: 89 | Classification loss: 0.07068 | Regression loss: 0.17922 | Running loss: 0.28065\n",
            "Epoch: 9 | Iteration: 90 | Classification loss: 0.00007 | Regression loss: 0.02650 | Running loss: 0.28002\n",
            "Epoch: 9 | Iteration: 91 | Classification loss: 0.03139 | Regression loss: 0.24116 | Running loss: 0.27984\n",
            "Epoch: 9 | Iteration: 92 | Classification loss: 0.00019 | Regression loss: 0.03228 | Running loss: 0.27936\n",
            "Epoch: 9 | Iteration: 93 | Classification loss: 0.02659 | Regression loss: 0.14080 | Running loss: 0.27929\n",
            "Epoch: 9 | Iteration: 94 | Classification loss: 0.13267 | Regression loss: 0.40666 | Running loss: 0.28029\n",
            "Epoch: 9 | Iteration: 95 | Classification loss: 0.07736 | Regression loss: 0.24155 | Running loss: 0.28061\n",
            "Epoch: 9 | Iteration: 96 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.27997\n",
            "Epoch: 9 | Iteration: 97 | Classification loss: 0.05279 | Regression loss: 0.19253 | Running loss: 0.27992\n",
            "Epoch: 9 | Iteration: 98 | Classification loss: 0.11364 | Regression loss: 0.37055 | Running loss: 0.28052\n",
            "Epoch: 9 | Iteration: 99 | Classification loss: 0.02063 | Regression loss: 0.14160 | Running loss: 0.28043\n",
            "Epoch: 9 | Iteration: 100 | Classification loss: 0.11391 | Regression loss: 0.29439 | Running loss: 0.28073\n",
            "Epoch: 9 | Iteration: 101 | Classification loss: 0.06888 | Regression loss: 0.26553 | Running loss: 0.28079\n",
            "Epoch: 9 | Iteration: 102 | Classification loss: 0.02018 | Regression loss: 0.17794 | Running loss: 0.28013\n",
            "Epoch: 9 | Iteration: 103 | Classification loss: 0.02251 | Regression loss: 0.22447 | Running loss: 0.28032\n",
            "Epoch: 9 | Iteration: 104 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.27977\n",
            "Epoch: 9 | Iteration: 105 | Classification loss: 0.02369 | Regression loss: 0.10170 | Running loss: 0.27950\n",
            "Epoch: 9 | Iteration: 106 | Classification loss: 0.02302 | Regression loss: 0.15108 | Running loss: 0.27942\n",
            "Epoch: 9 | Iteration: 107 | Classification loss: 0.07383 | Regression loss: 0.19316 | Running loss: 0.27943\n",
            "Epoch: 9 | Iteration: 108 | Classification loss: 0.02180 | Regression loss: 0.11683 | Running loss: 0.27940\n",
            "Epoch: 9 | Iteration: 109 | Classification loss: 0.00042 | Regression loss: 0.04660 | Running loss: 0.27937\n",
            "Epoch: 9 | Iteration: 110 | Classification loss: 0.01339 | Regression loss: 0.10877 | Running loss: 0.27830\n",
            "Epoch: 9 | Iteration: 111 | Classification loss: 0.00013 | Regression loss: 0.00942 | Running loss: 0.27703\n",
            "Epoch: 9 | Iteration: 112 | Classification loss: 0.03101 | Regression loss: 0.19006 | Running loss: 0.27707\n",
            "Epoch: 9 | Iteration: 113 | Classification loss: 0.09110 | Regression loss: 0.21623 | Running loss: 0.27769\n",
            "Epoch: 9 | Iteration: 114 | Classification loss: 0.00007 | Regression loss: 0.10198 | Running loss: 0.27773\n",
            "Epoch: 9 | Iteration: 115 | Classification loss: 0.04278 | Regression loss: 0.16676 | Running loss: 0.27802\n",
            "Epoch: 9 | Iteration: 116 | Classification loss: 0.00080 | Regression loss: 0.06464 | Running loss: 0.27694\n",
            "Epoch: 9 | Iteration: 117 | Classification loss: 0.03292 | Regression loss: 0.22400 | Running loss: 0.27665\n",
            "Epoch: 9 | Iteration: 118 | Classification loss: 0.00013 | Regression loss: 0.07516 | Running loss: 0.27648\n",
            "Epoch: 9 | Iteration: 119 | Classification loss: 0.06638 | Regression loss: 0.18453 | Running loss: 0.27638\n",
            "Epoch: 9 | Iteration: 120 | Classification loss: 0.00010 | Regression loss: 0.05393 | Running loss: 0.27598\n",
            "Epoch: 9 | Iteration: 121 | Classification loss: 0.00025 | Regression loss: 0.03592 | Running loss: 0.27605\n",
            "Epoch: 9 | Iteration: 122 | Classification loss: 0.00023 | Regression loss: 0.06035 | Running loss: 0.27543\n",
            "Epoch: 9 | Iteration: 123 | Classification loss: 0.12718 | Regression loss: 0.34701 | Running loss: 0.27517\n",
            "Epoch: 9 | Iteration: 124 | Classification loss: 0.02886 | Regression loss: 0.20880 | Running loss: 0.27533\n",
            "Epoch: 9 | Iteration: 125 | Classification loss: 0.02167 | Regression loss: 0.13761 | Running loss: 0.27469\n",
            "Epoch: 9 | Iteration: 126 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.27343\n",
            "Epoch: 9 | Iteration: 127 | Classification loss: 0.00005 | Regression loss: 0.02999 | Running loss: 0.27246\n",
            "Epoch: 9 | Iteration: 128 | Classification loss: 0.04957 | Regression loss: 0.24485 | Running loss: 0.27291\n",
            "Epoch: 9 | Iteration: 129 | Classification loss: 0.09932 | Regression loss: 0.29439 | Running loss: 0.27370\n",
            "Epoch: 9 | Iteration: 130 | Classification loss: 0.11330 | Regression loss: 0.33213 | Running loss: 0.27450\n",
            "Epoch: 9 | Iteration: 131 | Classification loss: 0.09033 | Regression loss: 0.28702 | Running loss: 0.27498\n",
            "Epoch: 9 | Iteration: 132 | Classification loss: 0.10559 | Regression loss: 0.37031 | Running loss: 0.27517\n",
            "Epoch: 9 | Iteration: 133 | Classification loss: 0.02811 | Regression loss: 0.18902 | Running loss: 0.27461\n",
            "Epoch: 9 | Iteration: 134 | Classification loss: 0.01762 | Regression loss: 0.21547 | Running loss: 0.27443\n",
            "Epoch: 9 | Iteration: 135 | Classification loss: 0.08577 | Regression loss: 0.30437 | Running loss: 0.27422\n",
            "Epoch: 9 | Iteration: 136 | Classification loss: 0.01720 | Regression loss: 0.08453 | Running loss: 0.27370\n",
            "Epoch: 9 | Iteration: 137 | Classification loss: 0.02266 | Regression loss: 0.09826 | Running loss: 0.27367\n",
            "Epoch: 9 | Iteration: 138 | Classification loss: 0.02236 | Regression loss: 0.12364 | Running loss: 0.27368\n",
            "Epoch: 9 | Iteration: 139 | Classification loss: 0.06665 | Regression loss: 0.24116 | Running loss: 0.27300\n",
            "Epoch: 9 | Iteration: 140 | Classification loss: 0.03794 | Regression loss: 0.18487 | Running loss: 0.27260\n",
            "Epoch: 9 | Iteration: 141 | Classification loss: 0.07211 | Regression loss: 0.27073 | Running loss: 0.27249\n",
            "Epoch: 9 | Iteration: 142 | Classification loss: 0.14281 | Regression loss: 0.25286 | Running loss: 0.27291\n",
            "Epoch: 9 | Iteration: 143 | Classification loss: 0.13186 | Regression loss: 0.43616 | Running loss: 0.27283\n",
            "Epoch: 9 | Iteration: 144 | Classification loss: 0.03441 | Regression loss: 0.17939 | Running loss: 0.27260\n",
            "Epoch: 9 | Iteration: 145 | Classification loss: 0.06068 | Regression loss: 0.24450 | Running loss: 0.27266\n",
            "Epoch: 9 | Iteration: 146 | Classification loss: 0.05895 | Regression loss: 0.36493 | Running loss: 0.27318\n",
            "Epoch: 9 | Iteration: 147 | Classification loss: 0.00003 | Regression loss: 0.00987 | Running loss: 0.27167\n",
            "Epoch: 9 | Iteration: 148 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.27117\n",
            "Epoch: 9 | Iteration: 149 | Classification loss: 0.02135 | Regression loss: 0.18674 | Running loss: 0.27010\n",
            "Epoch: 9 | Iteration: 150 | Classification loss: 0.02935 | Regression loss: 0.21109 | Running loss: 0.27030\n",
            "Epoch: 9 | Iteration: 151 | Classification loss: 0.01907 | Regression loss: 0.09434 | Running loss: 0.27026\n",
            "Epoch: 9 | Iteration: 152 | Classification loss: 0.04134 | Regression loss: 0.24696 | Running loss: 0.27080\n",
            "Epoch: 9 | Iteration: 153 | Classification loss: 0.02920 | Regression loss: 0.25330 | Running loss: 0.27071\n",
            "Epoch: 9 | Iteration: 154 | Classification loss: 0.19824 | Regression loss: 0.26224 | Running loss: 0.27112\n",
            "Epoch: 9 | Iteration: 155 | Classification loss: 0.15344 | Regression loss: 0.31302 | Running loss: 0.27140\n",
            "Epoch: 9 | Iteration: 156 | Classification loss: 0.05334 | Regression loss: 0.18375 | Running loss: 0.27100\n",
            "Epoch: 9 | Iteration: 157 | Classification loss: 0.00003 | Regression loss: 0.01099 | Running loss: 0.27023\n",
            "Epoch: 9 | Iteration: 158 | Classification loss: 0.00009 | Regression loss: 0.01297 | Running loss: 0.26974\n",
            "Epoch: 9 | Iteration: 159 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26970\n",
            "Epoch: 9 | Iteration: 160 | Classification loss: 0.03797 | Regression loss: 0.14124 | Running loss: 0.26945\n",
            "Epoch: 9 | Iteration: 161 | Classification loss: 0.00005 | Regression loss: 0.00707 | Running loss: 0.26947\n",
            "Epoch: 9 | Iteration: 162 | Classification loss: 0.00007 | Regression loss: 0.00439 | Running loss: 0.26807\n",
            "Epoch: 9 | Iteration: 163 | Classification loss: 0.12996 | Regression loss: 0.19443 | Running loss: 0.26778\n",
            "Epoch: 9 | Iteration: 164 | Classification loss: 0.09863 | Regression loss: 0.31262 | Running loss: 0.26773\n",
            "Epoch: 9 | Iteration: 165 | Classification loss: 0.12062 | Regression loss: 0.34355 | Running loss: 0.26806\n",
            "Epoch: 9 | Iteration: 166 | Classification loss: 0.10485 | Regression loss: 0.38761 | Running loss: 0.26900\n",
            "Epoch: 9 | Iteration: 167 | Classification loss: 0.13156 | Regression loss: 0.39021 | Running loss: 0.26962\n",
            "Epoch: 9 | Iteration: 168 | Classification loss: 0.08624 | Regression loss: 0.23178 | Running loss: 0.27020\n",
            "Epoch: 9 | Iteration: 169 | Classification loss: 0.05613 | Regression loss: 0.30402 | Running loss: 0.27026\n",
            "Epoch: 9 | Iteration: 170 | Classification loss: 0.01678 | Regression loss: 0.14289 | Running loss: 0.26942\n",
            "Epoch: 9 | Iteration: 171 | Classification loss: 0.00012 | Regression loss: 0.10319 | Running loss: 0.26908\n",
            "Epoch: 9 | Iteration: 172 | Classification loss: 0.01212 | Regression loss: 0.17734 | Running loss: 0.26943\n",
            "Epoch: 9 | Iteration: 173 | Classification loss: 0.01504 | Regression loss: 0.10294 | Running loss: 0.26967\n",
            "Epoch: 9 | Iteration: 174 | Classification loss: 0.02727 | Regression loss: 0.13075 | Running loss: 0.26996\n",
            "Epoch: 9 | Iteration: 175 | Classification loss: 0.07235 | Regression loss: 0.25193 | Running loss: 0.26898\n",
            "Epoch: 9 | Iteration: 176 | Classification loss: 0.10530 | Regression loss: 0.32370 | Running loss: 0.26896\n",
            "Epoch: 9 | Iteration: 177 | Classification loss: 0.02700 | Regression loss: 0.20498 | Running loss: 0.26940\n",
            "Epoch: 9 | Iteration: 178 | Classification loss: 0.02312 | Regression loss: 0.13155 | Running loss: 0.26911\n",
            "Epoch: 9 | Iteration: 179 | Classification loss: 0.08838 | Regression loss: 0.37930 | Running loss: 0.26954\n",
            "Epoch: 9 | Iteration: 180 | Classification loss: 0.03439 | Regression loss: 0.17546 | Running loss: 0.26923\n",
            "Epoch: 9 | Iteration: 181 | Classification loss: 0.04858 | Regression loss: 0.23512 | Running loss: 0.26912\n",
            "Epoch: 9 | Iteration: 182 | Classification loss: 0.05639 | Regression loss: 0.27002 | Running loss: 0.26932\n",
            "Epoch: 9 | Iteration: 183 | Classification loss: 0.06601 | Regression loss: 0.18967 | Running loss: 0.26916\n",
            "Epoch: 9 | Iteration: 184 | Classification loss: 0.04630 | Regression loss: 0.08957 | Running loss: 0.26877\n",
            "Epoch: 9 | Iteration: 185 | Classification loss: 0.07325 | Regression loss: 0.20564 | Running loss: 0.26933\n",
            "Epoch: 9 | Iteration: 186 | Classification loss: 0.02775 | Regression loss: 0.18429 | Running loss: 0.26949\n",
            "Epoch: 9 | Iteration: 187 | Classification loss: 0.12366 | Regression loss: 0.32344 | Running loss: 0.26962\n",
            "Epoch: 9 | Iteration: 188 | Classification loss: 0.02171 | Regression loss: 0.17166 | Running loss: 0.26948\n",
            "Epoch: 9 | Iteration: 189 | Classification loss: 0.05709 | Regression loss: 0.26568 | Running loss: 0.26931\n",
            "Epoch: 9 | Iteration: 190 | Classification loss: 0.11874 | Regression loss: 0.34281 | Running loss: 0.26996\n",
            "Epoch: 9 | Iteration: 191 | Classification loss: 0.12945 | Regression loss: 0.46044 | Running loss: 0.27022\n",
            "Epoch: 9 | Iteration: 192 | Classification loss: 0.01167 | Regression loss: 0.10652 | Running loss: 0.26971\n",
            "Epoch: 9 | Iteration: 193 | Classification loss: 0.01550 | Regression loss: 0.13650 | Running loss: 0.26900\n",
            "Epoch: 9 | Iteration: 194 | Classification loss: 0.00026 | Regression loss: 0.01510 | Running loss: 0.26835\n",
            "Epoch: 9 | Iteration: 195 | Classification loss: 0.10385 | Regression loss: 0.30096 | Running loss: 0.26876\n",
            "Epoch: 9 | Iteration: 196 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26853\n",
            "Epoch: 9 | Iteration: 197 | Classification loss: 0.00010 | Regression loss: 0.01842 | Running loss: 0.26851\n",
            "Epoch: 9 | Iteration: 198 | Classification loss: 0.02455 | Regression loss: 0.15008 | Running loss: 0.26845\n",
            "Epoch: 9 | Iteration: 199 | Classification loss: 0.07390 | Regression loss: 0.26319 | Running loss: 0.26868\n",
            "Epoch: 9 | Iteration: 200 | Classification loss: 0.02642 | Regression loss: 0.12103 | Running loss: 0.26894\n",
            "Epoch: 9 | Iteration: 201 | Classification loss: 0.13807 | Regression loss: 0.32479 | Running loss: 0.26983\n",
            "Epoch: 9 | Iteration: 202 | Classification loss: 0.15798 | Regression loss: 0.38432 | Running loss: 0.27030\n",
            "Epoch: 9 | Iteration: 203 | Classification loss: 0.00011 | Regression loss: 0.01727 | Running loss: 0.26938\n",
            "Epoch: 9 | Iteration: 204 | Classification loss: 0.03366 | Regression loss: 0.10031 | Running loss: 0.26849\n",
            "Epoch: 9 | Iteration: 205 | Classification loss: 0.02464 | Regression loss: 0.13429 | Running loss: 0.26804\n",
            "Epoch: 9 | Iteration: 206 | Classification loss: 0.11186 | Regression loss: 0.28693 | Running loss: 0.26847\n",
            "Epoch: 9 | Iteration: 207 | Classification loss: 0.00007 | Regression loss: 0.00922 | Running loss: 0.26775\n",
            "Epoch: 9 | Iteration: 208 | Classification loss: 0.15566 | Regression loss: 0.29620 | Running loss: 0.26842\n",
            "Epoch: 9 | Iteration: 209 | Classification loss: 0.00014 | Regression loss: 0.01269 | Running loss: 0.26758\n",
            "Epoch: 9 | Iteration: 210 | Classification loss: 0.00082 | Regression loss: 0.01505 | Running loss: 0.26690\n",
            "Epoch: 9 | Iteration: 211 | Classification loss: 0.04459 | Regression loss: 0.14734 | Running loss: 0.26645\n",
            "Epoch: 9 | Iteration: 212 | Classification loss: 0.06295 | Regression loss: 0.27895 | Running loss: 0.26688\n",
            "Epoch: 9 | Iteration: 213 | Classification loss: 0.00013 | Regression loss: 0.07106 | Running loss: 0.26694\n",
            "Epoch: 9 | Iteration: 214 | Classification loss: 0.09839 | Regression loss: 0.28571 | Running loss: 0.26760\n",
            "Epoch: 9 | Iteration: 215 | Classification loss: 0.04557 | Regression loss: 0.15180 | Running loss: 0.26755\n",
            "Epoch: 9 | Iteration: 216 | Classification loss: 0.17852 | Regression loss: 0.42828 | Running loss: 0.26837\n",
            "Epoch: 9 | Iteration: 217 | Classification loss: 0.08205 | Regression loss: 0.26913 | Running loss: 0.26904\n",
            "Epoch: 9 | Iteration: 218 | Classification loss: 0.11167 | Regression loss: 0.29628 | Running loss: 0.26907\n",
            "Epoch: 9 | Iteration: 219 | Classification loss: 0.00009 | Regression loss: 0.06597 | Running loss: 0.26861\n",
            "Epoch: 9 | Iteration: 220 | Classification loss: 0.00004 | Regression loss: 0.00741 | Running loss: 0.26779\n",
            "Epoch: 9 | Iteration: 221 | Classification loss: 0.04134 | Regression loss: 0.19501 | Running loss: 0.26796\n",
            "Epoch: 9 | Iteration: 222 | Classification loss: 0.00006 | Regression loss: 0.00808 | Running loss: 0.26735\n",
            "Epoch: 9 | Iteration: 223 | Classification loss: 0.09531 | Regression loss: 0.32888 | Running loss: 0.26746\n",
            "Epoch: 9 | Iteration: 224 | Classification loss: 0.01928 | Regression loss: 0.11996 | Running loss: 0.26723\n",
            "Epoch: 9 | Iteration: 225 | Classification loss: 0.04750 | Regression loss: 0.21676 | Running loss: 0.26732\n",
            "Epoch: 9 | Iteration: 226 | Classification loss: 0.05934 | Regression loss: 0.28594 | Running loss: 0.26761\n",
            "Epoch: 9 | Iteration: 227 | Classification loss: 0.10044 | Regression loss: 0.32730 | Running loss: 0.26786\n",
            "Epoch: 9 | Iteration: 228 | Classification loss: 0.08191 | Regression loss: 0.14501 | Running loss: 0.26832\n",
            "Epoch: 9 | Iteration: 229 | Classification loss: 0.12153 | Regression loss: 0.23439 | Running loss: 0.26870\n",
            "Epoch: 9 | Iteration: 230 | Classification loss: 0.00009 | Regression loss: 0.05345 | Running loss: 0.26757\n",
            "Epoch: 9 | Iteration: 231 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26748\n",
            "Epoch: 9 | Iteration: 232 | Classification loss: 0.00010 | Regression loss: 0.02361 | Running loss: 0.26715\n",
            "Epoch: 9 | Iteration: 233 | Classification loss: 0.03781 | Regression loss: 0.15023 | Running loss: 0.26731\n",
            "Epoch: 9 | Iteration: 234 | Classification loss: 0.06785 | Regression loss: 0.27762 | Running loss: 0.26727\n",
            "Epoch: 9 | Iteration: 235 | Classification loss: 0.00811 | Regression loss: 0.12188 | Running loss: 0.26578\n",
            "Epoch: 9 | Iteration: 236 | Classification loss: 0.04438 | Regression loss: 0.25798 | Running loss: 0.26583\n",
            "Epoch: 9 | Iteration: 237 | Classification loss: 0.03441 | Regression loss: 0.17662 | Running loss: 0.26593\n",
            "Epoch: 9 | Iteration: 238 | Classification loss: 0.08606 | Regression loss: 0.30187 | Running loss: 0.26550\n",
            "Epoch: 9 | Iteration: 239 | Classification loss: 0.05453 | Regression loss: 0.28349 | Running loss: 0.26577\n",
            "Epoch: 9 | Iteration: 240 | Classification loss: 0.07194 | Regression loss: 0.29402 | Running loss: 0.26650\n",
            "Epoch: 9 | Iteration: 241 | Classification loss: 0.12220 | Regression loss: 0.34235 | Running loss: 0.26672\n",
            "Epoch: 9 | Iteration: 242 | Classification loss: 0.23416 | Regression loss: 0.44090 | Running loss: 0.26768\n",
            "Epoch: 9 | Iteration: 243 | Classification loss: 0.01441 | Regression loss: 0.15002 | Running loss: 0.26672\n",
            "Epoch: 9 | Iteration: 244 | Classification loss: 0.00012 | Regression loss: 0.10926 | Running loss: 0.26656\n",
            "Epoch: 9 | Iteration: 245 | Classification loss: 0.09522 | Regression loss: 0.24209 | Running loss: 0.26664\n",
            "Epoch: 9 | Iteration: 246 | Classification loss: 0.06816 | Regression loss: 0.23778 | Running loss: 0.26655\n",
            "Epoch: 9 | Iteration: 247 | Classification loss: 0.05798 | Regression loss: 0.33190 | Running loss: 0.26714\n",
            "Epoch: 9 | Iteration: 248 | Classification loss: 0.06913 | Regression loss: 0.26054 | Running loss: 0.26701\n",
            "Epoch: 9 | Iteration: 249 | Classification loss: 0.00005 | Regression loss: 0.02358 | Running loss: 0.26687\n",
            "Epoch: 9 | Iteration: 250 | Classification loss: 0.11828 | Regression loss: 0.42971 | Running loss: 0.26749\n",
            "Epoch: 9 | Iteration: 251 | Classification loss: 0.02399 | Regression loss: 0.09345 | Running loss: 0.26667\n",
            "Epoch: 9 | Iteration: 252 | Classification loss: 0.11404 | Regression loss: 0.55332 | Running loss: 0.26743\n",
            "Epoch: 9 | Iteration: 253 | Classification loss: 0.06084 | Regression loss: 0.26577 | Running loss: 0.26793\n",
            "Epoch: 9 | Iteration: 254 | Classification loss: 0.05378 | Regression loss: 0.25980 | Running loss: 0.26842\n",
            "Epoch: 9 | Iteration: 255 | Classification loss: 0.09705 | Regression loss: 0.30185 | Running loss: 0.26921\n",
            "Epoch: 9 | Iteration: 256 | Classification loss: 0.05821 | Regression loss: 0.31642 | Running loss: 0.26895\n",
            "Epoch: 9 | Iteration: 257 | Classification loss: 0.11621 | Regression loss: 0.27078 | Running loss: 0.26910\n",
            "Epoch: 9 | Iteration: 258 | Classification loss: 0.09731 | Regression loss: 0.24192 | Running loss: 0.26941\n",
            "Epoch: 9 | Iteration: 259 | Classification loss: 0.03304 | Regression loss: 0.19729 | Running loss: 0.26977\n",
            "Epoch: 9 | Iteration: 260 | Classification loss: 0.07309 | Regression loss: 0.33681 | Running loss: 0.27021\n",
            "Epoch: 9 | Iteration: 261 | Classification loss: 0.03917 | Regression loss: 0.24476 | Running loss: 0.26993\n",
            "Epoch: 9 | Iteration: 262 | Classification loss: 0.07212 | Regression loss: 0.27107 | Running loss: 0.26977\n",
            "Epoch: 9 | Iteration: 263 | Classification loss: 0.01113 | Regression loss: 0.06551 | Running loss: 0.26939\n",
            "Epoch: 9 | Iteration: 264 | Classification loss: 0.07186 | Regression loss: 0.27935 | Running loss: 0.26880\n",
            "Epoch: 9 | Iteration: 265 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26801\n",
            "Epoch: 9 | Iteration: 266 | Classification loss: 0.03441 | Regression loss: 0.20898 | Running loss: 0.26792\n",
            "Epoch: 9 | Iteration: 267 | Classification loss: 0.06149 | Regression loss: 0.23597 | Running loss: 0.26822\n",
            "Epoch: 9 | Iteration: 268 | Classification loss: 0.08565 | Regression loss: 0.38839 | Running loss: 0.26914\n",
            "Epoch: 9 | Iteration: 269 | Classification loss: 0.04089 | Regression loss: 0.25191 | Running loss: 0.26875\n",
            "Epoch: 9 | Iteration: 270 | Classification loss: 0.08667 | Regression loss: 0.29993 | Running loss: 0.26912\n",
            "Epoch: 9 | Iteration: 271 | Classification loss: 0.10447 | Regression loss: 0.28267 | Running loss: 0.26930\n",
            "Epoch: 9 | Iteration: 272 | Classification loss: 0.04269 | Regression loss: 0.18764 | Running loss: 0.26937\n",
            "Epoch: 9 | Iteration: 273 | Classification loss: 0.02003 | Regression loss: 0.12798 | Running loss: 0.26935\n",
            "Epoch: 9 | Iteration: 274 | Classification loss: 0.02831 | Regression loss: 0.13308 | Running loss: 0.26925\n",
            "Epoch: 9 | Iteration: 275 | Classification loss: 0.12224 | Regression loss: 0.33785 | Running loss: 0.26930\n",
            "Epoch: 9 | Iteration: 276 | Classification loss: 0.05051 | Regression loss: 0.22099 | Running loss: 0.26930\n",
            "Epoch: 9 | Iteration: 277 | Classification loss: 0.00008 | Regression loss: 0.00393 | Running loss: 0.26930\n",
            "Epoch: 9 | Iteration: 278 | Classification loss: 0.06576 | Regression loss: 0.20759 | Running loss: 0.26913\n",
            "Epoch: 9 | Iteration: 279 | Classification loss: 0.06450 | Regression loss: 0.29959 | Running loss: 0.26966\n",
            "Epoch: 9 | Iteration: 280 | Classification loss: 0.00005 | Regression loss: 0.00000 | Running loss: 0.26926\n",
            "Epoch: 9 | Iteration: 281 | Classification loss: 0.04387 | Regression loss: 0.21774 | Running loss: 0.26922\n",
            "Epoch: 9 | Iteration: 282 | Classification loss: 0.00008 | Regression loss: 0.00772 | Running loss: 0.26883\n",
            "Epoch: 9 | Iteration: 283 | Classification loss: 0.08582 | Regression loss: 0.33323 | Running loss: 0.26963\n",
            "Epoch: 9 | Iteration: 284 | Classification loss: 0.00017 | Regression loss: 0.00170 | Running loss: 0.26900\n",
            "Epoch: 9 | Iteration: 285 | Classification loss: 0.05442 | Regression loss: 0.22207 | Running loss: 0.26954\n",
            "Epoch: 9 | Iteration: 286 | Classification loss: 0.04357 | Regression loss: 0.18949 | Running loss: 0.26899\n",
            "Epoch: 9 | Iteration: 287 | Classification loss: 0.00009 | Regression loss: 0.01602 | Running loss: 0.26806\n",
            "Epoch: 9 | Iteration: 288 | Classification loss: 0.14425 | Regression loss: 0.41702 | Running loss: 0.26916\n",
            "Epoch: 9 | Iteration: 289 | Classification loss: 0.06012 | Regression loss: 0.27490 | Running loss: 0.26983\n",
            "Epoch: 9 | Iteration: 290 | Classification loss: 0.04902 | Regression loss: 0.15173 | Running loss: 0.26932\n",
            "Epoch: 9 | Iteration: 291 | Classification loss: 0.12517 | Regression loss: 0.29944 | Running loss: 0.26918\n",
            "Epoch: 9 | Iteration: 292 | Classification loss: 0.01387 | Regression loss: 0.15030 | Running loss: 0.26950\n",
            "Epoch: 9 | Iteration: 293 | Classification loss: 0.06284 | Regression loss: 0.27587 | Running loss: 0.26952\n",
            "Epoch: 9 | Iteration: 294 | Classification loss: 0.00045 | Regression loss: 0.04690 | Running loss: 0.26935\n",
            "Epoch: 9 | Iteration: 295 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26912\n",
            "Epoch: 9 | Iteration: 296 | Classification loss: 0.05220 | Regression loss: 0.09645 | Running loss: 0.26837\n",
            "Epoch: 9 | Iteration: 297 | Classification loss: 0.06295 | Regression loss: 0.14347 | Running loss: 0.26858\n",
            "Epoch: 9 | Iteration: 298 | Classification loss: 0.06010 | Regression loss: 0.19467 | Running loss: 0.26872\n",
            "Epoch: 9 | Iteration: 299 | Classification loss: 0.00004 | Regression loss: 0.00000 | Running loss: 0.26856\n",
            "Epoch: 9 | Iteration: 300 | Classification loss: 0.03823 | Regression loss: 0.17533 | Running loss: 0.26860\n",
            "Epoch: 9 | Iteration: 301 | Classification loss: 0.00015 | Regression loss: 0.04658 | Running loss: 0.26836\n",
            "Epoch: 9 | Iteration: 302 | Classification loss: 0.07761 | Regression loss: 0.23079 | Running loss: 0.26842\n",
            "Epoch: 9 | Iteration: 303 | Classification loss: 0.12367 | Regression loss: 0.40595 | Running loss: 0.26892\n",
            "Epoch: 9 | Iteration: 304 | Classification loss: 0.02980 | Regression loss: 0.10202 | Running loss: 0.26819\n",
            "Epoch: 9 | Iteration: 305 | Classification loss: 0.00008 | Regression loss: 0.00770 | Running loss: 0.26789\n",
            "Epoch: 9 | Iteration: 306 | Classification loss: 0.08426 | Regression loss: 0.21502 | Running loss: 0.26786\n",
            "Epoch: 9 | Iteration: 307 | Classification loss: 0.05690 | Regression loss: 0.25035 | Running loss: 0.26776\n",
            "Epoch: 9 | Iteration: 308 | Classification loss: 0.00903 | Regression loss: 0.12893 | Running loss: 0.26664\n",
            "Epoch: 9 | Iteration: 309 | Classification loss: 0.02553 | Regression loss: 0.13130 | Running loss: 0.26665\n",
            "Epoch: 9 | Iteration: 310 | Classification loss: 0.05370 | Regression loss: 0.22441 | Running loss: 0.26665\n",
            "Epoch: 9 | Iteration: 311 | Classification loss: 0.02043 | Regression loss: 0.18595 | Running loss: 0.26655\n",
            "Epoch: 9 | Iteration: 312 | Classification loss: 0.06172 | Regression loss: 0.24193 | Running loss: 0.26713\n",
            "Epoch: 9 | Iteration: 313 | Classification loss: 0.04160 | Regression loss: 0.23882 | Running loss: 0.26767\n",
            "Epoch: 9 | Iteration: 314 | Classification loss: 0.11799 | Regression loss: 0.33796 | Running loss: 0.26773\n",
            "Epoch: 9 | Iteration: 315 | Classification loss: 0.04538 | Regression loss: 0.25528 | Running loss: 0.26792\n",
            "Epoch: 9 | Iteration: 316 | Classification loss: 0.08889 | Regression loss: 0.29399 | Running loss: 0.26816\n",
            "Epoch: 9 | Iteration: 317 | Classification loss: 0.00013 | Regression loss: 0.15939 | Running loss: 0.26817\n",
            "Epoch: 9 | Iteration: 318 | Classification loss: 0.01519 | Regression loss: 0.15297 | Running loss: 0.26739\n",
            "Epoch: 9 | Iteration: 319 | Classification loss: 0.23642 | Regression loss: 0.51873 | Running loss: 0.26879\n",
            "Epoch: 9 | Iteration: 320 | Classification loss: 0.00011 | Regression loss: 0.08168 | Running loss: 0.26824\n",
            "Epoch: 9 | Iteration: 321 | Classification loss: 0.00028 | Regression loss: 0.03451 | Running loss: 0.26775\n",
            "Epoch: 9 | Iteration: 322 | Classification loss: 0.00008 | Regression loss: 0.02911 | Running loss: 0.26687\n",
            "Epoch: 9 | Iteration: 323 | Classification loss: 0.00002 | Regression loss: 0.00000 | Running loss: 0.26638\n",
            "Epoch: 9 | Iteration: 324 | Classification loss: 0.00010 | Regression loss: 0.04764 | Running loss: 0.26645\n",
            "Epoch: 9 | Iteration: 325 | Classification loss: 0.04412 | Regression loss: 0.15489 | Running loss: 0.26631\n",
            "Epoch: 9 | Iteration: 326 | Classification loss: 0.03322 | Regression loss: 0.12581 | Running loss: 0.26629\n",
            "Epoch: 9 | Iteration: 327 | Classification loss: 0.06757 | Regression loss: 0.21778 | Running loss: 0.26576\n",
            "Epoch: 9 | Iteration: 328 | Classification loss: 0.00005 | Regression loss: 0.21013 | Running loss: 0.26618\n",
            "Epoch: 9 | Iteration: 329 | Classification loss: 0.02722 | Regression loss: 0.24997 | Running loss: 0.26638\n",
            "Epoch: 9 | Iteration: 330 | Classification loss: 0.03564 | Regression loss: 0.22293 | Running loss: 0.26617\n",
            "Epoch: 9 | Iteration: 331 | Classification loss: 0.14765 | Regression loss: 0.26476 | Running loss: 0.26624\n",
            "Epoch: 9 | Iteration: 332 | Classification loss: 0.04315 | Regression loss: 0.15703 | Running loss: 0.26661\n",
            "Epoch: 9 | Iteration: 333 | Classification loss: 0.07636 | Regression loss: 0.22836 | Running loss: 0.26637\n",
            "Epoch: 9 | Iteration: 334 | Classification loss: 0.03937 | Regression loss: 0.14790 | Running loss: 0.26669\n",
            "Epoch: 9 | Iteration: 335 | Classification loss: 0.01510 | Regression loss: 0.08766 | Running loss: 0.26581\n",
            "Epoch: 9 | Iteration: 336 | Classification loss: 0.00005 | Regression loss: 0.00794 | Running loss: 0.26545\n",
            "Epoch: 9 | Iteration: 337 | Classification loss: 0.33011 | Regression loss: 0.65636 | Running loss: 0.26705\n",
            "Epoch: 9 | Iteration: 338 | Classification loss: 0.06872 | Regression loss: 0.22986 | Running loss: 0.26728\n",
            "Epoch: 9 | Iteration: 339 | Classification loss: 0.08490 | Regression loss: 0.29365 | Running loss: 0.26803\n",
            "Epoch: 9 | Iteration: 340 | Classification loss: 0.06011 | Regression loss: 0.27310 | Running loss: 0.26762\n",
            "Epoch: 9 | Iteration: 341 | Classification loss: 0.03184 | Regression loss: 0.22284 | Running loss: 0.26781\n",
            "Epoch: 9 | Iteration: 342 | Classification loss: 0.23577 | Regression loss: 0.28367 | Running loss: 0.26839\n",
            "Epoch: 9 | Iteration: 343 | Classification loss: 0.00019 | Regression loss: 0.01228 | Running loss: 0.26768\n",
            "Epoch: 9 | Iteration: 344 | Classification loss: 0.05246 | Regression loss: 0.22091 | Running loss: 0.26790\n",
            "Epoch: 9 | Iteration: 345 | Classification loss: 0.02104 | Regression loss: 0.12761 | Running loss: 0.26775\n",
            "Epoch: 9 | Iteration: 346 | Classification loss: 0.04778 | Regression loss: 0.23380 | Running loss: 0.26724\n",
            "Epoch: 9 | Iteration: 347 | Classification loss: 0.00009 | Regression loss: 0.00871 | Running loss: 0.26726\n",
            "Epoch: 9 | Iteration: 348 | Classification loss: 0.00012 | Regression loss: 0.00320 | Running loss: 0.26723\n",
            "Epoch: 9 | Iteration: 349 | Classification loss: 0.00009 | Regression loss: 0.00485 | Running loss: 0.26671\n",
            "Epoch: 9 | Iteration: 350 | Classification loss: 0.00003 | Regression loss: 0.00000 | Running loss: 0.26591\n",
            "Epoch: 9 | Iteration: 351 | Classification loss: 0.05465 | Regression loss: 0.24984 | Running loss: 0.26638\n",
            "Epoch: 9 | Iteration: 352 | Classification loss: 0.19599 | Regression loss: 0.41189 | Running loss: 0.26664\n",
            "Epoch: 9 | Iteration: 353 | Classification loss: 0.07095 | Regression loss: 0.26775 | Running loss: 0.26696\n",
            "Epoch: 9 | Iteration: 354 | Classification loss: 0.00009 | Regression loss: 0.15416 | Running loss: 0.26707\n",
            "Epoch: 9 | Iteration: 355 | Classification loss: 0.02004 | Regression loss: 0.16507 | Running loss: 0.26683\n",
            "Epoch: 9 | Iteration: 356 | Classification loss: 0.02635 | Regression loss: 0.13131 | Running loss: 0.26674\n",
            "Epoch: 9 | Iteration: 357 | Classification loss: 0.11622 | Regression loss: 0.24044 | Running loss: 0.26627\n",
            "Epoch: 9 | Iteration: 358 | Classification loss: 0.07137 | Regression loss: 0.26451 | Running loss: 0.26692\n",
            "Epoch: 9 | Iteration: 359 | Classification loss: 0.18559 | Regression loss: 0.42488 | Running loss: 0.26696\n",
            "Epoch: 9 | Iteration: 360 | Classification loss: 0.03542 | Regression loss: 0.23586 | Running loss: 0.26747\n",
            "Epoch: 9 | Iteration: 361 | Classification loss: 0.05176 | Regression loss: 0.22153 | Running loss: 0.26708\n",
            "Epoch: 9 | Iteration: 362 | Classification loss: 0.00024 | Regression loss: 0.03807 | Running loss: 0.26715\n",
            "Epoch: 9 | Iteration: 363 | Classification loss: 0.04302 | Regression loss: 0.20357 | Running loss: 0.26763\n",
            "Epoch: 9 | Iteration: 364 | Classification loss: 0.03718 | Regression loss: 0.15629 | Running loss: 0.26743\n",
            "Epoch: 9 | Iteration: 365 | Classification loss: 0.04750 | Regression loss: 0.24952 | Running loss: 0.26679\n",
            "Epoch: 9 | Iteration: 366 | Classification loss: 0.12898 | Regression loss: 0.35459 | Running loss: 0.26657\n",
            "Epoch: 9 | Iteration: 367 | Classification loss: 0.09476 | Regression loss: 0.25334 | Running loss: 0.26720\n",
            "Epoch: 9 | Iteration: 368 | Classification loss: 0.00008 | Regression loss: 0.02660 | Running loss: 0.26667\n",
            "Epoch: 9 | Iteration: 369 | Classification loss: 0.06160 | Regression loss: 0.26431 | Running loss: 0.26664\n",
            "Epoch: 9 | Iteration: 370 | Classification loss: 0.01942 | Regression loss: 0.14812 | Running loss: 0.26662\n",
            "Epoch: 9 | Iteration: 371 | Classification loss: 0.10457 | Regression loss: 0.23760 | Running loss: 0.26727\n",
            "Epoch: 9 | Iteration: 372 | Classification loss: 0.00020 | Regression loss: 0.03412 | Running loss: 0.26731\n",
            "Epoch: 9 | Iteration: 373 | Classification loss: 0.03177 | Regression loss: 0.10093 | Running loss: 0.26665\n",
            "Epoch: 9 | Iteration: 374 | Classification loss: 0.11005 | Regression loss: 0.30071 | Running loss: 0.26722\n",
            "Epoch: 9 | Iteration: 375 | Classification loss: 0.07075 | Regression loss: 0.28749 | Running loss: 0.26734\n",
            "Epoch: 9 | Iteration: 376 | Classification loss: 0.07324 | Regression loss: 0.26386 | Running loss: 0.26747\n",
            "Epoch: 9 | Iteration: 377 | Classification loss: 0.01863 | Regression loss: 0.24002 | Running loss: 0.26733\n",
            "Epoch: 9 | Iteration: 378 | Classification loss: 0.02932 | Regression loss: 0.18803 | Running loss: 0.26767\n",
            "Epoch: 9 | Iteration: 379 | Classification loss: 0.24994 | Regression loss: 0.49987 | Running loss: 0.26870\n",
            "Epoch: 9 | Iteration: 380 | Classification loss: 0.00006 | Regression loss: 0.06575 | Running loss: 0.26851\n",
            "Epoch: 9 | Iteration: 381 | Classification loss: 0.18783 | Regression loss: 0.48776 | Running loss: 0.26981\n",
            "Epoch: 9 | Iteration: 382 | Classification loss: 0.01611 | Regression loss: 0.08809 | Running loss: 0.26960\n",
            "Epoch: 9 | Iteration: 383 | Classification loss: 0.00095 | Regression loss: 0.00798 | Running loss: 0.26867\n",
            "Epoch: 9 | Iteration: 384 | Classification loss: 0.07753 | Regression loss: 0.23886 | Running loss: 0.26778\n",
            "Epoch: 9 | Iteration: 385 | Classification loss: 0.03608 | Regression loss: 0.17579 | Running loss: 0.26736\n",
            "Epoch: 9 | Iteration: 386 | Classification loss: 0.09646 | Regression loss: 0.20395 | Running loss: 0.26704\n",
            "Epoch: 9 | Iteration: 387 | Classification loss: 0.00012 | Regression loss: 0.00764 | Running loss: 0.26672\n",
            "Epoch: 9 | Iteration: 388 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26629\n",
            "Epoch: 9 | Iteration: 389 | Classification loss: 0.01709 | Regression loss: 0.11805 | Running loss: 0.26650\n",
            "Epoch: 9 | Iteration: 390 | Classification loss: 0.08712 | Regression loss: 0.28729 | Running loss: 0.26721\n",
            "Epoch: 9 | Iteration: 391 | Classification loss: 0.22932 | Regression loss: 0.48638 | Running loss: 0.26747\n",
            "Epoch: 9 | Iteration: 392 | Classification loss: 0.00014 | Regression loss: 0.00759 | Running loss: 0.26628\n",
            "Epoch: 9 | Iteration: 393 | Classification loss: 0.13570 | Regression loss: 0.27345 | Running loss: 0.26629\n",
            "Epoch: 9 | Iteration: 394 | Classification loss: 0.00023 | Regression loss: 0.00918 | Running loss: 0.26582\n",
            "Epoch: 9 | Iteration: 395 | Classification loss: 0.01429 | Regression loss: 0.06385 | Running loss: 0.26561\n",
            "Epoch: 9 | Iteration: 396 | Classification loss: 0.08785 | Regression loss: 0.25243 | Running loss: 0.26574\n",
            "Epoch: 9 | Iteration: 397 | Classification loss: 0.13105 | Regression loss: 0.35141 | Running loss: 0.26596\n",
            "Epoch: 9 | Iteration: 398 | Classification loss: 0.03362 | Regression loss: 0.13965 | Running loss: 0.26551\n",
            "Epoch: 9 | Iteration: 399 | Classification loss: 0.11399 | Regression loss: 0.38216 | Running loss: 0.26597\n",
            "Epoch: 9 | Iteration: 400 | Classification loss: 0.08531 | Regression loss: 0.29207 | Running loss: 0.26573\n",
            "Epoch: 9 | Iteration: 401 | Classification loss: 0.01040 | Regression loss: 0.07292 | Running loss: 0.26539\n",
            "Epoch: 9 | Iteration: 402 | Classification loss: 0.03757 | Regression loss: 0.18126 | Running loss: 0.26503\n",
            "Epoch: 9 | Iteration: 403 | Classification loss: 0.00016 | Regression loss: 0.03684 | Running loss: 0.26421\n",
            "Epoch: 9 | Iteration: 404 | Classification loss: 0.02745 | Regression loss: 0.11373 | Running loss: 0.26361\n",
            "Epoch: 9 | Iteration: 405 | Classification loss: 0.03188 | Regression loss: 0.13220 | Running loss: 0.26366\n",
            "Epoch: 9 | Iteration: 406 | Classification loss: 0.05527 | Regression loss: 0.22022 | Running loss: 0.26354\n",
            "Epoch: 9 | Iteration: 407 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26214\n",
            "Epoch: 9 | Iteration: 408 | Classification loss: 0.13034 | Regression loss: 0.43707 | Running loss: 0.26282\n",
            "Epoch: 9 | Iteration: 409 | Classification loss: 0.05534 | Regression loss: 0.21985 | Running loss: 0.26245\n",
            "Epoch: 9 | Iteration: 410 | Classification loss: 0.05952 | Regression loss: 0.32082 | Running loss: 0.26314\n",
            "Epoch: 9 | Iteration: 411 | Classification loss: 0.00006 | Regression loss: 0.02194 | Running loss: 0.26240\n",
            "Epoch: 9 | Iteration: 412 | Classification loss: 0.00025 | Regression loss: 0.01832 | Running loss: 0.26127\n",
            "Epoch: 9 | Iteration: 413 | Classification loss: 0.02405 | Regression loss: 0.09609 | Running loss: 0.26105\n",
            "Epoch: 9 | Iteration: 414 | Classification loss: 0.04035 | Regression loss: 0.12872 | Running loss: 0.26077\n",
            "Epoch: 9 | Iteration: 415 | Classification loss: 0.04841 | Regression loss: 0.24968 | Running loss: 0.26010\n",
            "Epoch: 9 | Iteration: 416 | Classification loss: 0.06910 | Regression loss: 0.22351 | Running loss: 0.26018\n",
            "Epoch: 9 | Iteration: 417 | Classification loss: 0.09632 | Regression loss: 0.27397 | Running loss: 0.26092\n",
            "Epoch: 9 | Iteration: 418 | Classification loss: 0.04536 | Regression loss: 0.22830 | Running loss: 0.26140\n",
            "Epoch: 9 | Iteration: 419 | Classification loss: 0.00016 | Regression loss: 0.05744 | Running loss: 0.26085\n",
            "Epoch: 9 | Iteration: 420 | Classification loss: 0.00015 | Regression loss: 0.06576 | Running loss: 0.26053\n",
            "Epoch: 9 | Iteration: 421 | Classification loss: 0.09268 | Regression loss: 0.32303 | Running loss: 0.26062\n",
            "Epoch: 9 | Iteration: 422 | Classification loss: 0.15891 | Regression loss: 0.35000 | Running loss: 0.26080\n",
            "Epoch: 9 | Iteration: 423 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.25959\n",
            "Epoch: 9 | Iteration: 424 | Classification loss: 0.11930 | Regression loss: 0.39036 | Running loss: 0.25992\n",
            "Epoch: 9 | Iteration: 425 | Classification loss: 0.04954 | Regression loss: 0.18205 | Running loss: 0.26004\n",
            "Epoch: 9 | Iteration: 426 | Classification loss: 0.02449 | Regression loss: 0.20994 | Running loss: 0.25919\n",
            "Epoch: 9 | Iteration: 427 | Classification loss: 0.12149 | Regression loss: 0.28794 | Running loss: 0.25905\n",
            "Epoch: 9 | Iteration: 428 | Classification loss: 0.01606 | Regression loss: 0.12379 | Running loss: 0.25925\n",
            "Epoch: 9 | Iteration: 429 | Classification loss: 0.00025 | Regression loss: 0.04199 | Running loss: 0.25880\n",
            "Epoch: 9 | Iteration: 430 | Classification loss: 0.00011 | Regression loss: 0.03493 | Running loss: 0.25861\n",
            "Epoch: 9 | Iteration: 431 | Classification loss: 0.03271 | Regression loss: 0.19234 | Running loss: 0.25842\n",
            "Epoch: 9 | Iteration: 432 | Classification loss: 0.08113 | Regression loss: 0.24328 | Running loss: 0.25875\n",
            "Epoch: 9 | Iteration: 433 | Classification loss: 0.14534 | Regression loss: 0.42020 | Running loss: 0.25941\n",
            "Epoch: 9 | Iteration: 434 | Classification loss: 0.01811 | Regression loss: 0.12611 | Running loss: 0.25869\n",
            "Epoch: 9 | Iteration: 435 | Classification loss: 0.07844 | Regression loss: 0.35143 | Running loss: 0.25953\n",
            "Epoch: 9 | Iteration: 436 | Classification loss: 0.02644 | Regression loss: 0.17093 | Running loss: 0.25991\n",
            "Epoch: 9 | Iteration: 437 | Classification loss: 0.05747 | Regression loss: 0.23217 | Running loss: 0.25986\n",
            "Epoch: 9 | Iteration: 438 | Classification loss: 0.04600 | Regression loss: 0.23423 | Running loss: 0.25966\n",
            "Epoch: 9 | Iteration: 439 | Classification loss: 0.04078 | Regression loss: 0.14288 | Running loss: 0.25960\n",
            "Epoch: 9 | Iteration: 440 | Classification loss: 0.04854 | Regression loss: 0.21560 | Running loss: 0.25962\n",
            "Epoch: 9 | Iteration: 441 | Classification loss: 0.14976 | Regression loss: 0.34550 | Running loss: 0.26024\n",
            "Epoch: 9 | Iteration: 442 | Classification loss: 0.02249 | Regression loss: 0.11306 | Running loss: 0.26052\n",
            "Epoch: 9 | Iteration: 443 | Classification loss: 0.00010 | Regression loss: 0.04556 | Running loss: 0.26015\n",
            "Epoch: 9 | Iteration: 444 | Classification loss: 0.14701 | Regression loss: 0.47479 | Running loss: 0.26124\n",
            "Epoch: 9 | Iteration: 445 | Classification loss: 0.00018 | Regression loss: 0.02683 | Running loss: 0.26007\n",
            "Epoch: 9 | Iteration: 446 | Classification loss: 0.14666 | Regression loss: 0.30347 | Running loss: 0.26044\n",
            "Epoch: 9 | Iteration: 447 | Classification loss: 0.03321 | Regression loss: 0.18693 | Running loss: 0.26028\n",
            "Epoch: 9 | Iteration: 448 | Classification loss: 0.14574 | Regression loss: 0.24016 | Running loss: 0.26010\n",
            "Epoch: 9 | Iteration: 449 | Classification loss: 0.15073 | Regression loss: 0.33614 | Running loss: 0.26079\n",
            "Epoch: 9 | Iteration: 450 | Classification loss: 0.03700 | Regression loss: 0.14772 | Running loss: 0.26002\n",
            "Epoch: 9 | Iteration: 451 | Classification loss: 0.01124 | Regression loss: 0.08652 | Running loss: 0.25963\n",
            "Epoch: 9 | Iteration: 452 | Classification loss: 0.09596 | Regression loss: 0.32741 | Running loss: 0.25972\n",
            "Epoch: 9 | Iteration: 453 | Classification loss: 0.10478 | Regression loss: 0.32667 | Running loss: 0.26017\n",
            "Epoch: 9 | Iteration: 454 | Classification loss: 0.01870 | Regression loss: 0.11694 | Running loss: 0.25983\n",
            "Epoch: 9 | Iteration: 455 | Classification loss: 0.13920 | Regression loss: 0.31439 | Running loss: 0.25971\n",
            "Epoch: 9 | Iteration: 456 | Classification loss: 0.21850 | Regression loss: 0.39069 | Running loss: 0.26013\n",
            "Epoch: 9 | Iteration: 457 | Classification loss: 0.09041 | Regression loss: 0.37308 | Running loss: 0.26098\n",
            "Epoch: 9 | Iteration: 458 | Classification loss: 0.07961 | Regression loss: 0.16529 | Running loss: 0.26105\n",
            "Epoch: 9 | Iteration: 459 | Classification loss: 0.06615 | Regression loss: 0.25393 | Running loss: 0.26132\n",
            "Epoch: 9 | Iteration: 460 | Classification loss: 0.12320 | Regression loss: 0.29329 | Running loss: 0.26147\n",
            "Epoch: 9 | Iteration: 461 | Classification loss: 0.03952 | Regression loss: 0.21017 | Running loss: 0.26118\n",
            "Epoch: 9 | Iteration: 462 | Classification loss: 0.04140 | Regression loss: 0.15744 | Running loss: 0.26086\n",
            "Epoch: 9 | Iteration: 463 | Classification loss: 0.05557 | Regression loss: 0.25158 | Running loss: 0.26041\n",
            "Epoch: 9 | Iteration: 464 | Classification loss: 0.00001 | Regression loss: 0.00000 | Running loss: 0.26025\n",
            "Epoch: 9 | Iteration: 465 | Classification loss: 0.19426 | Regression loss: 0.39422 | Running loss: 0.26042\n",
            "Epoch: 9 | Iteration: 466 | Classification loss: 0.06096 | Regression loss: 0.22528 | Running loss: 0.26010\n",
            "Epoch: 9 | Iteration: 467 | Classification loss: 0.06129 | Regression loss: 0.23830 | Running loss: 0.26037\n",
            "Epoch: 9 | Iteration: 468 | Classification loss: 0.17957 | Regression loss: 0.38880 | Running loss: 0.26109\n",
            "Epoch: 9 | Iteration: 469 | Classification loss: 0.13206 | Regression loss: 0.31621 | Running loss: 0.26133\n",
            "Epoch: 9 | Iteration: 470 | Classification loss: 0.07260 | Regression loss: 0.29238 | Running loss: 0.26125\n",
            "Epoch: 9 | Iteration: 471 | Classification loss: 0.00009 | Regression loss: 0.01312 | Running loss: 0.26078\n",
            "Epoch: 9 | Iteration: 472 | Classification loss: 0.16837 | Regression loss: 0.41878 | Running loss: 0.26162\n",
            "Epoch: 9 | Iteration: 473 | Classification loss: 0.04005 | Regression loss: 0.19507 | Running loss: 0.26187\n",
            "Epoch: 9 | Iteration: 474 | Classification loss: 0.07509 | Regression loss: 0.21740 | Running loss: 0.26198\n",
            "Epoch: 9 | Iteration: 475 | Classification loss: 0.10277 | Regression loss: 0.36761 | Running loss: 0.26235\n",
            "Epoch: 9 | Iteration: 476 | Classification loss: 0.12874 | Regression loss: 0.37737 | Running loss: 0.26209\n",
            "Epoch: 9 | Iteration: 477 | Classification loss: 0.10335 | Regression loss: 0.15758 | Running loss: 0.26221\n",
            "Epoch: 9 | Iteration: 478 | Classification loss: 0.04561 | Regression loss: 0.15516 | Running loss: 0.26228\n",
            "Epoch: 9 | Iteration: 479 | Classification loss: 0.07263 | Regression loss: 0.20853 | Running loss: 0.26225\n",
            "Epoch: 9 | Iteration: 480 | Classification loss: 0.14107 | Regression loss: 0.30810 | Running loss: 0.26254\n",
            "Epoch: 9 | Iteration: 481 | Classification loss: 0.10669 | Regression loss: 0.24114 | Running loss: 0.26228\n",
            "Epoch: 9 | Iteration: 482 | Classification loss: 0.00005 | Regression loss: 0.01350 | Running loss: 0.26202\n",
            "Epoch: 9 | Iteration: 483 | Classification loss: 0.03908 | Regression loss: 0.18001 | Running loss: 0.26148\n",
            "Epoch: 9 | Iteration: 484 | Classification loss: 0.02729 | Regression loss: 0.19076 | Running loss: 0.26190\n",
            "Epoch: 9 | Iteration: 485 | Classification loss: 0.09199 | Regression loss: 0.29842 | Running loss: 0.26177\n",
            "Epoch: 9 | Iteration: 486 | Classification loss: 0.18665 | Regression loss: 0.40659 | Running loss: 0.26196\n",
            "Epoch: 9 | Iteration: 487 | Classification loss: 0.02641 | Regression loss: 0.10276 | Running loss: 0.26172\n",
            "Epoch: 9 | Iteration: 488 | Classification loss: 0.01256 | Regression loss: 0.08699 | Running loss: 0.26158\n",
            "Epoch: 9 | Iteration: 489 | Classification loss: 0.03635 | Regression loss: 0.14344 | Running loss: 0.26110\n",
            "Epoch: 9 | Iteration: 490 | Classification loss: 0.02708 | Regression loss: 0.16303 | Running loss: 0.26143\n",
            "Epoch: 9 | Iteration: 491 | Classification loss: 0.03720 | Regression loss: 0.21214 | Running loss: 0.26158\n",
            "Epoch: 9 | Iteration: 492 | Classification loss: 0.09167 | Regression loss: 0.38365 | Running loss: 0.26250\n",
            "Epoch: 9 | Iteration: 493 | Classification loss: 0.00006 | Regression loss: 0.00630 | Running loss: 0.26140\n",
            "Epoch: 9 | Iteration: 494 | Classification loss: 0.08105 | Regression loss: 0.33456 | Running loss: 0.26158\n",
            "Evaluating dataset\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.53s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.891\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.613\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.648\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.564\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "tcwWcaleyiBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-retinanet\n",
        "!python coco_validation.py --coco_path \"/content/PPE-Project-1\" --model_path \"/content/pytorch-retinanet/coco_retinanet_9.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Ni6StYymT8",
        "outputId": "2607d599-0c2c-4378-cd10-ecf1c41b751d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-retinanet\n",
            "CUDA available: True\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.72s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.891\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.613\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.648\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.564\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "41fDltP3fqgj",
        "gFVX0udzfqgv",
        "e5XDj7wBfqgz"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}